{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "5XQqp-uYIMWs",
        "Ohp9YS8GZHn8",
        "npuIYJEhUie_"
      ],
      "authorship_tag": "ABX9TyP2ig8QslYrR9FQur7DLNk3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/ssm/blob/main/Mamba_S6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "bgu3gbQOFnPv",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5b3bc06-1911-4a22-82f1-7231106e4322"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m511.6/511.6 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# @title hf byte dataset me\n",
        "!pip install -qU datasets # restart?\n",
        "import torch\n",
        "from torch.utils.data import IterableDataset\n",
        "\n",
        "class StreamDataset(IterableDataset):\n",
        "    def __init__(self, dataset, seq_len=129, buffer_size=1024):\n",
        "        self.vocab_size = 256 # utf-8 # self.enc.n_vocab # gpt2:50257\n",
        "        self.data = iter(dataset)\n",
        "        self.seq_len, self.buffer_size = seq_len, buffer_size  # must be ‚â• seq_len\n",
        "        self.buffer = []  # token buffer\n",
        "        self.fill_buffer()\n",
        "\n",
        "    def fill_buffer(self):\n",
        "        while len(self.buffer) < self.buffer_size:\n",
        "            x = next(self.data)\n",
        "            # print(x)\n",
        "            self.buffer.extend(x['text'].encode(\"utf-8\"))\n",
        "\n",
        "    def __iter__(self):\n",
        "        while True:\n",
        "            if len(self.buffer) < self.seq_len: self.fill_buffer()\n",
        "            if len(self.buffer) < self.seq_len: return # raise StopIteration\n",
        "            x, self.buffer = self.buffer[:self.seq_len], self.buffer[self.seq_len:]\n",
        "            yield torch.tensor(x, dtype=torch.int32) # uint8 int32\n",
        "\n",
        "from datasets import load_dataset\n",
        "name = 'Skylion007/openwebtext' if torch.cuda.is_available() else 'stas/openwebtext-10k'\n",
        "dataset = load_dataset(name, split=\"train\", streaming=True, revision='refs/convert/parquet', cache_dir=\"/content/hf\")\n",
        "\n",
        "seq_len = 128*1+1 # 128\n",
        "buffer_size = seq_len*1\n",
        "train_data = StreamDataset(dataset, seq_len, buffer_size) # train_data = StreamDataset(dataset[\"train\"], seq_len, buffer_size)\n",
        "# del dataset\n",
        "\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 64 if torch.cuda.is_available() else 16 #64 512\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, pin_memory=True, num_workers=0)\n",
        "# del train_data\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# https://github.com/facebookresearch/blt/blob/main/bytelatent/tokenizers/blt_tokenizer.py#L137\n",
        "# def encode(c): return torch.tensor(list(c.encode(\"utf-8\")), dtype=torch.uint8)#, device=device)#.unsqueeze(0)\n",
        "def encode(c): return torch.tensor(list(c.encode(\"utf-8\")), dtype=torch.int32, device=device).unsqueeze(0)\n",
        "def decode(x): return bytes(x.tolist()).decode(\"utf-8\", errors='replace') # replace ignore\n",
        "\n",
        "# for x in train_loader:\n",
        "#     print(x.shape, x)\n",
        "#     break\n",
        "# print(decode(x[0][:64]))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title seq conv lcse\n",
        "# ht = at*ht-1 + bt\n",
        "\n",
        "def seq(at, bt, h0=None): # [t,b,d], [t,b,d], [b,d]\n",
        "    h = 0 if h0==None else h0\n",
        "    ht = []\n",
        "    for t in range(at.size(0)):\n",
        "        h = at[t] * h + bt[t]\n",
        "        ht.append(h)\n",
        "    ht = torch.stack(ht, dim=0)\n",
        "    return ht\n",
        "\n",
        "# conv\n",
        "# h1=a1*h0+b1 =       a1*h0 +       b1\n",
        "# h2=a2*h1+b2 =    a2*a1*h0 +    a2*b1 +    b2\n",
        "# h3=a3*h2+b3 = a3*a2*a1*h0 + a3*a2*b1 + a3*b2 + b3\n",
        "\n",
        "# h1/a1   = h0 + b1/a1\n",
        "# h2/a12  = h0 + b1/a1 + b2/a12\n",
        "# h3/a123 = h0 + b1/a1 + b2/a12 + b3/a123\n",
        "\n",
        "def conv(at, bt, h0=None): # [t,b,d], [t,b,d], [b,d]\n",
        "    a123 = torch.cumprod(at, dim=0) # [t,b,d]\n",
        "    ht = torch.cumsum(bt/a123, dim=0) # cusum b1/a1, b2/a12, b3/a123, ...\n",
        "    ht = ((h0 if h0!=None else 0)+ht)*a123\n",
        "    return ht\n",
        "\n",
        "# ht = A* ht-1 + B*ut ; yt = C *ht + D*ut\n",
        "# let kernel K: Km = CA^mB\n",
        "\n",
        "# K = C @ A**torch.arange(1,t+1) @ B\n",
        "# u = F.pad(u, (t-1, 0))\n",
        "# y = F.conv1d(u, K) + D*u\n",
        "\n",
        "# fft\n",
        "# L_fft = 2 * T\n",
        "# uf = torch.fft.rfft(u, n=L_fft)            # (B, Din, L_fft//2+1)\n",
        "# kf = torch.fft.rfft(K, n=L_fft)            # (Dout, Din, L_fft//2+1)\n",
        "# # Multiply + sum over input channels\n",
        "# # out[b, dout, f] = sum_{din} uf[b,din,f] * kf[dout,din,f]\n",
        "# yf = torch.einsum(\"bdf, odf -> bof\", uf, kf)\n",
        "# y = torch.fft.irfft(yf, n=L_fft)[..., :T]   # causal output\n",
        "\n",
        "\n",
        "# lcse\n",
        "# computing the sequence ht = at*ht-1 + bt\n",
        "# xt = e^(a*_t +tail(LCSE(cat(log x0, log b_t -a*_t))))\n",
        "# b*_t = Sum t] e^( log bt - a*_t)\n",
        "# a*_t = Sum t] log a_t\n",
        "def lcse(at, bt, h0=None): # [t,b,d], [t,b,d], [b,d]\n",
        "    if h0!=None:\n",
        "        x_in = torch.cat([h0.log().unsqueeze(0), bt.log()], dim=0) # [1+t,b,d]\n",
        "        at = torch.cat([torch.zeros(1,*at.shape[1:]), torch.cumsum(at.log(), dim=0)], dim=0) # a*_t # [1+t,b,d]\n",
        "        ht = torch.exp(at[1:] + torch.logcumsumexp(x_in - at, dim=0)[1:])\n",
        "    else:\n",
        "        at = torch.cumsum(at.log(), dim=0) # a*_t # [t,b,d]\n",
        "        ht = torch.exp(at + torch.logcumsumexp(bt.log() - at, dim=0))\n",
        "    return ht\n",
        "\n",
        "\n",
        "t, b, d = 30, 4, 5\n",
        "# t, b, d = 200, 64, 64\n",
        "at = torch.randn(t,b,d, dtype=torch.complex64)\n",
        "bt = torch.randn(t,b,d, dtype=torch.complex64)\n",
        "h0 = torch.randn(b,d, dtype=torch.complex64)\n",
        "h0 = None\n",
        "ht = seq(at, bt, h0)\n",
        "ht1 = conv(at, bt, h0)\n",
        "# ht1 = lcse(at, bt, h0)\n",
        "print(abs(ht-ht1).sum())\n",
        "# print(ht[:3,0,:5])\n",
        "# print(ht1[:3,0,:5])\n",
        "\n",
        "# cpu; t, b, d = 200, 64, 64\n",
        "# %timeit seq(at, bt, h0) # 6.16 ms 6.09 6.25\n",
        "# %timeit conv(at, bt, h0) # 25.6 ms 24.2\n",
        "# %timeit lcse(at, bt, h0) # 344 ms 348 ms\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "B-CEHIj35vPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ssd\n",
        "# https://goombalab.github.io/blog/2024/mamba2-part4-systems/\n",
        "# https://tridao.me/blog/2024/mamba2-part3-algorithm/\n",
        "# # https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/triton/ssd_combined.py\n",
        "# y = mamba_chunk_scan_combined(x.unflatten(-1, (self.n_heads, self.d_head)), dt, A, # chunk_size=256,\n",
        "#     B.unflatten(-1, (self.ngroups, self.d_state)), C.unflatten(-1, (self.ngroups, self.d_state)),\n",
        "#     D=self.D, z=None).flatten(2) # norm before flatten? so norm is within group\n",
        "\n",
        "def segsum(x):\n",
        "    \"\"\"Naive segment sum calculation. exp(segsum(A)) produces a 1-SS matrix,\n",
        "       which is equivalent to a scalar SSM.\"\"\"\n",
        "    T = x.size(-1)\n",
        "    x_cumsum = torch.cumsum(x, dim=-1)\n",
        "    x_segsum = x_cumsum[...,:,None] - x_cumsum[...,None,:]\n",
        "    mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool), diagonal=0)\n",
        "    x_segsum = x_segsum.masked_fill(~mask, -torch.inf)\n",
        "    # mask = torch.triu(torch.ones(T, T, device=x.device, dtype=bool))\n",
        "    # x_segsum = x_segsum.masked_fill(mask, -torch.inf)\n",
        "    return x_segsum\n",
        "\n",
        "def ssd(X, A, B, C, block_len=64, h0=None): # X:[b,t,n_heads,d_head], A:[b,t,n_heads], B:[b,t,n_heads,d_state], C:[b,t,n_heads,d_state]\n",
        "    assert X.dtype == A.dtype == B.dtype == C.dtype\n",
        "    assert X.shape[1] % block_len == 0\n",
        "\n",
        "    X, A, B, C = [x.unflatten(1, (-1,block_len)) for x in (X, A, B, C)]\n",
        "    A = A.permute(0,3,1,2)\n",
        "    A_cumsum = torch.cumsum(A, dim=-1)\n",
        "    # 1. Compute the output for each intra-chunk (diagonal blocks)\n",
        "    L = torch.exp(segsum(A))\n",
        "    Y_diag  = torch.einsum(\"bclhn,bcshn,bhcls,bcshp->bclhp\", C, B, L, X)\n",
        "\n",
        "    # 2. Compute the state for each intra-chunk\n",
        "    # (right term of low-rank factorization of off-diagonal blocks; B terms)\n",
        "    decay_states = torch.exp((A_cumsum[:,:,:,-1:] - A_cumsum))\n",
        "    states = torch.einsum(\"bclhn,bhcl,bclhp->bchpn\", B, decay_states, X)\n",
        "\n",
        "    # 3. Compute the inter-chunk SSM recurrence; produces correct SSM states at chunk boundaries\n",
        "    # (middle term of factorization of off-diag blocks; A terms)\n",
        "    # if h0==None: h0 = torch.zeros_like(states[:,:1])\n",
        "    if h0==None:\n",
        "        h0 = torch.zeros(states.size(0),*states.shape[2:])\n",
        "        # print('h0', h0.shape)\n",
        "    # print('ssd', h0.shape, states.shape, [states.size(0),1,*states.shape[2:]])\n",
        "    states = torch.cat([h0.unsqueeze(1), states], dim=1)\n",
        "    decay_chunk = torch.exp(segsum(F.pad(A_cumsum[:,:,:,-1], (1,0))))\n",
        "    new_states = torch.einsum(\"bhzc,bchpn->bzhpn\", decay_chunk, states)\n",
        "\n",
        "    # 4. Compute state -> output conversion per chunk\n",
        "    # (left term of low-rank factorization of off-diagonal blocks; C terms)\n",
        "    Y_off = torch.einsum('bclhn,bchpn,bhcl->bclhp', C, new_states[:,:-1], torch.exp(A_cumsum))\n",
        "\n",
        "    # Add output of intra-chunk and inter-chunk terms (diagonal and off-diagonal blocks)\n",
        "    Y = (Y_diag+Y_off).flatten(1,2)\n",
        "    return Y, new_states[:,-1] # [b,t,n_heads,d_head],\n",
        "\n"
      ],
      "metadata": {
        "id": "pYvDiHSghoDL",
        "cellView": "form"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title mamba_ssm/modules/mamba2_simple.py\n",
        "# https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba2_simple.py\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class Mamba2Simple(nn.Module):\n",
        "    def __init__(self, in_dim, d_state=64, expand=1, n_heads=8, d_conv=4):\n",
        "        super().__init__()\n",
        "        self.in_dim, self.d_state, self.d_conv = in_dim, d_state, d_conv\n",
        "        self.d_inner = expand * self.in_dim\n",
        "        self.n_heads, self.d_head = n_heads, self.d_inner//n_heads\n",
        "\n",
        "        self.in_proj = nn.Linear(self.in_dim, 2* self.d_inner + 2* self.n_heads*self.d_state + self.n_heads, bias=False) # z,x,B,C,dt\n",
        "        conv_dim = self.d_inner + 2* self.n_heads*self.d_state # for x,B,C\n",
        "        self.conv1d = nn.Conv1d(in_channels=conv_dim, out_channels=conv_dim, bias=True, kernel_size=d_conv, groups=conv_dim, padding=d_conv-1)\n",
        "        # self.conv1d.weight._no_weight_decay = True\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "        # self.h0 = nn.Parameter(torch.zeros(self.n_heads, self.d_head, self.d_state))\n",
        "        # self.h0._no_weight_decay = True\n",
        "        dt_min, dt_max = .001, .1\n",
        "        dt = torch.exp(torch.rand(self.n_heads) * (math.log(dt_max)-math.log(dt_min)) + math.log(dt_min)).clamp(min=1e-4)\n",
        "        self.dt_bias = nn.Parameter(dt + torch.log(-torch.expm1(-dt))) # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n",
        "        self.dt_bias._no_weight_decay = True\n",
        "        A = torch.empty(self.n_heads, dtype=torch.float32).uniform_(1,16)\n",
        "        A_log = torch.log(A)\n",
        "        self.A_log = nn.Parameter(A_log)\n",
        "        self.A_log._no_weight_decay = True\n",
        "        self.D = nn.Parameter(torch.ones(self.n_heads))\n",
        "        self.D._no_weight_decay = True\n",
        "        self.norm = nn.RMSNorm(self.d_inner)\n",
        "        self.out_proj = nn.Linear(self.d_inner, self.in_dim, bias=False)\n",
        "\n",
        "    # def forward(self, u, h0=None): # [b,t,d]\n",
        "    def forward(self, u): # [b,t,d]\n",
        "        # if h0!=None: return self.step(u, h0)\n",
        "        # print('Mamba2Simple u', u.shape)\n",
        "        A = -torch.exp(self.A_log) # [n_heads]\n",
        "        z, xBC, dt = self.in_proj(u).split([self.d_inner, self.d_inner + 2* self.n_heads*self.d_state, self.n_heads], dim=-1)\n",
        "        dt = F.softplus(dt+self.dt_bias) # [b,t,h]\n",
        "\n",
        "        h_conv = F.pad(xBC, (0,0,self.d_conv-u.shape[1],0)) # [b,k,xbc]\n",
        "        xBC = self.act(self.conv1d(xBC.transpose(-2,-1)).transpose(-2,-1)[:,:u.size(1),:])  # [b,t, d_inner + 2* ngroups*d_state]\n",
        "        x, B, C = xBC.unflatten(-1, (self.n_heads,-1)).split([self.d_head, self.d_state, self.d_state], dim=-1) # x:[b,t,h,d], B/C:[b,t,h,s] # X, B, C correspond to V, K, Q respectively in the SSM/attention duality\n",
        "        # h0 = self.h0.expand(u.size(0),-1,-1,-1) # [b,n,d,s]\n",
        "        # print('x dt a b', x.shape, dt.shape, A.shape, B.shape)\n",
        "\n",
        "        # y, h_ssm = ssd(x*dt.unsqueeze(-1), A*dt, B, C, 64, h0) # 256\n",
        "        y, h_ssm = ssd(x*dt.unsqueeze(-1), A*dt, B, C, 64) # 256\n",
        "\n",
        "        y = y + x * self.D.unsqueeze(-1)\n",
        "        y = self.norm(y.flatten(2) * self.act(z)) # [b,t,d_inner] # norm(x)*silu(z) if norm_before_gate, else norm(x*silu(z)) # https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/triton/layernorm_gated.py#L18\n",
        "        out = self.out_proj(y)\n",
        "        return out, (h_conv, h_ssm) # [b,t,in], [b,h,d,s]\n",
        "\n",
        "    def step(self, u, h=None): # [b,t,in], (conv[b,k,xbc], ssm[b,h,d,s])\n",
        "        u = u[:,-self.d_conv:] # [b,min(k,t),in]\n",
        "        z, xBC, dt = self.in_proj(u).split([self.d_inner, self.d_inner + 2* self.n_heads*self.d_state, self.n_heads], dim=-1) # [b,min(k,t),_]\n",
        "\n",
        "        if h==None: h_conv = F.pad(xBC, (0,0,max(0,self.d_conv-u.shape[1]),0))\n",
        "        else: h_conv = torch.cat([h[0], xBC], dim=1)[:,-self.d_conv:] # [b,k,xbc]\n",
        "        # print(h_conv.shape, self.conv1d.weight.shape, self.conv1d.bias.shape)\n",
        "\n",
        "        xBC = F.silu(torch.einsum(\"bkc,cik->bc\", h_conv, self.conv1d.weight) + self.conv1d.bias) # [b,kernel,xbc], [xbc,1,kernel], +[xbc] -> [b,xbc]\n",
        "        x, B, C = xBC.unflatten(-1, (self.n_heads,-1)).split([self.d_head, self.d_state, self.d_state], dim=-1) # x:[b,t,h,d], B/C:[b,t,h,s] # X, B, C correspond to V, K, Q respectively in the SSM/attention duality\n",
        "        A = -torch.exp(self.A_log)  # [n_heads]\n",
        "\n",
        "        # SSM step\n",
        "        dt = F.softplus(dt[:,-1]+self.dt_bias) # [b,n_heads]\n",
        "        dA = torch.exp(dt*A) # [b,n_heads]\n",
        "        # print('dt, B, x', dt.shape, B.shape, x.shape)\n",
        "        dBx = torch.einsum(\"bh,bhs,bhd->bhds\", dt, B, x)\n",
        "        if h==None: h_ssm = dBx\n",
        "        else: h_ssm = h[1] * dA[...,None,None] + dBx # [b,h,d,s]*[b,h,1,1]+[b,h,d,s]\n",
        "        # print('h.ssm_state, C', h.ssm_state.shape, C.shape)\n",
        "        y = torch.einsum(\"bhds,bhs->bhd\", h_ssm, C) # [b,h,d,s]*[b,h,s]\n",
        "        y = y + self.D.unsqueeze(-1) * x\n",
        "        y = self.norm(y.flatten(1) * F.silu(z[:,-1]))\n",
        "        y = self.out_proj(y)\n",
        "        return y.unsqueeze(1), (h_conv, h_ssm)\n",
        "\n",
        "\n",
        "\n",
        "b,t,in_dim=5,256,32\n",
        "# b,t,in_dim=5,7,32\n",
        "d_model = 64\n",
        "u = torch.randn(b,t,in_dim, device=device)\n",
        "model = Mamba2Simple(in_dim, d_model).to(device)\n",
        "# out, h = model(u)\n",
        "# h0 = torch.randn(b, model.n_heads, model.d_head, model.d_state)\n",
        "# print(out.shape)\n",
        "# print(out.shape, h.shape)\n",
        "# print(out[0,-3:,:5], h[0,:2,:5,:5])\n",
        "# out, h = model(u, h)\n",
        "\n",
        "# u = torch.randn(b,7,in_dim, device=device)\n",
        "# # out, h = model(u[:,-1:], h)\n",
        "# out, h = model.step(u, h)\n",
        "out, h = model.step(u)\n",
        "print(out.shape)\n",
        "# print(out[0,-3:,:5], h[0,:2,:5,:5])\n",
        "\n"
      ],
      "metadata": {
        "id": "T6V3B_QM9iVX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "350d4388-2bfc-4ee0-a2ea-30e29c31a7d3"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 1, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title tommyip/mamba2-minimal\n",
        "# https://github.com/tommyip/mamba2-minimal/blob/main/mamba2.py\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "\n",
        "from torch import Tensor\n",
        "from typing import NamedTuple, cast\n",
        "class InferenceCache(NamedTuple):\n",
        "    conv_state: Tensor  # (batch, d_inner + 2* d_state, d_conv)\n",
        "    ssm_state: Tensor  # (batch, nheads, d_head, d_state)\n",
        "    @staticmethod\n",
        "    def alloc(batch_size, d_inner, n_heads, d_head, d_state, d_conv, device=None):\n",
        "        return InferenceCache(torch.zeros(batch_size, d_inner + 2* d_state, d_conv, device=device),\n",
        "            torch.zeros(batch_size, n_heads, d_head, d_state, device=device))\n",
        "\n",
        "\n",
        "class Mamba2(nn.Module):\n",
        "    # def __init__(self, args: Mamba2Config, device = None):\n",
        "    def __init__(self, d_model=256, d_state=128, n_heads=4, expand=2, d_conv=4, n_layer=2):\n",
        "        super().__init__()\n",
        "        self.d_model, self.d_state, self.d_conv = d_model, d_state, d_conv\n",
        "        self.d_inner = expand * d_model\n",
        "        self.n_heads, self.d_head = n_heads, self.d_inner//n_heads\n",
        "\n",
        "        self.in_proj = nn.Linear(self.d_model, 2* self.d_inner + 2* self.d_state + self.n_heads, bias=False) # z, x, B, C, dt\n",
        "        conv_dim = self.d_inner + 2* self.d_state\n",
        "        self.conv1d = nn.Conv1d(in_channels=conv_dim, out_channels=conv_dim, kernel_size=self.d_conv, groups=conv_dim, padding=self.d_conv-1)\n",
        "\n",
        "        self.dt_bias = nn.Parameter(torch.empty(self.n_heads))\n",
        "        self.A_log = nn.Parameter(torch.empty(self.n_heads))\n",
        "        self.D = nn.Parameter(torch.empty(self.n_heads))\n",
        "        self.norm = nn.RMSNorm(self.d_inner)\n",
        "        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=False)\n",
        "\n",
        "    def forward(self, u, h=None): # (batch, seqlen, d_model)\n",
        "        # print('fwd', u.shape, h.shape if h!=None else None)\n",
        "        # print('fwd', u.shape)\n",
        "        if h: return self.step(u, h)\n",
        "        A = -torch.exp(self.A_log)  # (n_heads,)\n",
        "        zxbcdt = self.in_proj(u)  # (batch, seqlen, d_in_proj)\n",
        "        z, xBC, dt = torch.split(zxbcdt, [self.d_inner, self.d_inner + 2* self.d_state, self.n_heads], dim=-1,)\n",
        "        dt = F.softplus(dt + self.dt_bias)  # (batch, seqlen, n_heads)\n",
        "\n",
        "        # Pad or truncate xBC seqlen to d_conv\n",
        "        conv_state = F.pad(rearrange(xBC, \"b l d -> b d l\"), (self.d_conv - u.shape[1], 0))\n",
        "        xBC = F.silu(self.conv1d(xBC.transpose(1, 2)).transpose(1, 2)[:, : u.shape[1], :])  # (batch, seqlen, d_inner + 2 * d_state))\n",
        "        x, B, C = torch.split(xBC, [self.d_inner, self.d_state, self.d_state], dim=-1)\n",
        "\n",
        "        x = rearrange(x, \"b l (h p) -> b l h p\", p=self.d_head)\n",
        "        y, ssm_state = ssd(x * dt.unsqueeze(-1), A * dt,\n",
        "            rearrange(B, \"b l n -> b l 1 n\"), rearrange(C, \"b l n -> b l 1 n\"), 8)\n",
        "\n",
        "        y = y + x * self.D.unsqueeze(-1)\n",
        "        y = rearrange(y, \"b l h p -> b l (h p)\")\n",
        "        y = self.norm(y * F.silu(z))\n",
        "        y = self.out_proj(y)\n",
        "        h = InferenceCache(conv_state, ssm_state)\n",
        "        return y, h # (batch, seqlen, d_model)\n",
        "\n",
        "    def step(self, u, h): # (batch, 1, d_model)\n",
        "        assert u.shape[1] == 1, \"Only one token can be decoded per inference step\"\n",
        "        A = -torch.exp(self.A_log)  # [n_heads]\n",
        "        zxbcdt = self.in_proj(u.squeeze(1))  # (batch, d_in_proj)\n",
        "        z, xBC, dt = torch.split(zxbcdt, [self.d_inner, self.d_inner + 2* self.d_state, self.n_heads], dim=-1,)\n",
        "        dt = F.softplus(dt+self.dt_bias)  # (batch, n_heads)\n",
        "\n",
        "        # Advance convolution input\n",
        "        h.conv_state.copy_(torch.roll(h.conv_state, shifts=-1, dims=-1))\n",
        "        h.conv_state[:,:,-1] = xBC\n",
        "        xBC = F.silu(torch.sum(h.conv_state * self.conv1d.weight.squeeze(1), dim=-1) + self.conv1d.bias)\n",
        "        x, B, C = xBC.split([self.d_inner, self.d_state, self.d_state], dim=-1)\n",
        "\n",
        "        # SSM step\n",
        "        dA = torch.exp(dt*A)  # (batch, n_heads)\n",
        "        x = x.unflatten(-1, (-1,self.d_head))\n",
        "        dBx = torch.einsum(\"bh,bn,bhp->bhpn\", dt, B, x)\n",
        "        h.ssm_state.copy_(h.ssm_state * dA[...,None,None] + dBx)\n",
        "        y = torch.einsum(\"bhpn,bn->bhp\", h.ssm_state, C)\n",
        "\n",
        "        y = y + self.D.unsqueeze(-1) * x\n",
        "        y = self.norm(y.flatten(1) * F.silu(z))\n",
        "        y = self.out_proj(y)\n",
        "        return y.unsqueeze(1), h # (batch, 1, d_model)\n",
        "\n",
        "\n",
        "# def segsum(x, device = None):\n",
        "#     \"\"\"Stable segment sum calculation.\n",
        "#     `exp(segsum(A))` produces a 1-semiseparable matrix, which is equivalent to a scalar SSM.\n",
        "#     Source: https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/ssd_minimal.py#L23-L32\"\"\"\n",
        "#     T = x.size(-1)\n",
        "#     x = repeat(x, \"... d -> ... d e\", e=T)\n",
        "#     mask = torch.tril(torch.ones(T, T, dtype=torch.bool, device=device), diagonal=-1)\n",
        "#     x = x.masked_fill(~mask, 0)\n",
        "#     x_segsum = torch.cumsum(x, dim=-2)\n",
        "#     mask = torch.tril(torch.ones(T, T, dtype=torch.bool, device=device), diagonal=0)\n",
        "#     x_segsum = x_segsum.masked_fill(~mask, -torch.inf)\n",
        "#     return x_segsum\n",
        "\n",
        "\n",
        "# def ssd(x, A, B, C, chunk_size, initial_states=None, device = None):\n",
        "#     \"\"\"Structed State Space Duality (SSD) - the core of Mamba-2\n",
        "#     This is almost the exact same minimal SSD code from the blog post.\n",
        "#     Arguments\n",
        "#         x: (batch, seqlen, n_heads, d_head)\n",
        "#         A: (batch, seqlen, n_heads)\n",
        "#         B: (batch, seqlen, n_heads, d_state)\n",
        "#         C: (batch, seqlen, n_heads, d_state)\n",
        "#     Return y: (batch, seqlen, n_heads, d_head)\n",
        "\n",
        "#     Source\n",
        "#      1. https://tridao.me/blog/2024/mamba2-part3-algorithm/\n",
        "#      2. https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/ssd_minimal.py#L34-L78\n",
        "#     \"\"\"\n",
        "#     assert x.shape[1] % chunk_size == 0\n",
        "\n",
        "#     # Rearrange into chunks\n",
        "#     # Step 1, 2 and 4 of SSD can be computed in parallel for each chunk across devices (sequence parallel)\n",
        "#     # This is not implemented and left as an exercise for the reader üòú\n",
        "#     x, A, B, C = [rearrange(m, \"b (c l) ... -> b c l ...\", l=chunk_size) for m in (x, A, B, C)]\n",
        "\n",
        "#     A = rearrange(A, \"b c l h -> b h c l\")\n",
        "#     A_cumsum = torch.cumsum(A, dim=-1)\n",
        "\n",
        "#     # 1. Compute the output for each intra-chunk (diagonal blocks)\n",
        "#     L = torch.exp(segsum(A, device=device))\n",
        "#     Y_diag = torch.einsum(\"bclhn, bcshn, bhcls, bcshp -> bclhp\", C, B, L, x)\n",
        "\n",
        "#     # 2. Compute the state for each intra-chunk\n",
        "#     # (right term of low-rank factorization of off-diagonal blocks; B terms)\n",
        "#     decay_states = torch.exp(A_cumsum[:, :, :, -1:] - A_cumsum)\n",
        "#     states = torch.einsum(\"bclhn, bhcl, bclhp -> bchpn\", B, decay_states, x)\n",
        "\n",
        "#     # 3. Compute the inter-chunk SSM recurrence; produces correct SSM states at chunk boundaries\n",
        "#     # (middle term of factorization of off-diag blocks; A terms)\n",
        "#     if initial_states is None:\n",
        "#         initial_states = torch.zeros_like(states[:, :1])\n",
        "#     states = torch.cat([initial_states, states], dim=1)\n",
        "#     decay_chunk = torch.exp(segsum(F.pad(A_cumsum[:, :, :, -1], (1, 0)), device=device))\n",
        "#     new_states = torch.einsum(\"bhzc, bchpn -> bzhpn\", decay_chunk, states)\n",
        "#     states, final_state = new_states[:, :-1], new_states[:, -1]\n",
        "\n",
        "#     # 4. Compute state -> output conversion per chunk\n",
        "#     # (left term of low-rank factorization of off-diagonal blocks; C terms)\n",
        "#     state_decay_out = torch.exp(A_cumsum)\n",
        "#     Y_off = torch.einsum(\"bclhn, bchpn, bhcl -> bclhp\", C, states, state_decay_out)\n",
        "\n",
        "#     # Add output of intra-chunk and inter-chunk terms (diagonal and off-diagonal blocks)\n",
        "#     Y = rearrange(Y_diag + Y_off, \"b c l h p -> b (c l) h p\")\n",
        "#     return Y, final_state\n",
        "\n",
        "\n",
        "batch=4\n",
        "seq_len=16\n",
        "d_model=256\n",
        "\n",
        "pred = Mamba2(d_model)\n",
        "\n",
        "x = torch.randn(batch, seq_len, d_model)\n",
        "y, h = pred(x)\n",
        "# print(x)\n",
        "# print(y)\n",
        "print(x.shape)\n",
        "print(y.shape)\n",
        "\n",
        "x = torch.randn(batch, 1, d_model)\n",
        "y, h = pred(x,h)\n",
        "\n",
        "print(y.shape)\n",
        "# print(y)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "drcz9JmNR9jx",
        "outputId": "c5389dcd-1d89-4e86-d681-e762b6d7a2bc"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 16, 256])\n",
            "torch.Size([4, 16, 256])\n",
            "torch.Size([4, 1, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title GPT\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "    def forward(self, x, hid=None):\n",
        "        # hidden = []\n",
        "        for i, layer in enumerate(self):\n",
        "            # x, h = layer(x, hid[i] if hid!=None else None)\n",
        "            x, h = layer(x)\n",
        "            # hidden.append(h)\n",
        "        return x, h\n",
        "        # return x, hidden\n",
        "        # return x, torch.stack(hidden)\n",
        "    def step(self, x, hid=None):\n",
        "        hidden = []\n",
        "        for i, layer in enumerate(self):\n",
        "            x, h = layer.step(x, hid[i] if hid!=None else None)\n",
        "            hidden.append(h)\n",
        "        return x, hidden\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, in_dim, d_model=64, out_dim=None, n_heads=8, n_layers=1, ff_dim=256, dropout=0):\n",
        "        super().__init__()\n",
        "        self.d_model, self.n_layers = d_model, n_layers\n",
        "        self.tok_emb = nn.Embedding(in_dim+1, d_model)\n",
        "        self.rnn = Seq(*[Mamba2Simple(d_model, d_model) for _ in range(n_layers)])\n",
        "        # self.rnn = Seq(*[Mamba2(d_model) for _ in range(n_layers)])\n",
        "\n",
        "        # self.out = nn.Linear(d_model, out_dim or in_dim)\n",
        "        # self.out = lambda x: x @ self.tok_emb.weight().T  # weight tying\n",
        "        self.out = lambda x: x @ self.tok_emb.weight[:-1].T # [vocab+1,d]->[d,vocab] # weight tying for nn.Embedding\n",
        "        emb_std = d_model**-.5\n",
        "        nn.init.trunc_normal_(self.tok_emb.weight, mean=0, std=emb_std, a=-3*emb_std, b=3*emb_std) # https://github.com/facebookresearch/blt/blob/main/bytelatent/model/local_models.py#L136\n",
        "\n",
        "    def forward(self, x, hid=None): # [b,t], [n_lyr,t,b,d]\n",
        "        x = self.tok_emb(x) # [b,t,d]\n",
        "        # x, hid = self.rnn(x, hid) # [b,t,d], list[b,t,t]\n",
        "        x, hid = self.rnn(x) # [b,t,d], list[b,t,t]\n",
        "        # x, hid = self.out(x), torch.stack(hid)\n",
        "        x = self.out(x)\n",
        "        return x, hid # [b,t,d], [n_lyr,t,b,d]\n",
        "\n",
        "    def step(self, x, hid=None): # [b,t], [n_lyr,t,b,d]\n",
        "        x = self.tok_emb(x) # [b,t,d]\n",
        "        x, hid = self.rnn.step(x, hid) # [b,t,d], list[b,t,t]\n",
        "        # x, hid = self.out(x), torch.stack(hid)\n",
        "        x = self.out(x)\n",
        "        return x, hid # [b,t,d], [n_lyr,t,b,d]\n",
        "\n",
        "# gpt 2\n",
        "# Parameters Layers dmodel\n",
        "# 117M 12 768 gpt1\n",
        "# 345M 24 1024\n",
        "# 762M 36 1280\n",
        "# 1542M 48 1600\n",
        "\n",
        "\n",
        "try: vocab_size=train_loader.dataset.vocab_size#50\n",
        "except NameError: vocab_size=50\n",
        "# d1024 12lyr h16, 768 8 12, 512 6 8, 256 4 8, , 64 1 8\n",
        "# 768 16 12, 768 12 18\n",
        "# model = GPT(vocab_size, d_model=512, out_dim=vocab_size, n_layers=3, n_heads=16).to(device)\n",
        "model = GPT(vocab_size, d_model=64, out_dim=vocab_size, n_layers=2, n_heads=8).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "# optim = torch.optim.AdamW(model.parameters(), 1e-3) # og betas=(0.9, 0.999), wd.01\n",
        "optim = torch.optim.AdamW(model.parameters(), 1e-3, betas=(0.9, 0.95), weight_decay=1e-1) # og wd.01\n",
        "# optim = torch.optim.AdamW(model.parameters(), 1e-3, weight_decay=1e-1) # og wd.01\n",
        "\n",
        "# gradclip important\n",
        "# betas, scheduler helps\n",
        "# diff num_heads not much diff, ~h_dim32 is good\n",
        "# tokemb init helps significantly\n",
        "# noinit init.02 < stableinit?\n",
        "\n",
        "\n",
        "x = torch.randint(0, vocab_size, (4, 128), device=device)\n",
        "out, hid = model(x)\n",
        "print(out.shape)\n",
        "out, hid = model(x, hid)\n",
        "print(out.shape)\n",
        "# print(out.shape, hid.shape)\n",
        "# print(out)\n",
        "\n",
        "# 0 time: 0.4052731990814209 0.4052739143371582\n",
        "# strain 4.823487281799316\n",
        "# this is whatr ÔøΩÔøΩx÷ábÔøΩ`ÔøΩÔøΩÔøΩ›õ?ÔøΩ6ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0007q/ÔøΩNÔøΩ2mÔøΩvaÔøΩpoXtplw[w%ÔøΩoÔøΩ«îÔøΩTBÔøΩAÔøΩÔøΩXÔøΩ\n",
        "# 100 time: 42.69579267501831 0.4301547909727191\n",
        "# strain 2.498152017593384\n",
        "# this is what eÔøΩsÔøΩkgp eunt.b etn  tthaemsu sd,  atlee 'l'i nwaatc,i loers etn\n",
        "# 200 time: 42.66009211540222 0.43077912378073924\n",
        "# strain 2.3037750720977783\n",
        "# this is what  baorveerd  Doirle rwea lwahbee  Trhoipsuedn,e nHalrs,s  tnlaei\n",
        "# 300 time: 42.13650465011597 0.42872389368836666\n",
        "# strain 2.261747121810913\n",
        "# this is whati obunt ,craotns ibnt  SAepraecnuatl lbaei ntuoen  itfhoimr utm\n",
        "# 400 time: 42.29053235054016 0.42815912275242984\n",
        "# strain 2.256495237350464\n",
        "# this is whatk eCn aant e/snoofn oIfttees  wihaetc tfhee dCeimnitna ism iasgt\n",
        "# 500 time: 42.19914698600769 0.4275917968826142\n",
        "# strain 2.033019781112671\n",
        "# this is what. Ah!e ceomoerl yioen tshialr  hfoew  oWf ihsytnea loomn ePnc ot\n",
        "\n",
        "\n",
        "\n",
        "# strain 5.660660743713379\n",
        "# this is what we've done it. I'm able to be something strong. I think their information can be highly specific to a busy intertwined. But why \"Effects\"/ who shouldn't do,\" suppose it would improve them.Earlier open in both countries is sent by documentation, who were involved by custom ways to cutting the carbon gas\n",
        "# strain 5.939525604248047\n",
        "# this is what I‚Äôd like he‚Äôs very important to learn, it‚Äôs a bug my liquid had nothing once his streaming user in their own. It‚Äôs enough to run the survey, if there‚Äôs also no very big interested in a very $2. It‚Äôs sufficient's\n",
        "# strain 5.96616792678833\n",
        "# this is what manyatform are neither even documented and looked down their intentions in writers in so far savigTS-ve nuite was never known that some dioxide the sizes celebrate was fighting, as guilty or innovation.672 Expressia lives to Islamic Cheney of your existence takes a free background rest: The book she probably read it or find\n",
        "# strain 5.811507225036621\n",
        "# this is what it has not seen in intelligence and have believed in a romantic order and possible and rapidly now. Many growing economic ear is unclear whether they can do that the country's job. According to a proposals, the majority of domestic trade on which are the greatest mission with rich law by people, violated stockrol, which they would\n",
        "\n",
        "\n",
        "# b64,l128 8.0ram,10.0gpu l128*2:oom\n",
        "\n",
        "# b64,l128 w64 8.2ram,6.8gpu\n",
        "# b64,l128*2 w64 8.1ram,14.0gpu\n",
        "\n",
        "# swa64 128*2 7.4,12.0\n",
        "\n",
        "# d256 6.9,6.7\n",
        "\n",
        "# dim128 3lyr 3.0,.7\n",
        "# this is whats more of ietre'soni, they asari, your very\n",
        "# they vate. This iat\n",
        "# 9600 time: 5.7726781368255615 0.0006012580827081866\n",
        "# strain 1.481871247291565\n",
        "# this is what line wasn't good to both feel\n",
        "# like a delieve able or so foll,\n",
        "\n",
        "# mha\n",
        "# seq128*4+1 3.0,4.3\n",
        "# seq128*8+1 oom\n",
        "\n",
        "\n",
        "# seq128*8+1 w64:oom w32:2.4,7.8\n",
        "\n",
        "# lru 31s, strain 2\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxQWouWCvzmk",
        "outputId": "47231453-5065-4e91-8ab2-b8e84324db59"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "184176\n",
            "torch.Size([4, 128, 256])\n",
            "torch.Size([4, 128, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "2Nd-sGe6Ku4S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "fdb710fc-8954-4f3d-8ac9-2281ac098ef0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbobdole\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.23.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251213_103949-heg0kzq3</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/gpt/runs/heg0kzq3' target=\"_blank\">soft-dawn-129</a></strong> to <a href='https://wandb.ai/bobdole/gpt' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/gpt' target=\"_blank\">https://wandb.ai/bobdole/gpt</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/gpt/runs/heg0kzq3' target=\"_blank\">https://wandb.ai/bobdole/gpt/runs/heg0kzq3</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "!pip install -q wandb\n",
        "import wandb # https://docs.wandb.ai/quickstart\n",
        "wandb.login(key='487a2109e55dce4e13fc70681781de9f50f27be7')\n",
        "try: run.finish()\n",
        "except NameError: pass\n",
        "run = wandb.init(project=\"gpt\", config={\"model\": \"mamba\",}) #"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title train generate\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "# https://www.comet.com/site/blog/perplexity-for-llm-evaluation/\n",
        "def Perplexity(logits, target): # [b,t,vocab_size], [b,t]\n",
        "    log_probs = F.log_softmax(logits, dim=-1)\n",
        "    nll = -log_probs.gather(dim=-1, index=target.unsqueeze(-1)).squeeze(-1) # [b,t]\n",
        "    perplexity = nll.mean().exp()\n",
        "    return perplexity\n",
        "\n",
        "import time\n",
        "def strain(model, dataloader, optim, scheduler=None): # train function with automatic mixed precision\n",
        "    start = begin = time.time()\n",
        "    model.train()\n",
        "    for i, x in enumerate(dataloader):\n",
        "        x, y = x[:,:-1].to(device), x[:,1:].to(device)\n",
        "        # with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
        "        logits, _ = model(x) #output = [batch size, trg len - 1, output dim]\n",
        "        loss = F.cross_entropy(logits.flatten(0,1), y.flatten().to(int)) # [b*t,d], [b*t]\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        # scaler.scale(loss).backward()\n",
        "        # scaler.unscale_(optimizer)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
        "        # scaler.step(optimizer)\n",
        "        # scaler.update()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "        if i % 100 == 0:\n",
        "            print(i, 'time:',time.time() - start, (time.time()-begin)/(i+1))\n",
        "            print(\"strain\",loss.item())\n",
        "            print(generate(model, \"this is what\"))\n",
        "            model.train()\n",
        "            # perplexity = Perplexity(logits.detach(), y).item()\n",
        "            start = time.time()\n",
        "        try: wandb.log({\"train loss\": loss.item()/len(y)})\n",
        "        except NameError: pass\n",
        "\n",
        "def generate(model, context, max_steps=64, temperature=1):\n",
        "    x = encode(context)#.to(device)\n",
        "    model.eval()\n",
        "    hid=None\n",
        "    for n in range(max_steps):\n",
        "        # with torch.no_grad(): out, hid = model(x, hid)\n",
        "        # with torch.no_grad(): out, hid = model(x if hid==None else x[:,:-1], hid)\n",
        "        with torch.no_grad():\n",
        "            out, hid = model.step(x, hid)\n",
        "            # out, hid = model(x, hid)\n",
        "        # print('generate', output.shape, hidden.shape)\n",
        "        # print('generate', out.shape)\n",
        "        # hid = hid[:,-1].unsqueeze(1) # RNN/GRU\n",
        "        out = out[:,-1] # get logit for last character\n",
        "        out = F.softmax(out/temperature, dim=-1) # vocab_size to char\n",
        "        ix = torch.multinomial(out, num_samples=1) # rand sample by output distribution\n",
        "        x = torch.cat([x, ix], dim=1)\n",
        "    completion = decode(x.squeeze(0))\n",
        "    return completion\n",
        "\n",
        "\n",
        "# scheduler = get_cosine_schedule_with_warmup(optim, num_warmup_steps=400, num_training_steps=40000) # https://docs.pytorch.org/torchtune/0.2/generated/torchtune.modules.get_cosine_schedule_with_warmup.html\n",
        "# import time\n",
        "# start = begin = time.time()\n",
        "for i in range(1):\n",
        "    # train_loss = strain(model, train_loader, optim, scheduler=None)\n",
        "    strain(model, train_loader, optim, scheduler=None)\n",
        "    # strain(model, train_loader, optim, scheduler=scheduler)\n",
        "    # print(generate(model, \"this is what\"))\n",
        "    # print(i, 'time:',time.time() - start, (time.time()-begin)/(i+1))\n",
        "    # start = time.time()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrpJDgB3w49H",
        "outputId": "4ae0f28c-4539-4918-ebd4-e12e4326b815",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2860904409.py:6: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  scaler = torch.GradScaler()\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 time: 0.49774813652038574 0.49774885177612305\n",
            "strain 5.7478532791137695\n",
            "this is whatÔøΩ~\u0013\u0006pÔøΩFÔøΩic'\u001a0 ÔøΩ\u0015ÔøΩÔøΩÔøΩd<ÔøΩÔøΩÔøΩ∆Ñ7pÔøΩbÔøΩ/8ÔøΩÔøΩÔøΩÔøΩ2ÔøΩtoJÔøΩÔøΩÔøΩÔøΩÔøΩ\bÔøΩÔøΩÔøΩ0ÔøΩ\u00122ÔøΩpÔøΩ?\u0007ÔøΩÔøΩÔøΩc\n",
            "100 time: 41.99482226371765 0.4220260794800107\n",
            "strain 2.5869271755218506\n",
            "this is whatre iomee ampo ar dubusus t√õ't i'E yowian infuor al the wifc tre\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate(model, \"this is what\"))\n"
      ],
      "metadata": {
        "id": "ROjbICJjf3Ct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## mamba implementations"
      ],
      "metadata": {
        "id": "5XQqp-uYIMWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title from johnma2006/mamba-minimal\n",
        "# https://github.com/johnma2006/mamba-minimal/blob/master/model.py\n",
        "\"\"\"[1] Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Albert Gu and Tri Dao) https://arxiv.org/abs/2312.00752\n",
        "    [2] The Annotated S4 (Sasha Rush and Sidd Karamcheti) https://srush.github.io/annotated-s4\n",
        "    n or d_state: latent state dim      (`N` in [1] Algorithm 2)\n",
        "    d_in or d_inner: d * expansion factor         (`D` in [1] Algorithm 2) ; expansion factor (`E` in [1] Section 3.4)\n",
        "    A, B, C, D: state space parameters  (See any state space representation formula)\n",
        "                                        (B, C are input-dependent (aka selective, a key innovation in Mamba); A, D are not)\n",
        "    Œî or delta: input-dependent step size\n",
        "    dt_rank: rank of Œî                  (See [1] Section 3.6 \"Parameterization of ‚àÜ\")\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class Mamba(nn.Module):\n",
        "    def __init__(self, d_model=256, d_state=16, d_inner=512, n_layer=3):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([ResidualBlock(d_model, d_state, d_inner) for _ in range(n_layer)])\n",
        "        self.norm_f = nn.RMSNorm(d_model)\n",
        "\n",
        "    def forward(self, x): # (b, l, d)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        x = self.norm_f(x)\n",
        "        return x\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_state, d_inner):\n",
        "        super().__init__()\n",
        "        self.mixer = MambaBlock(d_model, d_state, d_inner)\n",
        "        self.norm = nn.RMSNorm(d_model)\n",
        "\n",
        "    def forward(self, x): # (b, l, d) # Block.forward(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py#L297\n",
        "        output = x + self.mixer(self.norm(x))\n",
        "        return output # (b, l, d)\n",
        "\n",
        "\n",
        "class MambaBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_state, d_inner, dt_rank=16):\n",
        "        super().__init__()\n",
        "        self.dt_rank = dt_rank\n",
        "        self.in_proj = nn.Linear(d_model, d_inner * 2, bias=False)\n",
        "        d_conv = 4\n",
        "        self.conv1d = nn.Conv1d(d_inner, d_inner, d_conv, padding=d_conv-1, groups=d_inner)\n",
        "        self.x_proj = nn.Linear(d_inner, dt_rank + 2*d_state, bias=False) # takes in `x` and outputs the input-specific Œî, B, C\n",
        "        self.dt_proj = nn.Linear(dt_rank, d_inner, bias=True) # project Œî from dt_rank to d_in\n",
        "        self.A_log = nn.Parameter(torch.log(torch.arange(1, d_state + 1).repeat(d_inner, 1)))\n",
        "        self.D = nn.Parameter(torch.ones(d_inner))\n",
        "        self.out_proj = nn.Linear(d_inner, d_model, bias=False)\n",
        "\n",
        "    def forward(self, x): # (b, l, d) # https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py#L119\n",
        "        b, l, d = x.shape\n",
        "        x_res = self.in_proj(x)  # (b, l, 2 * d_in)\n",
        "        x, res = torch.chunk(x_res, 2, dim=-1) # [b, l, d_in]\n",
        "        x = self.conv1d(x.permute(0, 2, 1))[:, :, :l].permute(0, 2, 1) # (b, l, d_in)\n",
        "        y = self.ssm(F.silu(x)) * F.silu(res)\n",
        "        output = self.out_proj(y)\n",
        "        return output # (b, l, d)\n",
        "\n",
        "    def ssm(self, x): # (b, l, d_in)\n",
        "        \"\"\"- Algorithm 2 in Section 3.2 in the Mamba paper [1] - run_SSM(A, B, C, u) in The Annotated S4 [2]\n",
        "        Official Implementation: mamba_inner_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L311\"\"\"\n",
        "        d_in, n = self.A_log.shape\n",
        "        A = -torch.exp(self.A_log.float())  # shape (d_in, n)\n",
        "        # D = self.D.float()\n",
        "\n",
        "        # 2: B : (B, L, N) ‚Üê sB (x)\n",
        "        # 3: C : (B, L, N) ‚Üê sC (x)\n",
        "        # 4: Œî : (B, L, D) ‚Üê ùúèŒî(Parameter+ùë†Œî(x))\n",
        "        # 5: A, B : (B, L, D, N) ‚Üê discretize(Œî, A, B)\n",
        "        # 6: y ‚Üê SSM(A, B, C) (x)\n",
        "\n",
        "        delta, B, C = self.x_proj(x).split([self.dt_rank, n, n], dim=-1) # [b,l,dt_rank] [b,l,n], [b,l,n]\n",
        "        delta = F.softplus(self.dt_proj(delta)) # (b, l, d_in)\n",
        "        y = selective_scan(x, delta, A, B, C, self.D)  # This is similar to run_SSM(A, B, C, u) in The Annotated S4 [2]\n",
        "        return y # (b, l, d_in)\n",
        "\n",
        "def selective_scan(u, delta, A, B, C, D): # u: (b, l, d_in); delta: (b, l, d_in); A: (d_in, n); B: (b, l, n); C: (b, l, n); D: (d_in)\n",
        "    \"\"\"- Section 2 State Space Models in the Mamba paper [1] - Algorithm 2 in Section 3.2 in the Mamba paper [1] - run_SSM(A, B, C, u) in The Annotated S4 [2]\n",
        "    x(t+1) = Ax(t) + Bu(t)\n",
        "    y(t)   = Cx(t) + Du(t)\n",
        "    except B and C (and the step size delta, which is used for discretization) are dependent on the input x(t).\n",
        "\n",
        "    Official Implementation: selective_scan_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L86\n",
        "    Note: I refactored some parts out of `selective_scan_ref` out, so the functionality doesn't match exactly.\"\"\"\n",
        "    b, l, d_in = u.shape\n",
        "    n = A.shape[1]\n",
        "\n",
        "    # Discretize continuous parameters (A, B)\n",
        "    deltaA = torch.exp(torch.einsum('bld,dn->bldn', delta, A)) # [b l d_in n] # - A is discretized using zero-order hold (ZOH) discretization (see Section 2 Equation 4 in the Mamba paper [1])\n",
        "    deltaB_u = torch.einsum('bld,bln,bld->bldn', delta, B, u) # [b l d_in n] # - B is discretized using a simplified Euler discretization instead of ZOH. From a discussion with authors: \"A is the more important term and the performance doesn't change much with the simplification on B\"\n",
        "\n",
        "    # selective scan (scan_SSM() in The Annotated S4 [2])\n",
        "    # Note that the below is sequential, while the official implementation does a much faster parallel scan that is additionally hardware-aware (like FlashAttention).\n",
        "    x = torch.zeros((b, d_in, n), device=device)\n",
        "    ys = []\n",
        "    for i in range(l):\n",
        "        x = deltaA[:, i] * x + deltaB_u[:, i]\n",
        "        y = torch.einsum('bdn,bn->bd', x, C[:, i, :]) # b d_in\n",
        "        ys.append(y)\n",
        "    y = torch.stack(ys, dim=1)  # shape (b, l, d_in)\n",
        "    y = y + u * D\n",
        "    return y # (b, l, d_in)\n",
        "\n",
        "\n",
        "pred = Mamba()\n",
        "\n",
        "batch=4\n",
        "seq_len=500\n",
        "d_model=256\n",
        "x = torch.randn(batch, seq_len, d_model)\n",
        "y = pred(x)\n",
        "print(x.shape, y.shape)\n",
        "# print(x)\n",
        "# print(y)\n"
      ],
      "metadata": {
        "id": "-mrujusYX9Cc",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title tommyip/mamba2-minimal\n",
        "# https://github.com/tommyip/mamba2-minimal/blob/main/mamba2.py\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "from dataclasses import dataclass\n",
        "@dataclass\n",
        "class Mamba2Config:\n",
        "    d_model =256 # model dimension (D)\n",
        "    n_layer = 2  # number of Mamba-2 layers in the language model\n",
        "    d_state = 256  # state dimension (N)\n",
        "    d_conv = 4  # convolution kernel size\n",
        "    headdim = 64  # head dimension (P)\n",
        "    chunk_size = 8  # matrix partition size (Q)\n",
        "    d_inner = 2 * d_model # expansion factor (E) * d_model\n",
        "    nheads = d_inner // headdim\n",
        "\n",
        "from torch import Tensor\n",
        "from typing import NamedTuple, cast\n",
        "class InferenceCache(NamedTuple):\n",
        "    conv_state: Tensor  # (batch, d_inner + 2 * d_state, d_conv)\n",
        "    ssm_state: Tensor  # (batch, nheads, headdim, d_state)\n",
        "\n",
        "    @staticmethod\n",
        "    def alloc(batch_size, args: Mamba2Config, device = None):\n",
        "        return InferenceCache(torch.zeros(batch_size, args.d_inner + 2 * args.d_state, args.d_conv, device=device),\n",
        "            torch.zeros(batch_size, args.nheads, args.headdim, args.d_state, device=device),)\n",
        "\n",
        "\n",
        "class Mamba2LMHeadModel(nn.Module):\n",
        "    def __init__(self, args: Mamba2Config, device = None):\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "        self.device = device\n",
        "        self.backbone = nn.ModuleDict(dict(\n",
        "                layers=nn.ModuleList(\n",
        "                    [nn.ModuleDict(dict(mixer=Mamba2(args, device=device), norm=nn.RMSNorm(args.d_model, device=device),))\n",
        "                        for _ in range(args.n_layer)]),\n",
        "                norm_f=nn.RMSNorm(args.d_model, device=device),\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def forward(self, x, h: list[InferenceCache] | list[None] | None = None): # (batch, seqlen, d_model)\n",
        "        \"\"\"h: hidden states for inference step. If present the constant-time\n",
        "               (wrt sequence length) inference path will be taken, input_ids\n",
        "               should have shape (batch, 1) containing the next batch of prompt token.\"\"\"\n",
        "        seqlen = x.shape[1]\n",
        "        if h is None: h = [None for _ in range(self.args.n_layer)]\n",
        "        for i, layer in enumerate(self.backbone.layers):\n",
        "            y, h[i] = layer.mixer(layer.norm(x), h[i])\n",
        "            x = y + x\n",
        "        x = self.backbone.norm_f(x)\n",
        "        return x[:, :seqlen], cast(list[InferenceCache], h) # (batch, seqlen, d_model)\n",
        "\n",
        "\n",
        "class Mamba2(nn.Module):\n",
        "    def __init__(self, args: Mamba2Config, device = None):\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "        self.device = device\n",
        "        # Order: (z, x, B, C, dt)\n",
        "        d_in_proj = 2 * args.d_inner + 2 * args.d_state + args.nheads\n",
        "        self.in_proj = nn.Linear(args.d_model, d_in_proj, bias=False, device=device)\n",
        "\n",
        "        conv_dim = args.d_inner + 2 * args.d_state\n",
        "        self.conv1d = nn.Conv1d(in_channels=conv_dim, out_channels=conv_dim, kernel_size=args.d_conv,\n",
        "            groups=conv_dim, padding=args.d_conv-1, device=device,)\n",
        "\n",
        "        self.dt_bias = nn.Parameter(torch.empty(args.nheads, device=device))\n",
        "        self.A_log = nn.Parameter(torch.empty(args.nheads, device=device))\n",
        "        self.D = nn.Parameter(torch.empty(args.nheads, device=device))\n",
        "        self.norm = nn.RMSNorm(args.d_inner, device=device)\n",
        "        self.out_proj = nn.Linear(args.d_inner, args.d_model, bias=False, device=device)\n",
        "\n",
        "    def forward(self, u, h=None): # (batch, seqlen, d_model)\n",
        "        if h: return self.step(u, h)\n",
        "\n",
        "        A = -torch.exp(self.A_log)  # (nheads,)\n",
        "        zxbcdt = self.in_proj(u)  # (batch, seqlen, d_in_proj)\n",
        "        z, xBC, dt = torch.split(zxbcdt, [self.args.d_inner, self.args.d_inner + 2 * self.args.d_state, self.args.nheads,], dim=-1,)\n",
        "        dt = F.softplus(dt + self.dt_bias)  # (batch, seqlen, nheads)\n",
        "\n",
        "        # Pad or truncate xBC seqlen to d_conv\n",
        "        conv_state = F.pad(rearrange(xBC, \"b l d -> b d l\"), (self.args.d_conv - u.shape[1], 0))\n",
        "\n",
        "        xBC = F.silu(self.conv1d(xBC.transpose(1, 2)).transpose(1, 2)[:, : u.shape[1], :])  # (batch, seqlen, d_inner + 2 * d_state))\n",
        "        x, B, C = torch.split(xBC, [self.args.d_inner, self.args.d_state, self.args.d_state], dim=-1)\n",
        "        x = rearrange(x, \"b l (h p) -> b l h p\", p=self.args.headdim)\n",
        "        y, ssm_state = ssd(x * dt.unsqueeze(-1), A * dt,\n",
        "            rearrange(B, \"b l n -> b l 1 n\"), rearrange(C, \"b l n -> b l 1 n\"),\n",
        "            self.args.chunk_size, device=self.device,)\n",
        "        y = y + x * self.D.unsqueeze(-1)\n",
        "        y = rearrange(y, \"b l h p -> b l (h p)\")\n",
        "        # y = self.norm(y, z)\n",
        "        y = self.norm(y * F.silu(z))\n",
        "        y = self.out_proj(y)\n",
        "        h = InferenceCache(conv_state, ssm_state)\n",
        "        return y, h # (batch, seqlen, d_model)\n",
        "\n",
        "    def step(self, u, h): # (batch, 1, d_model)\n",
        "        \"\"\"Take a single inference step for the current input and hidden state\n",
        "\n",
        "        Unlike attention-based models, RNN-based models (eg Mamba) does not need\n",
        "        to look back at all the past tokens to generate a new token. Instead a\n",
        "        hidden state (initialized to 0s initially) is updated for each input and\n",
        "        passed to the next inference step. This means that the total inference\n",
        "        time is linear with respect to the sequence length instead of quadratic\n",
        "        in attention's case.\"\"\"\n",
        "        assert u.shape[1] == 1, \"Only one token can be decoded per inference step\"\n",
        "\n",
        "        zxbcdt = self.in_proj(u.squeeze(1))  # (batch, d_in_proj)\n",
        "        z, xBC, dt = torch.split(zxbcdt, [self.args.d_inner, self.args.d_inner + 2 * self.args.d_state, self.args.nheads,], dim=-1,)\n",
        "\n",
        "        # Advance convolution input\n",
        "        h.conv_state.copy_(torch.roll(h.conv_state, shifts=-1, dims=-1))\n",
        "        h.conv_state[:, :, -1] = xBC\n",
        "        # Convolution step\n",
        "        xBC = torch.sum(h.conv_state * rearrange(self.conv1d.weight, \"d 1 w -> d w\"), dim=-1)\n",
        "        xBC += self.conv1d.bias\n",
        "        xBC = F.silu(xBC)\n",
        "\n",
        "        x, B, C = torch.split(xBC, [self.args.d_inner, self.args.d_state, self.args.d_state], dim=-1)\n",
        "        A = -torch.exp(self.A_log)  # (nheads,)\n",
        "\n",
        "        # SSM step\n",
        "        dt = F.softplus(dt + self.dt_bias)  # (batch, nheads)\n",
        "        dA = torch.exp(dt * A)  # (batch, nheads)\n",
        "        x = rearrange(x, \"b (h p) -> b h p\", p=self.args.headdim)\n",
        "        dBx = torch.einsum(\"bh, bn, bhp -> bhpn\", dt, B, x)\n",
        "        h.ssm_state.copy_(h.ssm_state * rearrange(dA, \"b h -> b h 1 1\") + dBx)\n",
        "        y = torch.einsum(\"bhpn, bn -> bhp\", h.ssm_state, C)\n",
        "        y = y + rearrange(self.D, \"h -> h 1\") * x\n",
        "        y = rearrange(y, \"b h p -> b (h p)\")\n",
        "        # y = self.norm(y, z)\n",
        "        y = self.norm(y * F.silu(z))\n",
        "        y = self.out_proj(y)\n",
        "        return y.unsqueeze(1), h # (batch, 1, d_model)\n",
        "\n",
        "\n",
        "def segsum(x, device = None):\n",
        "    \"\"\"Stable segment sum calculation.\n",
        "    `exp(segsum(A))` produces a 1-semiseparable matrix, which is equivalent to a scalar SSM.\n",
        "    Source: https://github.com/state-spaces/mamba/blob/219f03c840d5a44e7d42e4e728134834fddccf45/mamba_ssm/modules/ssd_minimal.py#L23-L32\"\"\"\n",
        "    T = x.size(-1)\n",
        "    x = repeat(x, \"... d -> ... d e\", e=T)\n",
        "    mask = torch.tril(torch.ones(T, T, dtype=torch.bool, device=device), diagonal=-1)\n",
        "    x = x.masked_fill(~mask, 0)\n",
        "    x_segsum = torch.cumsum(x, dim=-2)\n",
        "    mask = torch.tril(torch.ones(T, T, dtype=torch.bool, device=device), diagonal=0)\n",
        "    x_segsum = x_segsum.masked_fill(~mask, -torch.inf)\n",
        "    return x_segsum\n",
        "\n",
        "\n",
        "def ssd(x, A, B, C, chunk_size, initial_states=None, device = None):\n",
        "    \"\"\"Structed State Space Duality (SSD) - the core of Mamba-2\n",
        "    This is almost the exact same minimal SSD code from the blog post.\n",
        "    Arguments\n",
        "        x: (batch, seqlen, n_heads, d_head)\n",
        "        A: (batch, seqlen, n_heads)\n",
        "        B: (batch, seqlen, n_heads, d_state)\n",
        "        C: (batch, seqlen, n_heads, d_state)\n",
        "    Return y: (batch, seqlen, n_heads, d_head)\n",
        "\n",
        "    Source\n",
        "     1. https://tridao.me/blog/2024/mamba2-part3-algorithm/\n",
        "     2. https://github.com/state-spaces/mamba/blob/219f03c840d5a44e7d42e4e728134834fddccf45/mamba_ssm/modules/ssd_minimal.py#L34-L78\n",
        "    \"\"\"\n",
        "    assert x.shape[1] % chunk_size == 0\n",
        "\n",
        "    # Rearrange into chunks\n",
        "    # Step 1, 2 and 4 of SSD can be computed in parallel for each chunk across devices (sequence parallel)\n",
        "    # This is not implemented and left as an exercise for the reader üòú\n",
        "    x, A, B, C = [rearrange(m, \"b (c l) ... -> b c l ...\", l=chunk_size) for m in (x, A, B, C)]\n",
        "\n",
        "    A = rearrange(A, \"b c l h -> b h c l\")\n",
        "    A_cumsum = torch.cumsum(A, dim=-1)\n",
        "\n",
        "    # 1. Compute the output for each intra-chunk (diagonal blocks)\n",
        "    L = torch.exp(segsum(A, device=device))\n",
        "    Y_diag = torch.einsum(\"bclhn, bcshn, bhcls, bcshp -> bclhp\", C, B, L, x)\n",
        "\n",
        "    # 2. Compute the state for each intra-chunk\n",
        "    # (right term of low-rank factorization of off-diagonal blocks; B terms)\n",
        "    decay_states = torch.exp(A_cumsum[:, :, :, -1:] - A_cumsum)\n",
        "    states = torch.einsum(\"bclhn, bhcl, bclhp -> bchpn\", B, decay_states, x)\n",
        "\n",
        "    # 3. Compute the inter-chunk SSM recurrence; produces correct SSM states at chunk boundaries\n",
        "    # (middle term of factorization of off-diag blocks; A terms)\n",
        "    if initial_states is None:\n",
        "        initial_states = torch.zeros_like(states[:, :1])\n",
        "    states = torch.cat([initial_states, states], dim=1)\n",
        "    decay_chunk = torch.exp(segsum(F.pad(A_cumsum[:, :, :, -1], (1, 0)), device=device))\n",
        "    new_states = torch.einsum(\"bhzc, bchpn -> bzhpn\", decay_chunk, states)\n",
        "    states, final_state = new_states[:, :-1], new_states[:, -1]\n",
        "\n",
        "    # 4. Compute state -> output conversion per chunk\n",
        "    # (left term of low-rank factorization of off-diagonal blocks; C terms)\n",
        "    state_decay_out = torch.exp(A_cumsum)\n",
        "    Y_off = torch.einsum(\"bclhn, bchpn, bhcl -> bclhp\", C, states, state_decay_out)\n",
        "\n",
        "    # Add output of intra-chunk and inter-chunk terms (diagonal and off-diagonal blocks)\n",
        "    Y = rearrange(Y_diag + Y_off, \"b c l h p -> b (c l) h p\")\n",
        "    return Y, final_state\n",
        "\n",
        "\n",
        "batch=4\n",
        "seq_len=16\n",
        "d_model=256\n",
        "\n",
        "pred = Mamba2(Mamba2Config)\n",
        "# pred = Mamba2LMHeadModel(Mamba2Config)\n",
        "\n",
        "x = torch.randn(batch, seq_len, d_model)\n",
        "y, h = pred(x)\n",
        "print(x)\n",
        "print(y)\n",
        "print(x.shape)\n",
        "print(y.shape)\n",
        "\n",
        "x = torch.randn(batch, 1, d_model)\n",
        "y, h = pred(x,h)\n",
        "\n",
        "print(y.shape)\n",
        "print(y)\n",
        "\n"
      ],
      "metadata": {
        "id": "DYrzogiIXLa_",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "307c8c51-8a08-4177-e684-a47c09c1697b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 1.3794e+00, -2.2004e-01,  1.8698e+00,  ...,  1.1831e+00,\n",
            "          -1.5262e+00, -4.6126e-02],\n",
            "         [-1.6847e+00, -8.0931e-01, -4.6936e-01,  ..., -3.0221e-01,\n",
            "           1.9958e+00,  1.1328e+00],\n",
            "         [-3.6427e-01, -3.2403e+00,  4.6751e-01,  ...,  7.1706e-01,\n",
            "           4.8731e-01, -5.2911e-01],\n",
            "         ...,\n",
            "         [ 8.8814e-01, -2.0862e-02, -6.0619e-01,  ..., -2.0147e+00,\n",
            "           1.2703e+00,  3.1025e-01],\n",
            "         [-5.6573e-01, -6.5905e-01,  3.0807e-01,  ..., -8.9768e-01,\n",
            "           2.1040e+00,  7.7190e-01],\n",
            "         [ 4.6106e-01, -2.5430e-01, -3.9951e-01,  ..., -1.9734e+00,\n",
            "          -9.8242e-01, -1.1092e+00]],\n",
            "\n",
            "        [[-7.0688e-01, -9.7973e-02, -6.8351e-01,  ..., -3.0498e-01,\n",
            "           3.5882e-02,  6.3546e-01],\n",
            "         [-1.3511e+00,  1.8292e+00,  9.6028e-01,  ..., -3.9836e-01,\n",
            "           4.7736e-02, -1.7731e+00],\n",
            "         [ 7.9752e-02,  1.5227e+00, -2.6314e-01,  ..., -2.2527e-01,\n",
            "          -3.3489e-01, -3.5680e-01],\n",
            "         ...,\n",
            "         [ 3.6340e-02, -5.7734e-01,  7.7398e-01,  ...,  6.5252e-01,\n",
            "           1.1960e+00,  6.0507e-01],\n",
            "         [-1.1861e+00,  4.0614e-01, -8.6865e-01,  ...,  2.2291e+00,\n",
            "           9.0778e-01, -2.2200e-01],\n",
            "         [-6.8012e-01,  1.3888e-01, -3.0032e-01,  ..., -9.2396e-02,\n",
            "          -1.1353e+00,  1.3843e+00]],\n",
            "\n",
            "        [[-1.7881e-01, -1.7631e+00,  2.2683e+00,  ..., -1.2020e+00,\n",
            "           4.6164e-01, -1.1835e+00],\n",
            "         [-2.0655e-01,  2.2461e-01,  1.9524e+00,  ...,  1.5176e+00,\n",
            "           1.5496e-03, -5.4335e-01],\n",
            "         [ 1.0197e+00,  4.9153e-01,  1.4347e+00,  ..., -9.3056e-01,\n",
            "           2.8896e-01, -9.5743e-01],\n",
            "         ...,\n",
            "         [-1.0233e+00,  3.7868e-01,  1.4277e+00,  ..., -1.4231e+00,\n",
            "          -8.7336e-01,  9.6587e-01],\n",
            "         [ 6.6944e-02,  1.4859e+00,  1.1622e+00,  ..., -1.2448e+00,\n",
            "          -1.9259e-01,  1.6348e+00],\n",
            "         [ 4.8028e-01, -1.9150e+00, -6.7469e-01,  ...,  7.8804e-01,\n",
            "           9.4096e-01,  3.6353e-01]],\n",
            "\n",
            "        [[ 9.9405e-01,  5.1399e-01, -7.0820e-01,  ..., -2.2699e-01,\n",
            "          -1.3832e+00,  2.1719e-01],\n",
            "         [ 1.0935e-02,  4.5707e-01,  1.8520e+00,  ..., -3.1327e-02,\n",
            "           1.3159e+00,  9.5234e-01],\n",
            "         [-9.3364e-01,  2.2672e+00, -2.5827e-01,  ...,  7.6231e-01,\n",
            "           4.8079e-01,  4.7181e-01],\n",
            "         ...,\n",
            "         [-1.5817e+00, -2.1381e-01, -7.2086e-01,  ...,  8.1007e-02,\n",
            "          -4.6445e-01, -1.7037e+00],\n",
            "         [-5.2520e-01,  3.6718e-01, -6.6015e-01,  ...,  1.4435e+00,\n",
            "           3.4328e-01,  9.3985e-01],\n",
            "         [-5.4148e-01,  4.0485e-01, -2.6153e-01,  ...,  2.0214e-01,\n",
            "          -1.0646e-01, -6.4157e-02]]])\n",
            "tensor([[[-4.1793e-01, -2.4930e-01, -6.4848e-01,  ...,  1.6638e-01,\n",
            "          -1.7741e-01, -5.5611e-01],\n",
            "         [ 3.2666e-01, -4.2404e-01,  1.0724e+00,  ..., -3.0128e-01,\n",
            "          -7.4610e-01,  7.4032e-01],\n",
            "         [ 5.1225e-01, -2.5148e-01,  3.7311e-01,  ...,  3.5663e-01,\n",
            "           3.1506e-01,  6.7444e-01],\n",
            "         ...,\n",
            "         [ 4.3829e-01,  8.7302e-01, -5.1092e-01,  ..., -3.3563e-02,\n",
            "          -6.7601e-01, -1.6857e-01],\n",
            "         [ 7.4552e-01, -1.1317e+00,  4.9769e-01,  ..., -3.6681e-01,\n",
            "          -1.4057e+00,  1.0645e+00],\n",
            "         [ 1.1470e+00, -4.0134e-01,  1.6492e-01,  ...,  7.8736e-01,\n",
            "           7.5628e-01, -5.0885e-01]],\n",
            "\n",
            "        [[ 6.6760e-01, -5.4994e-01,  4.3527e-01,  ...,  2.7237e-01,\n",
            "          -3.8397e-01,  7.4997e-01],\n",
            "         [ 4.3668e-02,  1.7461e-02,  3.2605e-01,  ..., -1.2415e+00,\n",
            "           1.0342e+00, -8.2049e-01],\n",
            "         [ 6.1034e-01,  1.0801e+00,  1.0939e+00,  ...,  4.3116e-01,\n",
            "          -5.1408e-01,  3.7899e-02],\n",
            "         ...,\n",
            "         [-1.3771e-01, -6.6394e-01, -9.4889e-01,  ..., -3.0216e-02,\n",
            "          -1.9686e-01, -2.4796e-01],\n",
            "         [-6.7879e-01, -7.8513e-01, -6.2261e-02,  ..., -1.2398e+00,\n",
            "          -1.1624e+00,  4.9261e-01],\n",
            "         [-1.1108e+00,  2.9243e-01,  5.5270e-01,  ..., -5.9843e-03,\n",
            "          -4.7103e-01,  1.4382e-01]],\n",
            "\n",
            "        [[ 4.1032e-01,  3.4978e-01,  1.0480e-01,  ...,  4.1825e-01,\n",
            "          -3.4573e-01, -3.6406e-01],\n",
            "         [ 8.8534e-01, -1.6386e-01, -8.4892e-01,  ...,  4.7838e-01,\n",
            "          -4.5418e-01,  3.4382e-01],\n",
            "         [ 6.8974e-01,  6.9394e-01, -4.8662e-01,  ..., -6.8031e-01,\n",
            "          -6.2428e-01, -6.7923e-01],\n",
            "         ...,\n",
            "         [-2.4308e-01,  1.3757e-02, -7.0165e-01,  ...,  8.4308e-01,\n",
            "           1.0472e-01,  5.7966e-02],\n",
            "         [-3.3247e-01, -2.0228e-02,  8.2071e-01,  ..., -6.3404e-02,\n",
            "           2.5697e-01, -4.4950e-01],\n",
            "         [ 4.4295e-01,  2.6668e-01,  6.4946e-01,  ..., -7.2763e-01,\n",
            "          -2.4325e-01, -6.4802e-04]],\n",
            "\n",
            "        [[ 4.9550e-01, -1.0815e-01,  6.5054e-01,  ..., -1.0635e+00,\n",
            "           4.3476e-01, -3.2016e-01],\n",
            "         [-7.1980e-01, -7.2976e-02, -4.9266e-01,  ...,  1.0544e+00,\n",
            "           1.8464e-01,  8.4423e-01],\n",
            "         [ 6.6074e-01, -3.4633e-02,  2.5045e-01,  ..., -3.8421e-01,\n",
            "           7.1481e-01, -3.8104e-01],\n",
            "         ...,\n",
            "         [ 9.1557e-01,  2.1390e-01, -1.5750e-01,  ..., -5.1511e-01,\n",
            "          -5.2118e-01, -3.8080e-02],\n",
            "         [-5.5256e-01,  1.2183e+00, -1.0141e+00,  ..., -1.5734e-01,\n",
            "           2.3792e-01, -3.8228e-01],\n",
            "         [-4.2146e-01,  2.2951e-01,  1.7443e+00,  ...,  7.0675e-02,\n",
            "           8.3565e-01,  7.4428e-01]]], grad_fn=<UnsafeViewBackward0>)\n",
            "torch.Size([4, 16, 256])\n",
            "torch.Size([4, 16, 256])\n",
            "torch.Size([4, 1, 256])\n",
            "tensor([[[-0.6017, -0.7088,  0.4274,  ...,  0.0909, -0.6064,  0.3285]],\n",
            "\n",
            "        [[-1.3053, -0.3619, -0.1817,  ..., -0.7444, -0.1828, -0.3992]],\n",
            "\n",
            "        [[ 0.5130, -1.1946, -0.2355,  ...,  0.2832, -0.5587,  0.1612]],\n",
            "\n",
            "        [[-0.0669,  0.5684, -0.9630,  ..., -0.6946,  0.2435, -0.4298]]],\n",
            "       grad_fn=<UnsqueezeBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title alxndrTL mamba.py\n",
        "# https://github.com/alxndrTL/mamba.py/blob/main/mambapy/mamba.py\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from typing import Union\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from mambapy.pscan import pscan\n",
        "\n",
        "\"\"\" This file closely follows the mamba_simple.py from the official Mamba implementation, and the mamba-minimal by @johnma2006.\n",
        "The major differences are :\n",
        "-the convolution is done with torch.nn.Conv1d\n",
        "-the selective scan is done in PyTorch\n",
        "\n",
        "A sequential version of the selective scan is also available for comparison. Also, it is possible to use the official Mamba implementation.\n",
        "\n",
        "This is the structure of the torch modules :\n",
        "- A Mamba model is composed of several layers, which are ResidualBlock.\n",
        "- A ResidualBlock is composed of a MambaBlock, a normalization, and a residual connection : ResidualBlock(x) = mamba(norm(x)) + x\n",
        "- This leaves us with the MambaBlock : its input x is (B, L, D) and its outputs y is also (B, L, D) (B=batch size, L=seq len, D=model dim).\n",
        "First, we expand x into (B, L, 2*ED) (where E is usually 2) and split it into x and z, each (B, L, ED).\n",
        "Then, we apply the short 1d conv to x, followed by an activation function (silu), then the SSM.\n",
        "We then multiply it by silu(z).\n",
        "See Figure 3 of the paper (page 8) for a visual representation of a MambaBlock.\"\"\"\n",
        "\n",
        "@dataclass\n",
        "class MambaConfig:\n",
        "    d_model: int #¬†D\n",
        "    n_layers: int\n",
        "    dt_rank: Union[int, str] = 'auto'\n",
        "    d_state: int = 16 #¬†N in paper/comments\n",
        "    expand_factor: int = 2 #¬†E in paper/comments\n",
        "    d_conv: int = 4\n",
        "\n",
        "    dt_min: float = 0.001\n",
        "    dt_max: float = 0.1\n",
        "    dt_init: str = \"random\" #¬†\"random\" or \"constant\"\n",
        "    dt_scale: float = 1.0\n",
        "    dt_init_floor = 1e-4\n",
        "\n",
        "    rms_norm_eps: float = 1e-5\n",
        "    base_std: float = 0.02\n",
        "\n",
        "    bias: bool = False\n",
        "    conv_bias: bool = True\n",
        "    inner_layernorms: bool = False # apply layernorms to internal activations\n",
        "\n",
        "    mup: bool = False\n",
        "    mup_base_width: float = 128 # width=d_model\n",
        "\n",
        "    pscan: bool = True #¬†use parallel scan mode or sequential mode when training\n",
        "    use_cuda: bool = False # use official CUDA implementation when training (not compatible with (b)float16)\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.d_inner = self.expand_factor * self.d_model # E*D = ED in comments\n",
        "\n",
        "        if self.dt_rank == 'auto':\n",
        "            self.dt_rank = math.ceil(self.d_model / 16)\n",
        "\n",
        "        # muP\n",
        "        if self.mup:\n",
        "            self.mup_width_mult = self.d_model / self.mup_base_width\n",
        "\n",
        "class Mamba(nn.Module):\n",
        "    def __init__(self, config: MambaConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layers = nn.ModuleList([ResidualBlock(config) for _ in range(config.n_layers)])\n",
        "\n",
        "    def forward(self, x): #¬†x : (B, L, D)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "    def step(self, x, caches):\n",
        "        #¬†x : (B, L, D)\n",
        "        #¬†caches : [cache(layer) for all layers], cache : (h, inputs)\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x, caches[i] = layer.step(x, caches[i])\n",
        "        return x, caches\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, config: MambaConfig):\n",
        "        super().__init__()\n",
        "        self.mixer = MambaBlock(config)\n",
        "        self.norm = RMSNorm(config.d_model, config.rms_norm_eps, config.mup)\n",
        "\n",
        "    def forward(self, x): #¬†x : (B, L, D)\n",
        "        output = self.mixer(self.norm(x)) + x\n",
        "        return output\n",
        "\n",
        "    def step(self, x, cache):\n",
        "        #¬†x : (B, D)\n",
        "        #¬†cache : (h, inputs)\n",
        "                # h : (B, ED, N)\n",
        "                #¬†inputs: (B, ED, d_conv-1)\n",
        "\n",
        "        #¬†output : (B, D)\n",
        "        #¬†cache : (h, inputs)\n",
        "        output, cache = self.mixer.step(self.norm(x), cache)\n",
        "        output = output + x\n",
        "        return output, cache\n",
        "\n",
        "class MambaBlock(nn.Module):\n",
        "    def __init__(self, config: MambaConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        #¬†projects block input from D to 2*ED (two branches)\n",
        "        self.in_proj = nn.Linear(config.d_model, 2 * config.d_inner, bias=config.bias)\n",
        "\n",
        "        self.conv1d = nn.Conv1d(in_channels=config.d_inner, out_channels=config.d_inner,\n",
        "                              kernel_size=config.d_conv, bias=config.conv_bias,\n",
        "                              groups=config.d_inner,\n",
        "                              padding=config.d_conv - 1)\n",
        "\n",
        "        #¬†projects x to input-dependent delta, B, C\n",
        "        self.x_proj = nn.Linear(config.d_inner, config.dt_rank + 2 * config.d_state, bias=False)\n",
        "\n",
        "        #¬†projects delta from dt_rank to d_inner\n",
        "        self.dt_proj = nn.Linear(config.dt_rank, config.d_inner, bias=True)\n",
        "\n",
        "        #¬†dt initialization\n",
        "        #¬†dt weights\n",
        "        dt_init_std = config.dt_rank**-0.5 * config.dt_scale\n",
        "        if config.dt_init == \"constant\":\n",
        "            nn.init.constant_(self.dt_proj.weight, dt_init_std)\n",
        "        elif config.dt_init == \"random\":\n",
        "            nn.init.uniform_(self.dt_proj.weight, -dt_init_std, dt_init_std)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        # delta bias\n",
        "        dt = torch.exp(\n",
        "            torch.rand(config.d_inner) * (math.log(config.dt_max) - math.log(config.dt_min)) + math.log(config.dt_min)\n",
        "        ).clamp(min=config.dt_init_floor)\n",
        "        inv_dt = dt + torch.log(-torch.expm1(-dt)) #¬†inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n",
        "        with torch.no_grad():\n",
        "            self.dt_proj.bias.copy_(inv_dt)\n",
        "        #self.dt_proj.bias._no_reinit = True # initialization would set all Linear.bias to zero, need to mark this one as _no_reinit\n",
        "        #¬†todo : explain why removed\n",
        "\n",
        "        # S4D real initialization\n",
        "        A = torch.arange(1, config.d_state + 1, dtype=torch.float32).repeat(config.d_inner, 1)\n",
        "        self.A_log = nn.Parameter(torch.log(A)) # why store A in log ? to keep A < 0 (cf -torch.exp(...)) ? for gradient stability ?\n",
        "        self.A_log._no_weight_decay = True\n",
        "\n",
        "        self.D = nn.Parameter(torch.ones(config.d_inner))\n",
        "        self.D._no_weight_decay = True\n",
        "\n",
        "        #¬†projects block output from ED back to D\n",
        "        self.out_proj = nn.Linear(config.d_inner, config.d_model, bias=config.bias)\n",
        "\n",
        "        # used in jamba\n",
        "        if self.config.inner_layernorms:\n",
        "            self.dt_layernorm = RMSNorm(self.config.dt_rank, config.rms_norm_eps, config.mup)\n",
        "            self.B_layernorm = RMSNorm(self.config.d_state, config.rms_norm_eps, config.mup)\n",
        "            self.C_layernorm = RMSNorm(self.config.d_state, config.rms_norm_eps, config.mup)\n",
        "        else:\n",
        "            self.dt_layernorm = None\n",
        "            self.B_layernorm = None\n",
        "            self.C_layernorm = None\n",
        "\n",
        "        if self.config.use_cuda:\n",
        "            try:\n",
        "                from mamba_ssm.ops.selective_scan_interface import selective_scan_fn\n",
        "                self.selective_scan_cuda = selective_scan_fn\n",
        "            except ImportError:\n",
        "                print(\"Failed to import mamba_ssm. Falling back to mamba.py.\")\n",
        "                self.config.use_cuda = False\n",
        "\n",
        "    def _apply_layernorms(self, dt, B, C):\n",
        "        if self.dt_layernorm is not None:\n",
        "            dt = self.dt_layernorm(dt)\n",
        "        if self.B_layernorm is not None:\n",
        "            B = self.B_layernorm(B)\n",
        "        if self.C_layernorm is not None:\n",
        "            C = self.C_layernorm(C)\n",
        "        return dt, B, C\n",
        "\n",
        "    def forward(self, x): #¬†x : (B, L, D)\n",
        "        _, L, _ = x.shape\n",
        "        xz = self.in_proj(x) # (B, L, 2*ED)\n",
        "        x, z = xz.chunk(2, dim=-1) #¬†(B, L, ED), (B, L, ED)\n",
        "\n",
        "        #¬†x branch\n",
        "        x = x.transpose(1, 2) #¬†(B, ED, L)\n",
        "        x = self.conv1d(x)[:, :, :L] #¬†depthwise convolution over time, with a short filter\n",
        "        x = x.transpose(1, 2) #¬†(B, L, ED)\n",
        "\n",
        "        x = F.silu(x)\n",
        "        y = self.ssm(x, z)\n",
        "\n",
        "        if self.config.use_cuda:\n",
        "            output = self.out_proj(y) # (B, L, D)\n",
        "            return output # the rest of the operations are done in the ssm function (fused with the CUDA pscan)\n",
        "\n",
        "        #¬†z branch\n",
        "        z = F.silu(z)\n",
        "\n",
        "        output = y * z\n",
        "        output = self.out_proj(output) #¬†(B, L, D)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def ssm(self, x, z): #¬†x : (B, L, ED)\n",
        "        A = -torch.exp(self.A_log.float()) # (ED, N)\n",
        "        D = self.D.float()\n",
        "\n",
        "        deltaBC = self.x_proj(x) #¬†(B, L, dt_rank+2*N)\n",
        "        delta, B, C = torch.split(deltaBC, [self.config.dt_rank, self.config.d_state, self.config.d_state], dim=-1) #¬†(B, L, dt_rank), (B, L, N), (B, L, N)\n",
        "        delta, B, C = self._apply_layernorms(delta, B, C)\n",
        "        delta = self.dt_proj.weight @ delta.transpose(1, 2) #¬†(ED, dt_rank) @ (B, L, dt_rank) -> (B, ED, L)\n",
        "        # here we just apply the matrix mul operation of delta = softplus(dt_proj(delta))\n",
        "        # the rest will be applied later (fused if using cuda)\n",
        "\n",
        "        # choose which selective_scan function to use, according to config\n",
        "        if self.config.use_cuda:\n",
        "            # these are unfortunately needed for the selective_scan_cuda function\n",
        "            x = x.transpose(1, 2)\n",
        "            B = B.transpose(1, 2)\n",
        "            C = C.transpose(1, 2)\n",
        "            z = z.transpose(1, 2)\n",
        "\n",
        "            # \"softplus\" + \"bias\" + \"y * silu(z)\" operations are fused\n",
        "            y = self.selective_scan_cuda(x, delta, A, B, C, D, z=z, delta_softplus=True, delta_bias=self.dt_proj.bias.float())\n",
        "            y = y.transpose(1, 2) # (B, L, ED)\n",
        "        else:\n",
        "            delta = delta.transpose(1, 2)\n",
        "            delta = F.softplus(delta + self.dt_proj.bias)\n",
        "            if self.config.pscan:\n",
        "                y = self.selective_scan(x, delta, A, B, C, D)\n",
        "            else:\n",
        "                y = self.selective_scan_seq(x, delta, A, B, C, D)\n",
        "        return y\n",
        "\n",
        "    def selective_scan(self, x, delta, A, B, C, D):\n",
        "        #¬†x : (B, L, ED)\n",
        "        #¬†Œî : (B, L, ED)\n",
        "        #¬†A : (ED, N)\n",
        "        #¬†B : (B, L, N)\n",
        "        #¬†C : (B, L, N)\n",
        "        #¬†D : (ED)\n",
        "        #¬†y : (B, L, ED)\n",
        "        deltaA = torch.exp(delta.unsqueeze(-1) * A) #¬†(B, L, ED, N)\n",
        "        deltaB = delta.unsqueeze(-1) * B.unsqueeze(2) #¬†(B, L, ED, N)\n",
        "        BX = deltaB * (x.unsqueeze(-1)) #¬†(B, L, ED, N)\n",
        "        hs = pscan(deltaA, BX)\n",
        "        y = (hs @ C.unsqueeze(-1)).squeeze(3) #¬†(B, L, ED, N) @¬†(B, L, N, 1) -> (B, L, ED, 1)\n",
        "        y = y + D * x\n",
        "        return y\n",
        "\n",
        "    def selective_scan_seq(self, x, delta, A, B, C, D):\n",
        "        #¬†x : (B, L, ED)\n",
        "        #¬†Œî : (B, L, ED)\n",
        "        #¬†A : (ED, N)\n",
        "        #¬†B : (B, L, N)\n",
        "        #¬†C : (B, L, N)\n",
        "        #¬†D : (ED)\n",
        "        #¬†y : (B, L, ED)\n",
        "        _, L, _ = x.shape\n",
        "        deltaA = torch.exp(delta.unsqueeze(-1) * A) #¬†(B, L, ED, N)\n",
        "        deltaB = delta.unsqueeze(-1) * B.unsqueeze(2) #¬†(B, L, ED, N)\n",
        "        BX = deltaB * (x.unsqueeze(-1)) #¬†(B, L, ED, N)\n",
        "        h = torch.zeros(x.size(0), self.config.d_inner, self.config.d_state, device=deltaA.device) #¬†(B, ED, N)\n",
        "        hs = []\n",
        "        for t in range(0, L):\n",
        "            h = deltaA[:, t] * h + BX[:, t]\n",
        "            hs.append(h)\n",
        "        hs = torch.stack(hs, dim=1) #¬†(B, L, ED, N)\n",
        "        y = (hs @ C.unsqueeze(-1)).squeeze(3) #¬†(B, L, ED, N) @¬†(B, L, N, 1) -> (B, L, ED, 1)\n",
        "        y = y + D * x\n",
        "        return y\n",
        "\n",
        "    #¬†-------------------------- inference -------------------------- #\n",
        "    \"\"\"\n",
        "    Concerning auto-regressive inference\n",
        "\n",
        "    The cool part of using Mamba : inference is constant wrt to sequence length\n",
        "    We just have to keep in cache, for each layer, two things :\n",
        "    - the hidden state h (which is (B, ED, N)), as you typically would when doing inference with a RNN\n",
        "    - the last d_conv-1 inputs of the layer, to be able to compute the 1D conv which is a convolution over the time dimension\n",
        "      (d_conv is fixed so this doesn't incur a growing cache as we progress on generating the sequence)\n",
        "      (and d_conv is usually very small, like 4, so we just have to \"remember\" the last 3 inputs)\n",
        "\n",
        "    Concretely, these two quantities are put inside a cache tuple, and are named h and inputs respectively.\n",
        "    h is (B, ED, N), and inputs is (B, ED, d_conv-1)\n",
        "    The MambaBlock.step() receives this cache, and, along with outputing the output, alos outputs the updated cache for the next call.\n",
        "\n",
        "    The cache object is initialized as follows : (None, torch.zeros()).\n",
        "    When h is None, the selective scan function detects it and start with h=0.\n",
        "    The torch.zeros() isn't a problem (it's same as just feeding the input, because the conv1d is padded)\n",
        "\n",
        "    As we need one such cache variable per layer, we store a caches object, which is simply a list of cache object. (See mamba_lm.py)\n",
        "    \"\"\"\n",
        "\n",
        "    def step(self, x, cache):\n",
        "        #¬†x : (B, D)\n",
        "        #¬†cache : (h, inputs)\n",
        "                # h : (B, ED, N)\n",
        "                #¬†inputs : (B, ED, d_conv-1)\n",
        "        #¬†y : (B, D)\n",
        "        #¬†cache : (h, inputs)\n",
        "        h, inputs = cache\n",
        "        xz = self.in_proj(x) # (B, 2*ED)\n",
        "        x, z = xz.chunk(2, dim=1) #¬†(B, ED), (B, ED)\n",
        "\n",
        "        #¬†x branch\n",
        "        x_cache = x.unsqueeze(2)\n",
        "        x = self.conv1d(torch.cat([inputs, x_cache], dim=2))[:, :, self.config.d_conv-1] #¬†(B, ED)\n",
        "\n",
        "        x = F.silu(x)\n",
        "        y, h = self.ssm_step(x, h)\n",
        "\n",
        "        #¬†z branch\n",
        "        z = F.silu(z)\n",
        "\n",
        "        output = y * z\n",
        "        output = self.out_proj(output) #¬†(B, D)\n",
        "\n",
        "        # prepare cache for next call\n",
        "        inputs = torch.cat([inputs[:, :, 1:], x_cache], dim=2) #¬†(B, ED, d_conv-1)\n",
        "        cache = (h, inputs)\n",
        "\n",
        "        return output, cache\n",
        "\n",
        "    def ssm_step(self, x, h): #¬†x : (B, ED) #¬†h : (B, ED, N)\n",
        "        A = -torch.exp(self.A_log.float()) # (ED, N) #¬†todo : ne pas le faire tout le temps, puisque c'est ind√©pendant de la timestep\n",
        "        D = self.D.float()\n",
        "        deltaBC = self.x_proj(x) #¬†(B, dt_rank+2*N)\n",
        "        delta, B, C = torch.split(deltaBC, [self.config.dt_rank, self.config.d_state, self.config.d_state], dim=-1) #¬†(B, dt_rank), (B, N), (B, N)\n",
        "        delta, B, C = self._apply_layernorms(delta, B, C)\n",
        "        delta = F.softplus(self.dt_proj(delta)) #¬†(B, ED)\n",
        "\n",
        "        deltaA = torch.exp(delta.unsqueeze(-1) * A) #¬†(B, ED, N)\n",
        "        deltaB = delta.unsqueeze(-1) * B.unsqueeze(1) #¬†(B, ED, N)\n",
        "        BX = deltaB * (x.unsqueeze(-1)) #¬†(B, ED, N)\n",
        "        if h is None:\n",
        "            h = torch.zeros(x.size(0), self.config.d_inner, self.config.d_state, device=deltaA.device) #¬†(B, ED, N)\n",
        "        h = deltaA * h + BX #¬†(B, ED, N)\n",
        "        y = (h @ C.unsqueeze(-1)).squeeze(2) #¬†(B, ED, N) @¬†(B, N, 1) -> (B, ED, 1)\n",
        "        y = y + D * x\n",
        "        return y, h\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, d_model: int, eps: float = 1e-5, use_mup: bool = False):\n",
        "        super().__init__()\n",
        "        self.use_mup = use_mup\n",
        "        self.eps = eps\n",
        "        #¬†https://arxiv.org/abs/2404.05728, RMSNorm gains prevents muTransfer (section 4.2.3)\n",
        "        if not use_mup:\n",
        "            self.weight = nn.Parameter(torch.ones(d_model))\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "        if not self.use_mup:\n",
        "            return output * self.weight\n",
        "        else:\n",
        "            return output\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "8YM8ybjmVEYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title alxndrTL mamba2.py\n",
        "# https://github.com/alxndrTL/mamba.py/blob/main/mambapy/mamba2.py\n",
        "\n",
        "\"\"\"adapted from https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba2_simple.py\n",
        "It implements a Mamba2 model BUT still relies on the official Triton code for Mamba2.\n",
        "(Coming soon hopefully is a full torch version, like in mamba.py and pscan.py)\n",
        "\n",
        "Also, the file implements a caching mecanism, and a config similar to what's being done in mamba.py, as well as supports muP.\n",
        "\n",
        "When passing an input of length 1 in the forward function, the model automatically routes the call to the step function.\n",
        "This step function does one \"classic\" RNN-like step of computation, using the input and the cache provided. It returns the output as well as the new cache.\n",
        "This is used for inference, to generate tokens one by one.\n",
        "\n",
        "Also, this model supports prefilling a prompt and decoding token by token : this is just a mix of forwarding (in parallel) the input, and then decoding step by step.\n",
        "In order to do that, we need the forward/parallel call (also used in training) to output the cache, then used to start the step by step decoding part.\n",
        "To use this mode, you need to call Mamba2.forward() with your input (of shape (B, L, D)) as well as with a non-None cache (like full zeros, it doesnt matter, just not \"None\").\n",
        "The forward call will thus return the output and the cache. From there, you can start your step by step decoding (see just above).\n",
        "\n",
        "The cache is composed of two objects :\n",
        "-h_cache: the last hidden state. Just like an RNN : you have to keep track of only the last h.\n",
        "-conv_state: because Mamba2 uses a convolution over the time sequence, with a filter of length d_conv=4, you have to keep the last d_conv-1=3 inputs of that convolution to be able to run it provided a new input.\n",
        "\n",
        "h_cache is of shape (B, n_heads, d_head, N) and is initialized at 0 (ie no starting hidden state, which is the default behavior in Mamba).\n",
        "conv_state is of shape (B, EDN * 2*n_groups*, d_conv) and is initialized at 0\n",
        "\n",
        "(B=batch_size, L=seq len, E = expand_factor, D=d_model, N=d_state)\n",
        "(TODO: make it full pytorch (ie translate mamba_chunk_scan_combined in pytorch))\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from typing import Union\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "try:\n",
        "    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\n",
        "except ImportError:\n",
        "    causal_conv1d_fn, causal_conv1d_update = None, None\n",
        "\n",
        "try:\n",
        "    from mamba_ssm.ops.triton.layernorm_gated import RMSNorm as RMSNormGated, LayerNorm\n",
        "    from mamba_ssm.ops.triton.ssd_combined import mamba_chunk_scan_combined, mamba_split_conv1d_scan_combined\n",
        "    from mamba_ssm.ops.triton.selective_state_update import selective_state_update\n",
        "\n",
        "except ImportError:\n",
        "    RMSNormGated, LayerNorm = None, None\n",
        "    mamba_chunk_scan_combined, mamba_split_conv1d_scan_combined = None, None\n",
        "    selective_state_update = None\n",
        "\n",
        "@dataclass\n",
        "class Mamba2Config:\n",
        "    d_model: int #¬†D\n",
        "    n_layers: int\n",
        "    d_head: int #¬†todo : plutot n_heads non ?\n",
        "    d_state: int = 64 #¬†N in paper/comments\n",
        "    expand_factor: int = 2 #¬†E in paper/comments\n",
        "    d_conv: int = 4\n",
        "    n_groups: int = 1#¬†todo : ??\n",
        "\n",
        "    A_init_range: tuple = (1, 16)\n",
        "    dt_min: float = 0.001\n",
        "    dt_max: float = 0.1\n",
        "    dt_init_floor: float = 1e-4\n",
        "    dt_limit: tuple = (0.0, float(\"inf\"))\n",
        "    conv_init = None\n",
        "\n",
        "    learnable_init_states: bool = False\n",
        "    activation: str = \"swish\" #¬†\"swish\" or \"silu\"\n",
        "\n",
        "    rms_norm_eps: float = 1e-5\n",
        "    base_std: float = 0.02\n",
        "\n",
        "    bias: bool = False\n",
        "    conv_bias: bool = True\n",
        "\n",
        "    mup: bool = False\n",
        "    mup_base_width: float = 128 # width=d_model\n",
        "\n",
        "    chunk_size: int = 256\n",
        "    use_mem_eff_path: bool = True\n",
        "    dtype=None\n",
        "    device=None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.d_inner = self.expand_factor * self.d_model # E*D = ED in comments\n",
        "        self.n_heads = self.d_inner // self.d_head\n",
        "        assert self.d_inner % self.d_head == 0\n",
        "        assert (self.d_inner / self.d_head) % 8 == 0, \"requierement of causal_conv1d\"\n",
        "\n",
        "        # # muP\n",
        "        # if self.mup:\n",
        "        #     self.mup_width_mult = self.d_model / self.mup_base_width\n",
        "\n",
        "class Mamba2(nn.Module):\n",
        "    def __init__(self, config: Mamba2Config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layers = nn.ModuleList([ResidualBlock(config) for _ in range(config.n_layers)])\n",
        "\n",
        "    def forward(self, x, caches=None): # (B, L, D)\n",
        "        if caches is None:\n",
        "            caches = [None] * self.config.n_layers\n",
        "\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x, caches[i] = layer(x, caches[i])\n",
        "\n",
        "        if caches[0] == None:\n",
        "            return x\n",
        "        else:\n",
        "            return x, caches\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, config: Mamba2Config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.mixer = Mamba2Block(self.config)\n",
        "        self.norm = nn.RMSNorm(self.config.d_model, self.config.rms_norm_eps, self.config.mup)\n",
        "\n",
        "    def forward(self, x, cache=None): # (B, L, D)\n",
        "        output, cache = self.mixer(self.norm(x), cache)\n",
        "        output = output + x # (B, L, D)\n",
        "        return output, cache\n",
        "\n",
        "    def get_empty_cache(self, batch_size):\n",
        "        h_cache = torch.zeros(batch_size, self.config.n_heads, self.config.d_head, self.config.d_state, device=self.mixer.in_proj.weight.device, dtype=self.mixer.in_proj.weight.dtype)\n",
        "        conv_cache = torch.zeros(batch_size, self.mixer.conv1d.weight.shape[0], self.config.d_conv, device=self.mixer.conv1d.weight.device, dtype=self.mixer.conv1d.weight.dtype)\n",
        "        return (h_cache, conv_cache)\n",
        "\n",
        "class Mamba2Block(nn.Module):\n",
        "    def __init__(self, config: Mamba2Config):\n",
        "        super().__init__()\n",
        "        factory_kwargs = {\"device\": config.device, \"dtype\": config.dtype}\n",
        "        self.config = config\n",
        "        # [z, x, B, C, dt]\n",
        "        d_in_proj = 2 * self.config.d_inner + 2 * self.config.n_groups * self.config.d_state + self.config.n_heads\n",
        "        self.in_proj = nn.Linear(self.config.d_model, d_in_proj, bias=self.config.bias, **factory_kwargs)\n",
        "\n",
        "        conv_dim = self.config.d_inner + 2 * self.config.n_groups * self.config.d_state\n",
        "        self.conv1d = nn.Conv1d(conv_dim, conv_dim, self.config.d_conv, padding=self.config.d_conv - 1,\n",
        "            bias=self.config.conv_bias, groups=conv_dim, **factory_kwargs,)\n",
        "\n",
        "        if self.config.conv_init is not None:\n",
        "            nn.init.uniform_(self.conv1d.weight, -self.config.conv_init, self.config.conv_init)\n",
        "        # self.conv1d.weight._no_weight_decay = True\n",
        "\n",
        "        #¬†todo : mup init + lr\n",
        "        if self.config.learnable_init_states:\n",
        "            self.init_states = nn.Parameter(torch.zeros(self.config.n_heads, self.config.d_head, self.config.d_state, **factory_kwargs))\n",
        "            self.init_states._no_weight_decay = True\n",
        "\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "        # Initialize log dt bias\n",
        "        dt = torch.exp(\n",
        "            torch.rand(self.config.n_heads, **factory_kwargs) * (math.log(self.config.dt_max) - math.log(self.config.dt_min))\n",
        "            + math.log(self.config.dt_min)\n",
        "        )\n",
        "        dt = torch.clamp(dt, min=self.config.dt_init_floor)\n",
        "        # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n",
        "        inv_dt = dt + torch.log(-torch.expm1(-dt))\n",
        "        self.dt_bias = nn.Parameter(inv_dt)\n",
        "        # Just to be explicit. Without this we already don't put wd on dt_bias because of the check\n",
        "        # name.endswith(\"bias\") in param_grouping.py\n",
        "        self.dt_bias._no_weight_decay = True\n",
        "\n",
        "        # A parameter\n",
        "        assert self.config.A_init_range[0] > 0 and self.config.A_init_range[1] >= self.config.A_init_range[0]\n",
        "        A = torch.empty(self.config.n_heads, dtype=torch.float32, device=self.config.device).uniform_(*self.config.A_init_range)\n",
        "        A_log = torch.log(A).to(dtype=self.config.dtype)\n",
        "        self.A_log = nn.Parameter(A_log)\n",
        "        # self.register_buffer(\"A_log\", torch.zeros(self.nheads, dtype=torch.float32, device=device), persistent=True)\n",
        "        self.A_log._no_weight_decay = True\n",
        "\n",
        "        # D \"skip\" parameter\n",
        "        self.D = nn.Parameter(torch.ones(self.config.n_heads, device=self.config.device))\n",
        "        self.D._no_weight_decay = True\n",
        "\n",
        "        # Extra normalization layer right before output projection\n",
        "        self.norm = RMSNormGated(self.config.d_inner, eps=1e-5, norm_before_gate=False, **factory_kwargs)\n",
        "\n",
        "        self.out_proj = nn.Linear(self.config.d_inner, self.config.d_model, bias=self.config.bias, **factory_kwargs)\n",
        "\n",
        "    def forward(self, u, cache=None, seq_idx=None): # (B, L, D)\n",
        "        batch, length, _ = u.shape\n",
        "        return_cache = False\n",
        "        if cache is not None and length > 1:\n",
        "            cache = None\n",
        "            return_cache = True\n",
        "\n",
        "        if cache is not None:\n",
        "            out, cache = self.step(u, cache)\n",
        "            return out, cache\n",
        "\n",
        "        zxbcdt = self.in_proj(u)  # (B, L, d_in_proj)\n",
        "        A = -torch.exp(self.A_log)  # (nheads) or (d_inner, d_state)\n",
        "        initial_states=repeat(self.init_states, \"... -> b ...\", b=batch) if self.config.learnable_init_states else None\n",
        "        dt_limit_kwargs = {} if self.config.dt_limit == (0.0, float(\"inf\")) else dict(dt_limit=self.config.dt_limit)\n",
        "\n",
        "        if self.config.use_mem_eff_path:\n",
        "            # Fully fused path\n",
        "            out = mamba_split_conv1d_scan_combined(\n",
        "                zxbcdt,\n",
        "                rearrange(self.conv1d.weight, \"d 1 w -> d w\"),\n",
        "                self.conv1d.bias,\n",
        "                self.dt_bias,\n",
        "                A,\n",
        "                D=self.D,\n",
        "                chunk_size=self.config.chunk_size,\n",
        "                seq_idx=seq_idx,\n",
        "                activation=self.config.activation,\n",
        "                rmsnorm_weight=self.norm.weight,\n",
        "                rmsnorm_eps=self.norm.eps,\n",
        "                outproj_weight=self.out_proj.weight,\n",
        "                outproj_bias=self.out_proj.bias,\n",
        "                headdim=self.config.d_head,\n",
        "                ngroups=self.config.n_groups,\n",
        "                norm_before_gate=False,\n",
        "                initial_states=initial_states,\n",
        "                return_final_states=return_cache,\n",
        "                **dt_limit_kwargs,\n",
        "            )\n",
        "\n",
        "            if return_cache:\n",
        "                #¬†get h_cache from output\n",
        "                out, h_cache = out\n",
        "\n",
        "                # compute conv_cache with last d_conv entries of xBC\n",
        "                _, xBC, _ = torch.split(zxbcdt, [self.config.d_inner, self.config.d_inner + 2 * self.config.n_groups * self.config.d_state, self.config.n_heads], dim=-1)\n",
        "                conv_cache = xBC[:, -self.config.d_conv:].transpose(1, 2) # (error if seqlen<d_conv)\n",
        "\n",
        "                cache = (h_cache, conv_cache)\n",
        "\n",
        "        else:\n",
        "            z, xBC, dt = torch.split(\n",
        "                zxbcdt, [self.config.d_inner, self.config.d_inner + 2 * self.config.n_groups * self.config.d_state, self.config.n_heads], dim=-1\n",
        "            )\n",
        "            dt = F.softplus(dt + self.dt_bias)  # (B, L, nheads)\n",
        "            assert self.config.activation in [\"silu\", \"swish\"]\n",
        "\n",
        "            # 1D Convolution\n",
        "            if causal_conv1d_fn is None or self.config.activation not in [\"silu\", \"swish\"]:\n",
        "                xBC = self.act(self.conv1d(xBC.transpose(1, 2)).transpose(1, 2)) # (B, L, self.d_inner + 2 * n_groups * d_state)\n",
        "            else:\n",
        "                xBC = causal_conv1d_fn(\n",
        "                    x=xBC.transpose(1, 2),\n",
        "                    weight=rearrange(self.conv1d.weight, \"d 1 w -> d w\"),\n",
        "                    bias=self.conv1d.bias,\n",
        "                    activation=self.config.activation,\n",
        "                ).transpose(1, 2)\n",
        "\n",
        "            # split into 3 main branches: X, B, C\n",
        "            # These correspond to V, K, Q respectively in the SSM/attention duality\n",
        "            x, B, C = torch.split(xBC, [self.config.d_inner, self.config.n_groups * self.config.d_state, self.config.n_groups * self.config.d_state], dim=-1)\n",
        "            y = mamba_chunk_scan_combined(\n",
        "                rearrange(x, \"b l (h p) -> b l h p\", p=self.config.d_head),\n",
        "                dt,\n",
        "                A,\n",
        "                rearrange(B, \"b l (g n) -> b l g n\", g=self.config.n_groups),\n",
        "                rearrange(C, \"b l (g n) -> b l g n\", g=self.config.n_groups),\n",
        "                chunk_size=self.config.chunk_size,\n",
        "                D=self.D,\n",
        "                z=None,\n",
        "                seq_idx=seq_idx,\n",
        "                initial_states=initial_states,\n",
        "                **dt_limit_kwargs,\n",
        "            )\n",
        "            y = rearrange(y, \"b l h p -> b l (h p)\")\n",
        "\n",
        "            # Multiply \"gate\" branch and apply extra normalization layer\n",
        "            y = self.norm(y, z)\n",
        "            out = self.out_proj(y) # (B, L, D)\n",
        "        return out, cache\n",
        "\n",
        "    def step(self, u, cache): # (B, 1, D)\n",
        "        h_cache, conv_cache = cache\n",
        "\n",
        "        zxbcdt = self.in_proj(u.squeeze(1))  # (B, 2D)\n",
        "        d_mlp = (zxbcdt.shape[-1] - 2 * self.config.d_inner - 2 * self.config.n_groups * self.config.d_state - self.config.n_heads) // 2\n",
        "        z0, x0, z, xBC, dt = torch.split(zxbcdt, [d_mlp, d_mlp, self.config.d_inner, self.config.d_inner + 2 * self.config.n_groups * self.config.d_state, self.config.n_heads], dim=-1)\n",
        "\n",
        "        # conv step\n",
        "        if causal_conv1d_update is None:\n",
        "            conv_cache.copy_(torch.roll(conv_cache, shifts=-1, dims=-1)) # update state (B, D, W)\n",
        "            conv_cache[:, :, -1] = xBC\n",
        "            xBC = torch.sum(conv_cache * rearrange(self.conv1d.weight, \"d 1 w -> d w\"), dim=-1) # (B, D)\n",
        "            if self.conv1d.bias is not None:\n",
        "                xBC = xBC + self.conv1d.bias\n",
        "            xBC = self.act(xBC).to(dtype=x.dtype)\n",
        "        else:\n",
        "            xBC = causal_conv1d_update(xBC, conv_cache, rearrange(self.conv1d.weight, \"d 1 w -> d w\"), self.conv1d.bias, self.config.activation)\n",
        "        x, B, C = torch.split(xBC, [self.config.d_inner, self.config.n_groups * self.config.d_state, self.config.n_groups * self.config.d_state], dim=-1)\n",
        "        A = -torch.exp(self.A_log.float()) # (n_heads)\n",
        "\n",
        "        # SSM step\n",
        "        if selective_state_update is None:\n",
        "            assert self.config.n_groups == 1, \"Only support ngroups=1 for this inference code path\"\n",
        "            # discretize A\n",
        "            dt = F.softplus(dt + self.dt_bias.to(dtype=dt.dtype))  # (B, n_heads)\n",
        "            dA = torch.exp(dt * A)  # (B, n_heads)\n",
        "            # discretize B\n",
        "            x = rearrange(x, \"b (h p) -> b h p\", p=self.config.d_head)\n",
        "            dBx = torch.einsum(\"bh,bn,bhp->bhpn\", dt, B, x)\n",
        "            # compute one step\n",
        "            h_cache.copy_(h_cache * rearrange(dA, \"b h -> b h 1 1\") + dBx)\n",
        "            # compute output\n",
        "            y = torch.einsum(\"bhpn,bn->bhp\", h_cache.to(x.dtype), C)\n",
        "            y = y + rearrange(self.D.to(x.dtype), \"h -> h 1\") * x\n",
        "            y = rearrange(y, \"b h p -> b (h p)\")\n",
        "\n",
        "        else:\n",
        "            A = repeat(A, \"h -> h p n\", p=self.config.d_head, n=self.config.d_state).to(dtype=torch.float32)\n",
        "            dt = repeat(dt, \"b h -> b h p\", p=self.config.d_head)\n",
        "            dt_bias = repeat(self.dt_bias, \"h -> h p\", p=self.config.d_head)\n",
        "            D = repeat(self.D, \"h -> h p\", p=self.config.d_head)\n",
        "            B = rearrange(B, \"b (g n) -> b g n\", g=self.config.n_groups)\n",
        "            C = rearrange(C, \"b (g n) -> b g n\", g=self.config.n_groups)\n",
        "            x_reshaped = rearrange(x, \"b (h p) -> b h p\", p=self.config.d_head)\n",
        "\n",
        "            y = selective_state_update(h_cache, x_reshaped, dt, A, B, C, D, z=None, dt_bias=dt_bias, dt_softplus=True)\n",
        "            y = rearrange(y, \"b h p -> b (h p)\")\n",
        "\n",
        "        #if self.rmsnorm:\n",
        "        y = self.norm(y, z)\n",
        "        if d_mlp > 0:\n",
        "            y = torch.cat([F.silu(z0) * x0, y], dim=-1)\n",
        "        out = self.out_proj(y)\n",
        "        return out.unsqueeze(1), (h_cache, conv_cache)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "E1wARPnMfjpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## state-spaces mamba-ssm"
      ],
      "metadata": {
        "id": "Ohp9YS8GZHn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install mamba-ssm[causal-conv1d]\n",
        "# !pip install mamba-ssm\n",
        "# !wget https://github.com/state-spaces/mamba/releases/download/v2.2.2/mamba_ssm-2.2.2+cu118torch2.1cxx11abiFALSE-cp310-cp310-linux_x86_64.whl\n",
        "# !pip install mamba_ssm-2.2.2+cu118torch2.1cxx11abiFALSE-cp310-cp310-linux_x86_64.whl\n",
        "\n",
        "!pip install torch==2.1.1 torchvision==0.16.1 torchaudio==2.1.1 --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install causal_conv1d==1.1.1\n",
        "!pip install mamba-ssm==1.2.0.post1\n",
        "\n",
        "\n",
        "# !pip install https://github.com/Dao-AILab/causal-conv1d/releases/download/v1.4.0/causal_conv1d-1.4.0+cu118torch2.0cxx11abiFALSE-cp310-cp310-linux_x86_64.whl\n",
        "# !pip install mamba-ssm\n",
        "\n",
        "# !apt-get install -y cuda-11-8\n",
        "\n",
        "# import os\n",
        "# os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda-11.8/lib64:' + os.environ.get('LD_LIBRARY_PATH', '') # Adds the CUDA library path to LD_LIBRARY_PATH\n"
      ],
      "metadata": {
        "id": "kIBNsqJhm0T6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title mamba S6\n",
        "# https://arxiv.org/pdf/2312.00752\n",
        "# https://github.com/state-spaces/mamba\n",
        "\n",
        "from mamba_ssm import Mamba2\n",
        "model = Mamba2(\n",
        "    # This module uses roughly 3 * expand * d_model^2 parameters\n",
        "    d_model=dim, # Model dimension d_model\n",
        "    d_state=64,  # SSM state expansion factor, typically 64 or 128\n",
        "    d_conv=4,    # Local convolution width\n",
        "    expand=2,    # Block expansion factor\n",
        ").to(\"cuda\")\n",
        "y = model(x)\n",
        "assert y.shape == x.shape\n",
        "\n",
        "\n",
        "\n",
        "# https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba2_simple.py\n",
        "\n",
        "# https://github.com/state-spaces/mamba/blob/main/mamba_ssm/models/mixer_seq_simple.py#L118\n",
        "pred = MixerModel(d_model=256, n_layer=1, d_intermediate=1, vocab_size=0,\n",
        "        # ssm_cfg=None,\n",
        "        # attn_layer_idx=None,\n",
        "        # attn_cfg=None,\n",
        "        # norm_epsilon: float = 1e-5,\n",
        "        rms_norm = True,\n",
        "        # initializer_cfg=None,\n",
        "        # fused_add_norm=True,\n",
        "        # residual_in_fp32=False,\n",
        "        # device=None,\n",
        "        # dtype=None,\n",
        "    )\n",
        "\n",
        "\n",
        "pred.embedding = lambda x: x\n",
        "\n",
        "batch=4\n",
        "seq_len=5\n",
        "d_model=256\n",
        "x = torch.randn(batch, seq_len, d_model, device=\"cuda\")\n",
        "y = pred(x)\n",
        "\n",
        "\n",
        "# torch.rand()\n",
        "\n",
        "\n",
        "\n",
        "# # https://github.com/state-spaces/mamba/blob/main/mamba_ssm/models/config_mamba.py\n",
        "# from dataclasses import dataclass, field\n",
        "\n",
        "# dataclass\n",
        "# class MambaConfig:\n",
        "#     d_model = 256\n",
        "#     d_intermediate = 0\n",
        "#     n_layer = 1\n",
        "#     vocab_size = 1\n",
        "#     ssm_cfg: dict = field(default_factory=dict)\n",
        "#     attn_layer_idx: list = field(default_factory=list)\n",
        "#     attn_cfg: dict = field(default_factory=dict)\n",
        "#     rms_norm = True\n",
        "#     residual_in_fp32 = True\n",
        "#     fused_add_norm = True\n",
        "#     pad_vocab_size_multiple = 8\n",
        "#     tie_embeddings = True\n",
        "\n",
        "\n",
        "# https://github.com/state-spaces/mamba/blob/main/mamba_ssm/models/mixer_seq_simple.py#L215\n",
        "# pred = MambaLMHeadModel(config: MambaConfig,\n",
        "#         initializer_cfg=None,\n",
        "#         device=None,\n",
        "#         dtype=None,\n",
        "#     )\n",
        "\n",
        "# pred.backbone.embedding = lambda x: x\n",
        "\n"
      ],
      "metadata": {
        "id": "qXaHvdXQ07e1",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training\n",
        "\n",
        "\n",
        "# https://github.com/redotvideo/mamba-chat/blob/main/train_mamba.py\n",
        "# https://github.com/redotvideo/mamba-chat/blob/main/trainer/mamba_trainer.py\n",
        "\n"
      ],
      "metadata": {
        "id": "mmPNbyTn06pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title mamba_ssm/models/block.py\n",
        "# https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/block.py\n",
        "from typing import Optional\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from mamba_ssm.ops.triton.layer_norm import RMSNorm, layer_norm_fn\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(\n",
        "        self, dim, mixer_cls, mlp_cls, norm_cls=nn.LayerNorm, fused_add_norm=False, residual_in_fp32=False\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Simple block wrapping a mixer class with LayerNorm/RMSNorm and residual connection\"\n",
        "\n",
        "        This Block has a slightly different structure compared to a regular\n",
        "        prenorm Transformer block.\n",
        "        The standard block is: LN -> MHA/MLP -> Add.\n",
        "        [Ref: https://arxiv.org/abs/2002.04745]\n",
        "        Here we have: Add -> LN -> Mixer, returning both\n",
        "        the hidden_states (output of the mixer) and the residual.\n",
        "        This is purely for performance reasons, as we can fuse add and LayerNorm.\n",
        "        The residual needs to be provided (except for the very first block).\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.residual_in_fp32 = residual_in_fp32\n",
        "        self.fused_add_norm = fused_add_norm\n",
        "        self.norm = norm_cls(dim)\n",
        "        self.mixer = mixer_cls(dim)\n",
        "        if mlp_cls is not nn.Identity:\n",
        "            self.norm2 = norm_cls(dim)\n",
        "            self.mlp = mlp_cls(dim)\n",
        "        else:\n",
        "            self.mlp = None\n",
        "        if self.fused_add_norm:\n",
        "            assert RMSNorm is not None, \"RMSNorm import fails\"\n",
        "            assert isinstance(\n",
        "                self.norm, (nn.LayerNorm, RMSNorm)\n",
        "            ), \"Only LayerNorm and RMSNorm are supported for fused_add_norm\"\n",
        "\n",
        "    def forward(self, hidden_states: Tensor, residual: Optional[Tensor] = None, inference_params=None, **mixer_kwargs):\n",
        "        r\"\"\"Pass the input through the encoder layer.\n",
        "\n",
        "        Args:\n",
        "            hidden_states: the sequence to the encoder layer (required).\n",
        "            residual: hidden_states = Mixer(LN(residual))\n",
        "        \"\"\"\n",
        "        if not self.fused_add_norm:\n",
        "            residual = (hidden_states + residual) if residual is not None else hidden_states\n",
        "            hidden_states = self.norm(residual.to(dtype=self.norm.weight.dtype))\n",
        "            if self.residual_in_fp32:\n",
        "                residual = residual.to(torch.float32)\n",
        "        else:\n",
        "            hidden_states, residual = layer_norm_fn(\n",
        "                hidden_states,\n",
        "                self.norm.weight,\n",
        "                self.norm.bias,\n",
        "                residual=residual,\n",
        "                prenorm=True,\n",
        "                residual_in_fp32=self.residual_in_fp32,\n",
        "                eps=self.norm.eps,\n",
        "                is_rms_norm=isinstance(self.norm, RMSNorm)\n",
        "            )\n",
        "        hidden_states = self.mixer(hidden_states, inference_params=inference_params, **mixer_kwargs)\n",
        "\n",
        "        if self.mlp is not None:\n",
        "            if not self.fused_add_norm:\n",
        "                residual = hidden_states + residual\n",
        "                hidden_states = self.norm2(residual.to(dtype=self.norm2.weight.dtype))\n",
        "                if self.residual_in_fp32:\n",
        "                    residual = residual.to(torch.float32)\n",
        "            else:\n",
        "                hidden_states, residual = layer_norm_fn(\n",
        "                    hidden_states,\n",
        "                    self.norm2.weight,\n",
        "                    self.norm2.bias,\n",
        "                    residual=residual,\n",
        "                    prenorm=True,\n",
        "                    residual_in_fp32=self.residual_in_fp32,\n",
        "                    eps=self.norm2.eps,\n",
        "                    is_rms_norm=isinstance(self.norm2, RMSNorm)\n",
        "                )\n",
        "            hidden_states = self.mlp(hidden_states)\n",
        "\n",
        "        return hidden_states, residual\n",
        "\n",
        "    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):\n",
        "        return self.mixer.allocate_inference_cache(batch_size, max_seqlen, dtype=dtype, **kwargs)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "tI7vvGRmtOzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title mamba_ssm/modules/mamba2_simple.py\n",
        "# https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba2_simple.py\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "try: from causal_conv1d import causal_conv1d_fn\n",
        "except ImportError: causal_conv1d_fn = None\n",
        "# from mamba_ssm.ops.triton.layernorm_gated import RMSNorm as RMSNormGated, LayerNorm\n",
        "# from mamba_ssm.ops.triton.ssd_combined import mamba_chunk_scan_combined\n",
        "# from mamba_ssm.ops.triton.ssd_combined import mamba_split_conv1d_scan_combined\n",
        "\n",
        "class Mamba2Simple(nn.Module):\n",
        "    def __init__(self, d_model, d_state=64, d_conv=4,\n",
        "        conv_init=None,\n",
        "        expand=2, headdim=128, ngroups=1,\n",
        "        A_init_range=(1, 16),\n",
        "        dt_min=0.001, dt_max=0.1, dt_init_floor=1e-4, dt_limit=(0.0, float(\"inf\")),\n",
        "        learnable_init_states=False,\n",
        "        activation=\"swish\",\n",
        "        bias=False, conv_bias=True,\n",
        "        # Fused kernel and sharding options\n",
        "        chunk_size=256, use_mem_eff_path=True,\n",
        "        layer_idx=None,  # Absorb kwarg for general module\n",
        "        device=None, dtype=None,):\n",
        "\n",
        "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_state = d_state\n",
        "        self.d_conv = d_conv\n",
        "        self.conv_init = conv_init\n",
        "        self.expand = expand\n",
        "        self.d_inner = self.expand * self.d_model\n",
        "        self.headdim = headdim\n",
        "        self.ngroups = ngroups\n",
        "        assert self.d_inner % self.headdim == 0\n",
        "        self.nheads = self.d_inner // self.headdim\n",
        "        self.dt_limit = dt_limit\n",
        "        self.learnable_init_states = learnable_init_states\n",
        "        self.activation = activation\n",
        "        self.chunk_size = chunk_size\n",
        "        self.use_mem_eff_path = use_mem_eff_path\n",
        "        self.layer_idx = layer_idx\n",
        "\n",
        "        # Order: [z, x, B, C, dt]\n",
        "        d_in_proj = 2 * self.d_inner + 2 * self.ngroups * self.d_state + self.nheads\n",
        "        self.in_proj = nn.Linear(self.d_model, d_in_proj, bias=bias, **factory_kwargs)\n",
        "\n",
        "        conv_dim = self.d_inner + 2 * self.ngroups * self.d_state\n",
        "        self.conv1d = nn.Conv1d(in_channels=conv_dim, out_channels=conv_dim,\n",
        "            bias=conv_bias,\n",
        "            kernel_size=d_conv,\n",
        "            groups=conv_dim, padding=d_conv - 1, **factory_kwargs,)\n",
        "        if self.conv_init is not None:\n",
        "            nn.init.uniform_(self.conv1d.weight, -self.conv_init, self.conv_init)\n",
        "        # self.conv1d.weight._no_weight_decay = True\n",
        "\n",
        "        if self.learnable_init_states:\n",
        "            self.init_states = nn.Parameter(torch.zeros(self.nheads, self.headdim, self.d_state, **factory_kwargs))\n",
        "            self.init_states._no_weight_decay = True\n",
        "\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "        # Initialize log dt bias\n",
        "        dt = torch.exp(torch.rand(self.nheads, **factory_kwargs) * (math.log(dt_max) - math.log(dt_min)) + math.log(dt_min))\n",
        "        dt = torch.clamp(dt, min=dt_init_floor)\n",
        "        # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n",
        "        inv_dt = dt + torch.log(-torch.expm1(-dt))\n",
        "        self.dt_bias = nn.Parameter(inv_dt)\n",
        "        # Just to be explicit. Without this we already don't put wd on dt_bias because of the check\n",
        "        # name.endswith(\"bias\") in param_grouping.py\n",
        "        self.dt_bias._no_weight_decay = True\n",
        "\n",
        "        # A parameter\n",
        "        assert A_init_range[0] > 0 and A_init_range[1] >= A_init_range[0]\n",
        "        A = torch.empty(self.nheads, dtype=torch.float32, device=device).uniform_(*A_init_range)\n",
        "        A_log = torch.log(A).to(dtype=dtype)\n",
        "        self.A_log = nn.Parameter(A_log)\n",
        "        # self.register_buffer(\"A_log\", torch.zeros(self.nheads, dtype=torch.float32, device=device), persistent=True)\n",
        "        self.A_log._no_weight_decay = True\n",
        "\n",
        "        # D \"skip\" parameter\n",
        "        self.D = nn.Parameter(torch.ones(self.nheads, device=device))\n",
        "        self.D._no_weight_decay = True\n",
        "\n",
        "        # Extra normalization layer right before output projection\n",
        "        self.norm = RMSNormGated(self.d_inner, eps=1e-5, norm_before_gate=False, **factory_kwargs)\n",
        "        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=bias, **factory_kwargs)\n",
        "\n",
        "    def forward(self, u, seq_idx=None):\n",
        "        \"\"\"u: (B, L, D)\n",
        "        Returns: same shape as u\"\"\"\n",
        "        batch, seqlen, dim = u.shape\n",
        "\n",
        "        zxbcdt = self.in_proj(u)  # (B, L, d_in_proj)\n",
        "        A = -torch.exp(self.A_log)  # (nheads) or (d_inner, d_state)\n",
        "        initial_states=repeat(self.init_states, \"... -> b ...\", b=batch) if self.learnable_init_states else None\n",
        "        dt_limit_kwargs = {} if self.dt_limit == (0.0, float(\"inf\")) else dict(dt_limit=self.dt_limit)\n",
        "\n",
        "        if self.use_mem_eff_path:\n",
        "            # Fully fused path\n",
        "            out = mamba_split_conv1d_scan_combined(\n",
        "                zxbcdt,\n",
        "                rearrange(self.conv1d.weight, \"d 1 w -> d w\"),\n",
        "                self.conv1d.bias,\n",
        "                self.dt_bias,\n",
        "                A,\n",
        "                D=self.D,\n",
        "                chunk_size=self.chunk_size,\n",
        "                seq_idx=seq_idx,\n",
        "                activation=self.activation,\n",
        "                rmsnorm_weight=self.norm.weight,\n",
        "                rmsnorm_eps=self.norm.eps,\n",
        "                outproj_weight=self.out_proj.weight,\n",
        "                outproj_bias=self.out_proj.bias,\n",
        "                headdim=self.headdim,\n",
        "                ngroups=self.ngroups,\n",
        "                norm_before_gate=False,\n",
        "                initial_states=initial_states,\n",
        "                **dt_limit_kwargs,\n",
        "            )\n",
        "        else:\n",
        "            z, xBC, dt = torch.split(zxbcdt, [self.d_inner, self.d_inner + 2 * self.ngroups * self.d_state, self.nheads], dim=-1)\n",
        "            dt = F.softplus(dt + self.dt_bias)  # (B, L, nheads)\n",
        "            assert self.activation in [\"silu\", \"swish\"]\n",
        "\n",
        "            # 1D Convolution\n",
        "            if causal_conv1d_fn is None or self.activation not in [\"silu\", \"swish\"]:\n",
        "                xBC = self.act(self.conv1d(xBC.transpose(1, 2)).transpose(1, 2))  # (B, L, self.d_inner + 2 * ngroups * d_state)\n",
        "                xBC = xBC[:, :seqlen, :]\n",
        "            else:\n",
        "                xBC = causal_conv1d_fn(\n",
        "                    x=xBC.transpose(1, 2),\n",
        "                    weight=rearrange(self.conv1d.weight, \"d 1 w -> d w\"),\n",
        "                    bias=self.conv1d.bias,\n",
        "                    activation=self.activation,\n",
        "                ).transpose(1, 2)\n",
        "\n",
        "            # Split into 3 main branches: X, B, C\n",
        "            # These correspond to V, K, Q respectively in the SSM/attention duality\n",
        "            x, B, C = torch.split(xBC, [self.d_inner, self.ngroups * self.d_state, self.ngroups * self.d_state], dim=-1)\n",
        "            y = mamba_chunk_scan_combined(\n",
        "                rearrange(x, \"b l (h p) -> b l h p\", p=self.headdim),\n",
        "                dt,\n",
        "                A,\n",
        "                rearrange(B, \"b l (g n) -> b l g n\", g=self.ngroups),\n",
        "                rearrange(C, \"b l (g n) -> b l g n\", g=self.ngroups),\n",
        "                chunk_size=self.chunk_size,\n",
        "                D=self.D,\n",
        "                z=None,\n",
        "                seq_idx=seq_idx,\n",
        "                initial_states=initial_states,\n",
        "                **dt_limit_kwargs,\n",
        "            )\n",
        "            y = rearrange(y, \"b l h p -> b l (h p)\")\n",
        "\n",
        "            # Multiply \"gate\" branch and apply extra normalization layer\n",
        "            y = self.norm(y, z)\n",
        "            out = self.out_proj(y)\n",
        "        return out\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jHsJQqPbt7mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title mamba_ssm/modules/mha.py\n",
        "# https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mha.py\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange\n",
        "\n",
        "try: from flash_attn import flash_attn_with_kvcache\n",
        "except ImportError: flash_attn_with_kvcache = None\n",
        "try: from flash_attn.layers.rotary import RotaryEmbedding\n",
        "except ImportError: RotaryEmbedding = None\n",
        "try: from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\n",
        "except ImportError: causal_conv1d_fn, causal_conv1d_update = None, None\n",
        "\n",
        "\n",
        "def _update_kv_cache(kv, inference_params, layer_idx):\n",
        "    \"\"\"kv: (batch_size, seqlen, 2, nheads, head_dim) or (batch_size, 1, 2, nheads, head_dim)\"\"\"\n",
        "    # Pre-allocate memory for key-values for inference.\n",
        "    num_heads, head_dim = kv.shape[-2:]\n",
        "    assert layer_idx in inference_params.key_value_memory_dict\n",
        "    kv_cache, _ = inference_params.key_value_memory_dict[layer_idx]\n",
        "    # Adjust key and value for inference\n",
        "    batch_start = inference_params.batch_size_offset\n",
        "    batch_end = batch_start + kv.shape[0]\n",
        "    sequence_start = inference_params.seqlen_offset\n",
        "    sequence_end = sequence_start + kv.shape[1]\n",
        "    assert batch_end <= kv_cache.shape[0]\n",
        "    assert sequence_end <= kv_cache.shape[1]\n",
        "    assert kv_cache is not None\n",
        "    kv_cache[batch_start:batch_end, sequence_start:sequence_end, ...] = kv\n",
        "    return kv_cache[batch_start:batch_end, :sequence_end, ...]\n",
        "\n",
        "\n",
        "class MHA(nn.Module):\n",
        "    \"\"\"Multi-head self-attention and cross-attention\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim,\n",
        "        num_heads,\n",
        "        num_heads_kv=None,\n",
        "        head_dim=None,  # If None, use embed_dim // num_heads\n",
        "        mlp_dim=0,\n",
        "        qkv_proj_bias=True,\n",
        "        out_proj_bias=True,\n",
        "        softmax_scale=None,\n",
        "        causal=False,\n",
        "        layer_idx=None,\n",
        "        d_conv=0,\n",
        "        rotary_emb_dim=0,\n",
        "        rotary_emb_base=10000.0,\n",
        "        rotary_emb_interleaved=False,\n",
        "        device=None,\n",
        "        dtype=None,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        num_heads_kv: can be used to toggle MQA / GQA. If None, use num_heads.\n",
        "        return_residual: whether to return the input x along with the output. This is for\n",
        "            performance reason: for post-norm architecture, returning the input allows us\n",
        "            to fuse the backward of nn.Linear with the residual connection.\n",
        "        \"\"\"\n",
        "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.layer_idx = layer_idx\n",
        "        self.d_conv = d_conv\n",
        "        self.rotary_emb_dim = rotary_emb_dim\n",
        "        self.softmax_scale = softmax_scale\n",
        "        self.causal = causal\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.num_heads_kv = num_heads_kv if num_heads_kv is not None else num_heads\n",
        "        assert (\n",
        "            self.num_heads % self.num_heads_kv == 0\n",
        "        ), \"num_heads must be divisible by num_heads_kv\"\n",
        "        if head_dim is None:\n",
        "            assert self.embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
        "        self.head_dim = head_dim if head_dim is not None else self.embed_dim // num_heads\n",
        "        self.mlp_dim = math.ceil(mlp_dim / 256) * 256\n",
        "        qkv_dim = self.head_dim * (self.num_heads + 2 * self.num_heads_kv)\n",
        "        out_dim = self.head_dim * self.num_heads\n",
        "\n",
        "        if self.rotary_emb_dim > 0:\n",
        "            assert RotaryEmbedding is not None, \"rotary requires flash_attn to be installed\"\n",
        "            self.rotary_emb = RotaryEmbedding(\n",
        "                self.rotary_emb_dim,\n",
        "                base=rotary_emb_base,\n",
        "                interleaved=rotary_emb_interleaved,\n",
        "                device=device,\n",
        "            )\n",
        "\n",
        "        self.in_proj = nn.Linear(embed_dim, qkv_dim + self.mlp_dim, bias=qkv_proj_bias, **factory_kwargs)\n",
        "        if self.d_conv > 0:\n",
        "            self.conv1d = nn.Conv1d(\n",
        "                qkv_dim, qkv_dim, kernel_size=self.d_conv, padding=self.d_conv - 1, groups=qkv_dim,\n",
        "                **factory_kwargs\n",
        "            )\n",
        "        self.out_proj = nn.Linear(out_dim + self.mlp_dim // 2, embed_dim, bias=out_proj_bias, **factory_kwargs)\n",
        "\n",
        "    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None):\n",
        "        dtype = self.out_proj.weight.dtype if dtype is None else dtype\n",
        "        device = self.out_proj.weight.device\n",
        "        if self.d_conv > 0:\n",
        "            conv_state = torch.zeros(\n",
        "                batch_size, self.conv1d.weight.shape[0], self.d_conv, device=device, dtype=dtype\n",
        "            )\n",
        "        else:\n",
        "            conv_state = None\n",
        "        kv_cache = torch.empty(\n",
        "            batch_size, max_seqlen, 2, self.num_heads_kv, self.head_dim, dtype=dtype, device=device,\n",
        "        )\n",
        "        return kv_cache, conv_state\n",
        "\n",
        "    def _update_kv_cache(self, kv, inference_params):\n",
        "        \"\"\"kv: (batch_size, seqlen, 2, nheads, head_dim) or (batch_size, 1, 2, nheads, head_dim)\"\"\"\n",
        "        assert self.layer_idx is not None, \"Generation requires layer_idx in the constructor\"\n",
        "        return _update_kv_cache(kv, inference_params, self.layer_idx)\n",
        "\n",
        "    def _apply_rotary_update_kvcache_attention(self, q, kv, inference_params):\n",
        "        \"\"\"\n",
        "        Fast path that combine 3 steps: apply rotary to Q and K, update kv cache, and apply attention.\n",
        "        q: (batch_size, seqlen_q, nheads, head_dim)\n",
        "        kv: (batch_size, seqlen_k, 2, nheads_kv, head_dim)\n",
        "        \"\"\"\n",
        "        assert inference_params is not None and inference_params.seqlen_offset > 0\n",
        "        if self.rotary_emb_dim > 0:\n",
        "            self.rotary_emb._update_cos_sin_cache(\n",
        "                inference_params.max_seqlen, device=q.device, dtype=q.dtype\n",
        "            )\n",
        "            rotary_cos, rotary_sin = self.rotary_emb._cos_cached, self.rotary_emb._sin_cached\n",
        "        else:\n",
        "            rotary_cos, rotary_sin = None, None\n",
        "        batch = q.shape[0]\n",
        "        kv_cache, _ = inference_params.key_value_memory_dict[self.layer_idx]\n",
        "        kv_cache = kv_cache[:batch]\n",
        "        cache_seqlens = (\n",
        "            inference_params.lengths_per_sample[:batch]\n",
        "            if inference_params.lengths_per_sample is not None\n",
        "            else inference_params.seqlen_offset\n",
        "        )\n",
        "        assert flash_attn_with_kvcache is not None, \"flash_attn must be installed\"\n",
        "        context = flash_attn_with_kvcache(\n",
        "            q,\n",
        "            kv_cache[:, :, 0],\n",
        "            kv_cache[:, :, 1],\n",
        "            kv[:, :, 0],\n",
        "            kv[:, :, 1],\n",
        "            rotary_cos=rotary_cos,\n",
        "            rotary_sin=rotary_sin,\n",
        "            cache_seqlens=cache_seqlens,\n",
        "            softmax_scale=self.softmax_scale,\n",
        "            causal=self.causal,\n",
        "            rotary_interleaved=self.rotary_emb.interleaved if self.rotary_emb_dim > 0 else False,\n",
        "        )\n",
        "        return context\n",
        "\n",
        "    def _update_kvcache_attention(self, q, kv, inference_params):\n",
        "        \"\"\"Write kv to inference_params, then do attention\"\"\"\n",
        "        if (\n",
        "            inference_params.seqlen_offset == 0\n",
        "            or flash_attn_with_kvcache is None\n",
        "        ):\n",
        "            # TODO: this only uses seqlen_offset and not lengths_per_sample.\n",
        "            kv = self._update_kv_cache(kv, inference_params)\n",
        "            k, v = kv.unbind(dim=-3)\n",
        "            k = torch.repeat_interleave(k, dim=2, repeats=self.num_heads // self.num_heads_kv)\n",
        "            v = torch.repeat_interleave(v, dim=2, repeats=self.num_heads // self.num_heads_kv)\n",
        "            return F.scaled_dot_product_attention(\n",
        "                q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=self.causal, scale=self.softmax_scale\n",
        "            ).transpose(1, 2)\n",
        "        else:\n",
        "            batch = q.shape[0]\n",
        "            kv_cache, _ = inference_params.key_value_memory_dict[self.layer_idx]\n",
        "            kv_cache = kv_cache[:batch]\n",
        "            cache_seqlens = (\n",
        "                inference_params.lengths_per_sample[:batch]\n",
        "                if inference_params.lengths_per_sample is not None\n",
        "                else inference_params.seqlen_offset\n",
        "            )\n",
        "            return flash_attn_with_kvcache(\n",
        "                q,\n",
        "                kv_cache[:, :, 0],\n",
        "                kv_cache[:, :, 1],\n",
        "                kv[:, :, 0],\n",
        "                kv[:, :, 1],\n",
        "                cache_seqlens=cache_seqlens,\n",
        "                softmax_scale=self.softmax_scale,\n",
        "                causal=self.causal,\n",
        "            )\n",
        "\n",
        "    def forward(self, x, inference_params=None):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: (batch, seqlen, hidden_dim) (where hidden_dim = num heads * head dim) if\n",
        "                cu_seqlens is None and max_seqlen is None, else (total, hidden_dim) where total\n",
        "                is the is the sum of the sequence lengths in the batch.\n",
        "            inference_params: for generation. Adapted from Megatron-LM (and Apex)\n",
        "            https://github.com/NVIDIA/apex/blob/3ff1a10f72ec07067c4e44759442329804ac5162/apex/transformer/testing/standalone_transformer_lm.py#L470\n",
        "        \"\"\"\n",
        "        if inference_params is not None and self.layer_idx not in inference_params.key_value_memory_dict:\n",
        "            inference_params.key_value_memory_dict[self.layer_idx] = self.allocate_inference_cache(\n",
        "                x.shape[0], inference_params.max_seqlen, dtype=x.dtype\n",
        "            )\n",
        "        seqlen_offset = (\n",
        "            0\n",
        "            if inference_params is None\n",
        "            else (\n",
        "                inference_params.lengths_per_sample\n",
        "                if inference_params.lengths_per_sample is not None\n",
        "                else inference_params.seqlen_offset\n",
        "            )\n",
        "        )\n",
        "        rotary_max_seqlen = inference_params.max_seqlen if inference_params is not None else None\n",
        "        qkv = self.in_proj(x)\n",
        "        if self.mlp_dim > 0:\n",
        "            qkv, x_mlp = qkv.split([qkv.shape[-1] - self.mlp_dim, self.mlp_dim], dim=-1)\n",
        "            x_mlp_up, x_mlp_gate = x_mlp.chunk(2, dim=-1)\n",
        "            x_mlp = x_mlp_up * F.silu(x_mlp_gate)\n",
        "        if self.d_conv > 0:\n",
        "            # The inference code for conv1d is pretty messy, should clean it up\n",
        "            if (inference_params is None or inference_params.seqlen_offset == 0):\n",
        "                if causal_conv1d_fn is None:\n",
        "                    qkv = rearrange(\n",
        "                        self.conv1d(rearrange(qkv, \"b s d -> b d s\"))[..., :-(self.d_conv - 1)], \"b d s -> b s d\"\n",
        "                    ).contiguous()\n",
        "                else:\n",
        "                    qkv = causal_conv1d_fn(\n",
        "                        qkv.transpose(1, 2),\n",
        "                        rearrange(self.conv1d.weight, \"d 1 w -> d w\"),\n",
        "                        self.conv1d.bias\n",
        "                    ).transpose(1, 2)\n",
        "                if inference_params is not None:\n",
        "                    _, conv_state = inference_params.key_value_memory_dict[self.layer_idx]\n",
        "                    # If we just take qkv[:, :, -self.d_conv :], it will error if seqlen < self.d_conv\n",
        "                    # Instead F.pad will pad with zeros if seqlen < self.d_conv, and truncate otherwise.\n",
        "                    qkv_t = rearrange(qkv, \"b l d -> b d l\")\n",
        "                    conv_state.copy_(F.pad(qkv_t, (self.d_conv - qkv_t.shape[-1], 0)))  # Update state (B D W)\n",
        "            else:\n",
        "                _, conv_state = inference_params.key_value_memory_dict[self.layer_idx]\n",
        "                assert qkv.shape[1] == 1, \"Only support decoding with 1 token at a time for now\"\n",
        "                qkv = qkv.squeeze(1)\n",
        "                # Conv step\n",
        "                if causal_conv1d_update is None:\n",
        "                    conv_state.copy_(torch.roll(conv_state, shifts=-1, dims=-1))  # Update state (B D W)\n",
        "                    conv_state[:, :, -1] = qkv\n",
        "                    qkv = torch.sum(conv_state * rearrange(self.conv1d.weight, \"d 1 w -> d w\"), dim=-1)  # (B D)\n",
        "                    if self.conv1d.bias is not None:\n",
        "                        qkv = qkv + self.conv1d.bias\n",
        "                else:\n",
        "                    qkv = causal_conv1d_update(\n",
        "                        qkv,\n",
        "                        conv_state,\n",
        "                        rearrange(self.conv1d.weight, \"d 1 w -> d w\"),\n",
        "                        self.conv1d.bias\n",
        "                    )\n",
        "                qkv = qkv.unsqueeze(1)\n",
        "        q, kv = qkv.split([self.num_heads * self.head_dim, self.num_heads_kv * 2 * self.head_dim], dim=-1)\n",
        "        q = rearrange(q, \"... (h d) -> ... h d\", d=self.head_dim)\n",
        "        kv = rearrange(kv, \"... (two hkv d) -> ... two hkv d\", two=2, d=self.head_dim)\n",
        "        if (\n",
        "            inference_params is None\n",
        "            or inference_params.seqlen_offset == 0\n",
        "            or (self.rotary_emb_dim == 0 or self.rotary_emb_dim % 16 != 0)\n",
        "        ):\n",
        "            if self.rotary_emb_dim > 0:\n",
        "                q, kv = self.rotary_emb(\n",
        "                    q, kv, seqlen_offset=seqlen_offset, max_seqlen=rotary_max_seqlen\n",
        "                )\n",
        "            if inference_params is None:\n",
        "                k, v = kv.unbind(dim=-3)\n",
        "                k = torch.repeat_interleave(k, dim=2, repeats=self.num_heads // self.num_heads_kv)\n",
        "                v = torch.repeat_interleave(v, dim=2, repeats=self.num_heads // self.num_heads_kv)\n",
        "                context = F.scaled_dot_product_attention(\n",
        "                    q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=self.causal, scale=self.softmax_scale\n",
        "                ).transpose(1, 2)\n",
        "            else:\n",
        "                context = self._update_kvcache_attention(q, kv, inference_params)\n",
        "        else:\n",
        "            context = self._apply_rotary_update_kvcache_attention(q, kv, inference_params)\n",
        "        context = rearrange(context, \"... h d -> ... (h d)\")\n",
        "        if self.mlp_dim > 0:\n",
        "            context = torch.cat([context, x_mlp], dim=-1)\n",
        "        out = self.out_proj(context)\n",
        "        return out\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "AqMmHadbuEo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title mamba_ssm/modules/mlp.py\n",
        "# https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mlp.py\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class GatedMLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None,\n",
        "        activation=F.silu, bias=False,\n",
        "        multiple_of=128, device=None, dtype=None,):\n",
        "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
        "        super().__init__()\n",
        "        out_features = out_features if out_features is not None else in_features\n",
        "        hidden_features = (hidden_features if hidden_features is not None else int(8 * in_features / 3))\n",
        "        hidden_features = (hidden_features + multiple_of - 1) // multiple_of * multiple_of\n",
        "        self.fc1 = nn.Linear(in_features, 2 * hidden_features, bias=bias, **factory_kwargs)\n",
        "        self.activation = activation\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias, **factory_kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.fc1(x)\n",
        "        y, gate = y.chunk(2, dim=-1)\n",
        "        y = y * self.activation(gate)\n",
        "        y = self.fc2(y)\n",
        "        return y\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "lf4BD3_duGl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vulD1EcSs9xc"
      },
      "outputs": [],
      "source": [
        "# @title mamba_ssm/models/mixer_seq_simple.py\n",
        "# https://github.com/state-spaces/mamba/blob/main/mamba_ssm/models/mixer_seq_simple.py\n",
        "import math\n",
        "from functools import partial\n",
        "import json\n",
        "import os\n",
        "import copy\n",
        "from collections import namedtuple\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from mamba_ssm.models.config_mamba import MambaConfig\n",
        "from mamba_ssm.modules.mamba_simple import Mamba\n",
        "from mamba_ssm.modules.mamba2 import Mamba2\n",
        "from mamba_ssm.modules.mha import MHA\n",
        "from mamba_ssm.modules.mlp import GatedMLP\n",
        "from mamba_ssm.modules.block import Block\n",
        "from mamba_ssm.utils.generation import GenerationMixin\n",
        "from mamba_ssm.utils.hf import load_config_hf, load_state_dict_hf\n",
        "\n",
        "try:\n",
        "    from mamba_ssm.ops.triton.layer_norm import RMSNorm, layer_norm_fn, rms_norm_fn\n",
        "except ImportError:\n",
        "    RMSNorm, layer_norm_fn, rms_norm_fn = None, None, None\n",
        "\n",
        "\n",
        "def create_block(\n",
        "    d_model,\n",
        "    d_intermediate,\n",
        "    ssm_cfg=None,\n",
        "    attn_layer_idx=None,\n",
        "    attn_cfg=None,\n",
        "    norm_epsilon=1e-5,\n",
        "    rms_norm=False,\n",
        "    residual_in_fp32=False,\n",
        "    fused_add_norm=False,\n",
        "    layer_idx=None,\n",
        "    device=None,\n",
        "    dtype=None,\n",
        "):\n",
        "    if ssm_cfg is None:\n",
        "        ssm_cfg = {}\n",
        "    if attn_layer_idx is None:\n",
        "        attn_layer_idx = []\n",
        "    if attn_cfg is None:\n",
        "        attn_cfg = {}\n",
        "    factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
        "    if layer_idx not in attn_layer_idx:\n",
        "        # Create a copy of the config to modify\n",
        "        ssm_cfg = copy.deepcopy(ssm_cfg) if ssm_cfg is not None else {}\n",
        "        ssm_layer = ssm_cfg.pop(\"layer\", \"Mamba1\")\n",
        "        if ssm_layer not in [\"Mamba1\", \"Mamba2\"]:\n",
        "            raise ValueError(f\"Invalid ssm_layer: {ssm_layer}, only support Mamba1 and Mamba2\")\n",
        "        mixer_cls = partial(\n",
        "            Mamba2 if ssm_layer == \"Mamba2\" else Mamba,\n",
        "            layer_idx=layer_idx,\n",
        "            **ssm_cfg,\n",
        "            **factory_kwargs\n",
        "        )\n",
        "    else:\n",
        "        mixer_cls = partial(MHA, layer_idx=layer_idx, **attn_cfg, **factory_kwargs)\n",
        "    norm_cls = partial(nn.LayerNorm if not rms_norm else RMSNorm, eps=norm_epsilon, **factory_kwargs)\n",
        "    if d_intermediate == 0:\n",
        "        mlp_cls = nn.Identity\n",
        "    else:\n",
        "        mlp_cls = partial(GatedMLP, hidden_features=d_intermediate, out_features=d_model, **factory_kwargs)\n",
        "    block = Block(\n",
        "        d_model,\n",
        "        mixer_cls,\n",
        "        mlp_cls,\n",
        "        norm_cls=norm_cls,\n",
        "        fused_add_norm=fused_add_norm,\n",
        "        residual_in_fp32=residual_in_fp32,\n",
        "    )\n",
        "    block.layer_idx = layer_idx\n",
        "    return block\n",
        "\n",
        "\n",
        "# https://github.com/huggingface/transformers/blob/c28d04e9e252a1a099944e325685f14d242ecdcd/src/transformers/models/gpt2/modeling_gpt2.py#L454\n",
        "def _init_weights(\n",
        "    module,\n",
        "    n_layer,\n",
        "    initializer_range=0.02,  # Now only used for embedding layer.\n",
        "    rescale_prenorm_residual=True,\n",
        "    n_residuals_per_layer=1,  # Change to 2 if we have MLP\n",
        "):\n",
        "    if isinstance(module, nn.Linear):\n",
        "        if module.bias is not None:\n",
        "            if not getattr(module.bias, \"_no_reinit\", False):\n",
        "                nn.init.zeros_(module.bias)\n",
        "    elif isinstance(module, nn.Embedding):\n",
        "        nn.init.normal_(module.weight, std=initializer_range)\n",
        "\n",
        "    if rescale_prenorm_residual:\n",
        "        # Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:\n",
        "        #   > A modified initialization which accounts for the accumulation on the residual path with model depth. Scale\n",
        "        #   > the weights of residual layers at initialization by a factor of 1/‚àöN where N is the # of residual layers.\n",
        "        #   >   -- GPT-2 :: https://openai.com/blog/better-language-models/\n",
        "        #\n",
        "        # Reference (Megatron-LM): https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py\n",
        "        for name, p in module.named_parameters():\n",
        "            if name in [\"out_proj.weight\", \"fc2.weight\"]:\n",
        "                # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block\n",
        "                # Following Pytorch init, except scale by 1/sqrt(2 * n_layer)\n",
        "                # We need to reinit p since this code could be called multiple times\n",
        "                # Having just p *= scale would repeatedly scale it down\n",
        "                nn.init.kaiming_uniform_(p, a=math.sqrt(5))\n",
        "                with torch.no_grad():\n",
        "                    p /= math.sqrt(n_residuals_per_layer * n_layer)\n",
        "\n",
        "\n",
        "class MixerModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model,\n",
        "        n_layer,\n",
        "        d_intermediate,\n",
        "        vocab_size,\n",
        "        ssm_cfg=None,\n",
        "        attn_layer_idx=None,\n",
        "        attn_cfg=None,\n",
        "        norm_epsilon: float = 1e-5,\n",
        "        rms_norm: bool = False,\n",
        "        initializer_cfg=None,\n",
        "        fused_add_norm=False,\n",
        "        residual_in_fp32=False,\n",
        "        device=None,\n",
        "        dtype=None,\n",
        "    ) -> None:\n",
        "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
        "        super().__init__()\n",
        "        self.residual_in_fp32 = residual_in_fp32\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model, **factory_kwargs)\n",
        "\n",
        "        # We change the order of residual and layer norm:\n",
        "        # Instead of LN -> Attn / MLP -> Add, we do:\n",
        "        # Add -> LN -> Attn / MLP / Mixer, returning both the residual branch (output of Add) and\n",
        "        # the main branch (output of MLP / Mixer). The model definition is unchanged.\n",
        "        # This is for performance reason: we can fuse add + layer_norm.\n",
        "        self.fused_add_norm = fused_add_norm\n",
        "        if self.fused_add_norm:\n",
        "            if layer_norm_fn is None or rms_norm_fn is None:\n",
        "                raise ImportError(\"Failed to import Triton LayerNorm / RMSNorm kernels\")\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "                create_block(\n",
        "                    d_model,\n",
        "                    d_intermediate=d_intermediate,\n",
        "                    ssm_cfg=ssm_cfg,\n",
        "                    attn_layer_idx=attn_layer_idx,\n",
        "                    attn_cfg=attn_cfg,\n",
        "                    norm_epsilon=norm_epsilon,\n",
        "                    rms_norm=rms_norm,\n",
        "                    residual_in_fp32=residual_in_fp32,\n",
        "                    fused_add_norm=fused_add_norm,\n",
        "                    layer_idx=i,\n",
        "                    **factory_kwargs,\n",
        "                )\n",
        "                for i in range(n_layer)])\n",
        "\n",
        "        self.norm_f = (nn.LayerNorm if not rms_norm else RMSNorm)(\n",
        "            d_model, eps=norm_epsilon, **factory_kwargs\n",
        "        )\n",
        "\n",
        "        self.apply(\n",
        "            partial(\n",
        "                _init_weights,\n",
        "                n_layer=n_layer,\n",
        "                **(initializer_cfg if initializer_cfg is not None else {}),\n",
        "                n_residuals_per_layer=1 if d_intermediate == 0 else 2,  # 2 if we have MLP\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):\n",
        "        return {\n",
        "            i: layer.allocate_inference_cache(batch_size, max_seqlen, dtype=dtype, **kwargs)\n",
        "            for i, layer in enumerate(self.layers)\n",
        "        }\n",
        "\n",
        "    def forward(self, input_ids, inference_params=None, **mixer_kwargs):\n",
        "        hidden_states = self.embedding(input_ids)\n",
        "        residual = None\n",
        "        for layer in self.layers:\n",
        "            hidden_states, residual = layer(hidden_states, residual, inference_params=inference_params, **mixer_kwargs)\n",
        "        if not self.fused_add_norm:\n",
        "            residual = (hidden_states + residual) if residual is not None else hidden_states\n",
        "            hidden_states = self.norm_f(residual.to(dtype=self.norm_f.weight.dtype))\n",
        "        else:\n",
        "            # Set prenorm=False here since we don't need the residual\n",
        "            hidden_states = layer_norm_fn(\n",
        "                hidden_states,\n",
        "                self.norm_f.weight,\n",
        "                self.norm_f.bias,\n",
        "                eps=self.norm_f.eps,\n",
        "                residual=residual,\n",
        "                prenorm=False,\n",
        "                residual_in_fp32=self.residual_in_fp32,\n",
        "                is_rms_norm=isinstance(self.norm_f, RMSNorm)\n",
        "            )\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class MambaLMHeadModel(nn.Module, GenerationMixin):\n",
        "    def __init__(self, config: MambaConfig, initializer_cfg=None, device=None, dtype=None,) -> None:\n",
        "        self.config = config\n",
        "        d_model = config.d_model\n",
        "        n_layer = config.n_layer\n",
        "        d_intermediate = config.d_intermediate\n",
        "        vocab_size = config.vocab_size\n",
        "        ssm_cfg = config.ssm_cfg\n",
        "        attn_layer_idx = config.attn_layer_idx\n",
        "        attn_cfg = config.attn_cfg\n",
        "        rms_norm = config.rms_norm\n",
        "        residual_in_fp32 = config.residual_in_fp32\n",
        "        fused_add_norm = config.fused_add_norm\n",
        "        pad_vocab_size_multiple = config.pad_vocab_size_multiple\n",
        "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
        "\n",
        "        super().__init__()\n",
        "        if vocab_size % pad_vocab_size_multiple != 0:\n",
        "            vocab_size += pad_vocab_size_multiple - (vocab_size % pad_vocab_size_multiple)\n",
        "        self.backbone = MixerModel(\n",
        "            d_model=d_model,\n",
        "            n_layer=n_layer,\n",
        "            d_intermediate=d_intermediate,\n",
        "            vocab_size=vocab_size,\n",
        "            ssm_cfg=ssm_cfg,\n",
        "            attn_layer_idx=attn_layer_idx,\n",
        "            attn_cfg=attn_cfg,\n",
        "            rms_norm=rms_norm,\n",
        "            initializer_cfg=initializer_cfg,\n",
        "            fused_add_norm=fused_add_norm,\n",
        "            residual_in_fp32=residual_in_fp32,\n",
        "            **factory_kwargs,\n",
        "        )\n",
        "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False, **factory_kwargs)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.apply(\n",
        "            partial(\n",
        "                _init_weights,\n",
        "                n_layer=n_layer,\n",
        "                **(initializer_cfg if initializer_cfg is not None else {}),\n",
        "            )\n",
        "        )\n",
        "        self.tie_weights()\n",
        "\n",
        "    def tie_weights(self):\n",
        "        if self.config.tie_embeddings:\n",
        "            self.lm_head.weight = self.backbone.embedding.weight\n",
        "\n",
        "    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):\n",
        "        return self.backbone.allocate_inference_cache(batch_size, max_seqlen, dtype=dtype, **kwargs)\n",
        "\n",
        "    def forward(self, input_ids, position_ids=None, inference_params=None, num_last_tokens=0, **mixer_kwargs):\n",
        "        \"\"\"\n",
        "        \"position_ids\" is just to be compatible with Transformer generation. We don't use it.\n",
        "        num_last_tokens: if > 0, only return the logits for the last n tokens\n",
        "        \"\"\"\n",
        "        hidden_states = self.backbone(input_ids, inference_params=inference_params, **mixer_kwargs)\n",
        "        if num_last_tokens > 0:\n",
        "            hidden_states = hidden_states[:, -num_last_tokens:]\n",
        "        lm_logits = self.lm_head(hidden_states)\n",
        "        CausalLMOutput = namedtuple(\"CausalLMOutput\", [\"logits\"])\n",
        "        return CausalLMOutput(logits=lm_logits)\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, pretrained_model_name, device=None, dtype=None, **kwargs):\n",
        "        config_data = load_config_hf(pretrained_model_name)\n",
        "        config = MambaConfig(**config_data)\n",
        "        model = cls(config, device=device, dtype=dtype, **kwargs)\n",
        "        model.load_state_dict(load_state_dict_hf(pretrained_model_name, device=device, dtype=dtype))\n",
        "        return model\n",
        "\n",
        "    def save_pretrained(self, save_directory):\n",
        "        \"\"\"\n",
        "        Minimal implementation of save_pretrained for MambaLMHeadModel.\n",
        "        Save the model and its configuration file to a directory.\n",
        "        \"\"\"\n",
        "        # Ensure save_directory exists\n",
        "        os.makedirs(save_directory, exist_ok=True)\n",
        "\n",
        "        # Save the model's state_dict\n",
        "        model_path = os.path.join(save_directory, 'pytorch_model.bin')\n",
        "        torch.save(self.state_dict(), model_path)\n",
        "\n",
        "        # Save the configuration of the model\n",
        "        config_path = os.path.join(save_directory, 'config.json')\n",
        "        with open(config_path, 'w') as f:\n",
        "            json.dump(self.config.__dict__, f, indent=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title mamba_ssm/utils/generation.py#L247\n",
        "# https://github.com/state-spaces/mamba/blob/main/mamba_ssm/utils/generation.py#L247\n",
        "\n",
        "class GenerationMixin:\n",
        "    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def generate(\n",
        "        self,\n",
        "        input_ids,\n",
        "        max_length,\n",
        "        top_k=1,\n",
        "        top_p=0.0,\n",
        "        min_p=0.0,\n",
        "        temperature=1.0,\n",
        "        return_dict_in_generate=False,\n",
        "        output_scores=False,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        output = decode(\n",
        "            input_ids, self, max_length, top_k=top_k, top_p=top_p, min_p = min_p, temperature=temperature, output_scores=output_scores, **kwargs\n",
        "        )\n",
        "        if not output_scores:\n",
        "            output.scores = None\n",
        "        return output if return_dict_in_generate else output.sequences\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "SorwaCX4wo8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title mamba_ssm/modules/ssd_minimal.py\n",
        "# https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/ssd_minimal.py\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange, repeat\n",
        "from mamba_ssm.ops.triton.ssd_combined import mamba_chunk_scan_combined\n",
        "\n",
        "\n",
        "def segsum_unstable(x):\n",
        "    \"\"\"Naive segment sum calculation.\"\"\"\n",
        "    T = x.size(-1)\n",
        "    x_cumsum = torch.cumsum(x, dim=-1)\n",
        "    x_segsum = x_cumsum[..., :, None] - x_cumsum[..., None, :]\n",
        "    mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool), diagonal=0)\n",
        "    x_segsum = x_segsum.masked_fill(~mask, -torch.inf)\n",
        "    return x_segsum\n",
        "\n",
        "def segsum(x):\n",
        "    \"\"\"More stable segment sum calculation.\"\"\"\n",
        "    T = x.size(-1)\n",
        "    x = repeat(x, \"... d -> ... d e\", e=T)\n",
        "    mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool), diagonal=-1)\n",
        "    x = x.masked_fill(~mask, 0)\n",
        "    x_segsum = torch.cumsum(x, dim=-2)\n",
        "    mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool), diagonal=0)\n",
        "    x_segsum = x_segsum.masked_fill(~mask, -torch.inf)\n",
        "    return x_segsum\n",
        "\n",
        "def ssd_minimal_discrete(X, A, B, C, block_len, initial_states=None):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "        X: (batch, length, n_heads, d_head)\n",
        "        A: (batch, length, n_heads)\n",
        "        B: (batch, length, n_heads, d_state)\n",
        "        C: (batch, length, n_heads, d_state)\n",
        "    Return:\n",
        "        Y: (batch, length, n_heads, d_head)\n",
        "    \"\"\"\n",
        "    assert X.dtype == A.dtype == B.dtype == C.dtype\n",
        "    assert X.shape[1] % block_len == 0\n",
        "\n",
        "    # Rearrange into blocks/chunks\n",
        "    X, A, B, C = [rearrange(x, \"b (c l) ... -> b c l ...\", l=block_len) for x in (X, A, B, C)]\n",
        "\n",
        "    A = rearrange(A, \"b c l h -> b h c l\")\n",
        "    A_cumsum = torch.cumsum(A, dim=-1)\n",
        "\n",
        "    # 1. Compute the output for each intra-chunk (diagonal blocks)\n",
        "    L = torch.exp(segsum(A))\n",
        "    Y_diag  = torch.einsum(\"bclhn,bcshn,bhcls,bcshp->bclhp\", C, B, L, X)\n",
        "\n",
        "    # 2. Compute the state for each intra-chunk\n",
        "    # (right term of low-rank factorization of off-diagonal blocks; B terms)\n",
        "    decay_states = torch.exp((A_cumsum[:, :, :, -1:] - A_cumsum))\n",
        "    states = torch.einsum(\"bclhn,bhcl,bclhp->bchpn\", B, decay_states, X)\n",
        "\n",
        "    # 3. Compute the inter-chunk SSM recurrence; produces correct SSM states at chunk boundaries\n",
        "    # (middle term of factorization of off-diag blocks; A terms)\n",
        "    if initial_states is None:\n",
        "        initial_states = torch.zeros_like(states[:, :1])\n",
        "    states = torch.cat([initial_states, states], dim=1)\n",
        "    decay_chunk = torch.exp(segsum(F.pad(A_cumsum[:, :, :, -1], (1, 0))))\n",
        "    new_states = torch.einsum(\"bhzc,bchpn->bzhpn\", decay_chunk, states)\n",
        "    states, final_state = new_states[:, :-1], new_states[:, -1]\n",
        "\n",
        "    # 4. Compute state -> output conversion per chunk\n",
        "    # (left term of low-rank factorization of off-diagonal blocks; C terms)\n",
        "    state_decay_out = torch.exp(A_cumsum)\n",
        "    Y_off = torch.einsum('bclhn,bchpn,bhcl->bclhp', C, states, state_decay_out)\n",
        "\n",
        "    # Add output of intra-chunk and inter-chunk terms (diagonal and off-diagonal blocks)\n",
        "    Y = rearrange(Y_diag+Y_off, \"b c l h p -> b (c l) h p\")\n",
        "    return Y, final_state\n",
        "\n",
        "\n",
        "# Simple test\n",
        "def test_correctness():\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    ## Dimensions\n",
        "    # Denoted (B, T, Q, D, P) in the paper\n",
        "    batch, seqlen, chunk_size, dim, headdim = 1, 2048, 64, 2048, 64\n",
        "    nheads = dim // headdim  # (H) in the paper\n",
        "    ngroups = 1 # (G) in the paper\n",
        "    dstate = 64  # (N) in the paper\n",
        "    dtype = torch.float32\n",
        "    device = \"cuda\"\n",
        "\n",
        "    x = torch.randn(batch, seqlen, nheads, headdim, dtype=dtype, device=device)\n",
        "    dt = F.softplus(torch.randn(batch, seqlen, nheads, dtype=torch.float32, device=device) - 4).requires_grad_()\n",
        "    A = (-torch.exp(torch.rand(nheads, dtype=torch.float32, device=device))).requires_grad_()\n",
        "    B = torch.randn(batch, seqlen, ngroups, dstate, dtype=dtype, device=device)\n",
        "    C = torch.randn(batch, seqlen, ngroups, dstate, dtype=dtype, device=device)\n",
        "    D = torch.randn(nheads, dtype=dtype, device=device)\n",
        "\n",
        "    # Comparing fused version and minimal version\n",
        "    y = mamba_chunk_scan_combined(x, dt, A, B, C, chunk_size, D=None)\n",
        "    y_min, _ = ssd_minimal_discrete(x*dt.unsqueeze(-1), A*dt, B, C, chunk_size)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "dtbn54KmHm79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "A4-OhAewvjom"
      },
      "outputs": [],
      "source": [
        "# @title mixer_seq_simple\n",
        "# https://github.com/state-spaces/mamba/blob/main/mamba_ssm/models/mixer_seq_simple.py\n",
        "import math\n",
        "from functools import partial\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from mamba_ssm.models.config_mamba import MambaConfig\n",
        "from mamba_ssm.modules.mamba_simple import Mamba\n",
        "from mamba_ssm.modules.mamba2 import Mamba2\n",
        "# from mamba_ssm.modules.mha import MHA\n",
        "# from mamba_ssm.modules.mlp import GatedMLP\n",
        "# from mamba_ssm.modules.block import Block\n",
        "from mamba_ssm.utils.generation import GenerationMixin\n",
        "# from mamba_ssm.utils.hf import load_config_hf, load_state_dict_hf\n",
        "\n",
        "# try: from mamba_ssm.ops.triton.layer_norm import RMSNorm, layer_norm_fn, rms_norm_fn\n",
        "# except ImportError: RMSNorm, layer_norm_fn, rms_norm_fn = None, None, None\n",
        "\n",
        "import copy\n",
        "def create_block(\n",
        "    d_model,\n",
        "    d_intermediate,\n",
        "    ssm_cfg=None,\n",
        "    attn_layer_idx=None,\n",
        "    attn_cfg=None,\n",
        "    norm_epsilon=1e-5,\n",
        "    rms_norm=False,\n",
        "    residual_in_fp32=False,\n",
        "    fused_add_norm=False,\n",
        "    layer_idx=None,\n",
        "    device=None,\n",
        "    dtype=None,\n",
        "):\n",
        "    if ssm_cfg is None:\n",
        "        ssm_cfg = {}\n",
        "    if attn_layer_idx is None:\n",
        "        attn_layer_idx = []\n",
        "    if attn_cfg is None:\n",
        "        attn_cfg = {}\n",
        "    factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
        "    if layer_idx not in attn_layer_idx:\n",
        "        # Create a copy of the config to modify\n",
        "        ssm_cfg = copy.deepcopy(ssm_cfg) if ssm_cfg is not None else {}\n",
        "        ssm_layer = ssm_cfg.pop(\"layer\", \"Mamba1\")\n",
        "        if ssm_layer not in [\"Mamba1\", \"Mamba2\"]:\n",
        "            raise ValueError(f\"Invalid ssm_layer: {ssm_layer}, only support Mamba1 and Mamba2\")\n",
        "        mixer_cls = partial(\n",
        "            Mamba2 if ssm_layer == \"Mamba2\" else Mamba,\n",
        "            layer_idx=layer_idx,\n",
        "            **ssm_cfg,\n",
        "            **factory_kwargs\n",
        "        )\n",
        "    else:\n",
        "        mixer_cls = partial(MHA, layer_idx=layer_idx, **attn_cfg, **factory_kwargs)\n",
        "    norm_cls = partial(\n",
        "        nn.LayerNorm if not rms_norm else RMSNorm, eps=norm_epsilon, **factory_kwargs\n",
        "    )\n",
        "    if d_intermediate == 0:\n",
        "        mlp_cls = nn.Identity\n",
        "    else:\n",
        "        mlp_cls = partial(\n",
        "            GatedMLP, hidden_features=d_intermediate, out_features=d_model, **factory_kwargs\n",
        "        )\n",
        "    block = Block(\n",
        "        d_model,\n",
        "        mixer_cls,\n",
        "        mlp_cls,\n",
        "        norm_cls=norm_cls,\n",
        "        fused_add_norm=fused_add_norm,\n",
        "        residual_in_fp32=residual_in_fp32,\n",
        "    )\n",
        "    block.layer_idx = layer_idx\n",
        "    return block\n",
        "\n",
        "\n",
        "# https://github.com/huggingface/transformers/blob/c28d04e9e252a1a099944e325685f14d242ecdcd/src/transformers/models/gpt2/modeling_gpt2.py#L454\n",
        "def _init_weights(\n",
        "    module,\n",
        "    n_layer,\n",
        "    initializer_range=0.02,  # Now only used for embedding layer.\n",
        "    rescale_prenorm_residual=True,\n",
        "    n_residuals_per_layer=1,  # Change to 2 if we have MLP\n",
        "):\n",
        "    if isinstance(module, nn.Linear):\n",
        "        if module.bias is not None:\n",
        "            if not getattr(module.bias, \"_no_reinit\", False):\n",
        "                nn.init.zeros_(module.bias)\n",
        "    elif isinstance(module, nn.Embedding):\n",
        "        nn.init.normal_(module.weight, std=initializer_range)\n",
        "\n",
        "    if rescale_prenorm_residual:\n",
        "        # Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:\n",
        "        #   > A modified initialization which accounts for the accumulation on the residual path with model depth. Scale\n",
        "        #   > the weights of residual layers at initialization by a factor of 1/‚àöN where N is the # of residual layers.\n",
        "        #   >   -- GPT-2 :: https://openai.com/blog/better-language-models/\n",
        "        #\n",
        "        # Reference (Megatron-LM): https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py\n",
        "        for name, p in module.named_parameters():\n",
        "            if name in [\"out_proj.weight\", \"fc2.weight\"]:\n",
        "                # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block\n",
        "                # Following Pytorch init, except scale by 1/sqrt(2 * n_layer)\n",
        "                # We need to reinit p since this code could be called multiple times\n",
        "                # Having just p *= scale would repeatedly scale it down\n",
        "                nn.init.kaiming_uniform_(p, a=math.sqrt(5))\n",
        "                with torch.no_grad():\n",
        "                    p /= math.sqrt(n_residuals_per_layer * n_layer)\n",
        "\n",
        "\n",
        "class MixerModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model,\n",
        "        n_layer,\n",
        "        d_intermediate,\n",
        "        vocab_size,\n",
        "        ssm_cfg=None,\n",
        "        attn_layer_idx=None,\n",
        "        attn_cfg=None,\n",
        "        norm_epsilon: float = 1e-5,\n",
        "        rms_norm: bool = False,\n",
        "        initializer_cfg=None,\n",
        "        fused_add_norm=False,\n",
        "        residual_in_fp32=False,\n",
        "        device=None,\n",
        "        dtype=None,\n",
        "    ) -> None:\n",
        "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
        "        super().__init__()\n",
        "        self.residual_in_fp32 = residual_in_fp32\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model, **factory_kwargs)\n",
        "\n",
        "        # We change the order of residual and layer norm:\n",
        "        # Instead of LN -> Attn / MLP -> Add, we do:\n",
        "        # Add -> LN -> Attn / MLP / Mixer, returning both the residual branch (output of Add) and\n",
        "        # the main branch (output of MLP / Mixer). The model definition is unchanged.\n",
        "        # This is for performance reason: we can fuse add + layer_norm.\n",
        "        self.fused_add_norm = fused_add_norm\n",
        "        if self.fused_add_norm:\n",
        "            if layer_norm_fn is None or rms_norm_fn is None:\n",
        "                raise ImportError(\"Failed to import Triton LayerNorm / RMSNorm kernels\")\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "                create_block(\n",
        "                    d_model,\n",
        "                    d_intermediate=d_intermediate,\n",
        "                    ssm_cfg=ssm_cfg,\n",
        "                    attn_layer_idx=attn_layer_idx,\n",
        "                    attn_cfg=attn_cfg,\n",
        "                    norm_epsilon=norm_epsilon,\n",
        "                    rms_norm=rms_norm,\n",
        "                    residual_in_fp32=residual_in_fp32,\n",
        "                    fused_add_norm=fused_add_norm,\n",
        "                    layer_idx=i,\n",
        "                    **factory_kwargs,\n",
        "                )\n",
        "                for i in range(n_layer)])\n",
        "\n",
        "        self.norm_f = (nn.LayerNorm if not rms_norm else RMSNorm)(\n",
        "            d_model, eps=norm_epsilon, **factory_kwargs\n",
        "        )\n",
        "\n",
        "        self.apply(partial(_init_weights, n_layer=n_layer,\n",
        "                **(initializer_cfg if initializer_cfg is not None else {}),\n",
        "                n_residuals_per_layer=1 if d_intermediate == 0 else 2,  # 2 if we have MLP\n",
        "            ))\n",
        "\n",
        "    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):\n",
        "        return {\n",
        "            i: layer.allocate_inference_cache(batch_size, max_seqlen, dtype=dtype, **kwargs)\n",
        "            for i, layer in enumerate(self.layers)\n",
        "        }\n",
        "\n",
        "    def forward(self, input_ids, inference_params=None, **mixer_kwargs):\n",
        "        hidden_states = self.embedding(input_ids)\n",
        "        residual = None\n",
        "        for layer in self.layers:\n",
        "            hidden_states, residual = layer(hidden_states, residual, inference_params=inference_params, **mixer_kwargs)\n",
        "        if not self.fused_add_norm:\n",
        "            residual = (hidden_states + residual) if residual is not None else hidden_states\n",
        "            hidden_states = self.norm_f(residual.to(dtype=self.norm_f.weight.dtype))\n",
        "        else:\n",
        "            # Set prenorm=False here since we don't need the residual\n",
        "            hidden_states = layer_norm_fn(\n",
        "                hidden_states,\n",
        "                self.norm_f.weight,\n",
        "                self.norm_f.bias,\n",
        "                eps=self.norm_f.eps,\n",
        "                residual=residual,\n",
        "                prenorm=False,\n",
        "                residual_in_fp32=self.residual_in_fp32,\n",
        "                is_rms_norm=isinstance(self.norm_f, RMSNorm)\n",
        "            )\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "from collections import namedtuple\n",
        "class MambaLMHeadModel(nn.Module, GenerationMixin):\n",
        "    def __init__(self, config: MambaConfig, initializer_cfg=None, device=None, dtype=None,) -> None:\n",
        "        self.config = config\n",
        "        d_model = config.d_model\n",
        "        n_layer = config.n_layer\n",
        "        d_intermediate = config.d_intermediate\n",
        "        vocab_size = config.vocab_size\n",
        "        ssm_cfg = config.ssm_cfg\n",
        "        attn_layer_idx = config.attn_layer_idx\n",
        "        attn_cfg = config.attn_cfg\n",
        "        rms_norm = config.rms_norm\n",
        "        residual_in_fp32 = config.residual_in_fp32\n",
        "        fused_add_norm = config.fused_add_norm\n",
        "        pad_vocab_size_multiple = config.pad_vocab_size_multiple\n",
        "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
        "\n",
        "        super().__init__()\n",
        "        if vocab_size % pad_vocab_size_multiple != 0:\n",
        "            vocab_size += pad_vocab_size_multiple - (vocab_size % pad_vocab_size_multiple)\n",
        "        self.backbone = MixerModel(\n",
        "            d_model=d_model,\n",
        "            n_layer=n_layer,\n",
        "            d_intermediate=d_intermediate,\n",
        "            vocab_size=vocab_size,\n",
        "            ssm_cfg=ssm_cfg,\n",
        "            attn_layer_idx=attn_layer_idx,\n",
        "            attn_cfg=attn_cfg,\n",
        "            rms_norm=rms_norm,\n",
        "            initializer_cfg=initializer_cfg,\n",
        "            fused_add_norm=fused_add_norm,\n",
        "            residual_in_fp32=residual_in_fp32,\n",
        "            **factory_kwargs,\n",
        "        )\n",
        "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False, **factory_kwargs)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.apply(partial(_init_weights, n_layer=n_layer,\n",
        "                **(initializer_cfg if initializer_cfg is not None else {}),\n",
        "            ))\n",
        "        self.tie_weights()\n",
        "\n",
        "    def tie_weights(self):\n",
        "        if self.config.tie_embeddings:\n",
        "            self.lm_head.weight = self.backbone.embedding.weight\n",
        "\n",
        "    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):\n",
        "        return self.backbone.allocate_inference_cache(batch_size, max_seqlen, dtype=dtype, **kwargs)\n",
        "\n",
        "    def forward(self, input_ids, position_ids=None, inference_params=None, num_last_tokens=0, **mixer_kwargs):\n",
        "        \"\"\"\n",
        "        \"position_ids\" is just to be compatible with Transformer generation. We don't use it.\n",
        "        num_last_tokens: if > 0, only return the logits for the last n tokens\n",
        "        \"\"\"\n",
        "        hidden_states = self.backbone(input_ids, inference_params=inference_params, **mixer_kwargs)\n",
        "        if num_last_tokens > 0:\n",
        "            hidden_states = hidden_states[:, -num_last_tokens:]\n",
        "        lm_logits = self.lm_head(hidden_states)\n",
        "        CausalLMOutput = namedtuple(\"CausalLMOutput\", [\"logits\"])\n",
        "        return CausalLMOutput(logits=lm_logits)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# model_path = os.path.join(save_directory, 'pytorch_model.bin')\n",
        "# torch.save(self.state_dict(), model_path)\n",
        "# model.load_state_dict(load_state_dict_hf(pretrained_model_name, device=device, dtype=dtype))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## basement"
      ],
      "metadata": {
        "id": "npuIYJEhUie_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title huggingface mamba2\n",
        "# https://huggingface.co/docs/transformers/en/model_doc/mamba2\n",
        "# https://github.com/huggingface/transformers/blob/main/src/transformers/models/mamba2/modeling_mamba2.py#L825\n",
        "import torch\n",
        "from transformers import Mamba2Config, Mamba2Model\n",
        "\n",
        "# import torch.nn as nn\n",
        "# class Id(nn.Module):\n",
        "#     def __init__(self): super().__init__()\n",
        "#     def forward(self, x): return x\n",
        "# pred.embeddings = Id()\n",
        "\n",
        "batch=4\n",
        "seq_len=5\n",
        "in_dim=132\n",
        "d_model=128#256\n",
        "# num_heads*head_dim=2*hidden_size ?\n",
        "\n",
        "config = Mamba2Config(hidden_size=d_model, num_heads=8, head_dim=32, state_size=128, num_hidden_layers=1, output_hidden_states=True, vocab_size=0)\n",
        "pred = Mamba2Model(config)\n",
        "pred.embeddings = nn.Linear(in_dim, d_model)\n",
        "\n",
        "x = torch.randn(batch, seq_len, in_dim)\n",
        "\n",
        "# out = pred(inputs_embeds=x, use_cache=True)\n",
        "out = pred(x, use_cache=True)\n",
        "y, cache = out.last_hidden_state, out.cache_params\n",
        "# print(x)\n",
        "# print(y)\n",
        "print(x.shape) # [batch, seq_len, d_model]\n",
        "print(y.shape) # [batch, seq_len, d_model]\n",
        "# print(len(h)) # num_hidden_layers + 1 ?\n",
        "# h = torch.cat(out.hidden_states, dim=-1) # [batch, seq_len, (num_hidden_layers + 1 ?) * d_model]\n",
        "# print(h.shape)\n",
        "print(cache)\n",
        "\n",
        "#  if key/value cache contains 10 tokens (no matter how many of it is a pad token), the cache position for the next token should be torch.tensor([10]) https://huggingface.co/docs/transformers/main/en/kv_cache\n",
        "# out = pred(inputs_embeds=x, cache_params=cache, use_cache=True, cache_position=x.shape[1]) # torch.tensor([10])\n",
        "out = pred(x, cache_params=cache, use_cache=True, cache_position=x.shape[1]) # torch.tensor([10])\n",
        "y, cache = out.last_hidden_state, out.cache_params\n",
        "\n",
        "print(y.shape)\n",
        "print(len(h)) # num_hidden_layers + 1 ?\n",
        "# print(h.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "x = torch.randn(batch, seq_len, in_dim)\n",
        "out = pred(x, use_cache=True)\n",
        "y, cache = out.last_hidden_state, out.cache_params\n",
        "h = torch.cat(out.hidden_states, dim=-1) # [batch, seq_len, (num_hidden_layers + 1 ?) * d_model]\n",
        "out = pred(x, cache_params=cache, use_cache=True, cache_position=x.shape[1]) # torch.tensor([10])\n",
        "y, cache = out.last_hidden_state, out.cache_params\n",
        "\n",
        "\n",
        "# Mamba2Output\n",
        "# Mamba2Cache\n",
        "pred = Mamba2Model(Mamba2Config(hidden_size=d_model, num_heads=8, head_dim=32, state_size=128, num_hidden_layers=1, output_hidden_states=True, vocab_size=0))\n",
        "\n",
        "self.ssm_states = {\n",
        "    i: torch.zeros(batch_size, config.num_heads, config.head_dim, config.state_size, device=device, dtype=dtype)\n",
        "    for i in range(config.num_hidden_layers)\n",
        "}\n",
        "\n",
        "\n",
        "# x = torch.randn(4*batch, seq_len, in_dim)\n",
        "# out = pred(x, cache_params=cache, use_cache=True, cache_position=x.shape[1]) # torch.tensor([10])\n",
        "# y, cache = out.last_hidden_state, out.cache_params\n",
        "\n",
        "# print(cache)\n",
        "# print(cache.ssm_states)\n",
        "# print(len(cache.ssm_states))\n",
        "\n",
        "# config = Mamba2Config(hidden_size=d_model, num_heads=8, head_dim=32, state_size=128, num_hidden_layers=1, output_hidden_states=True, vocab_size=0)\n",
        "# print(config.num_heads, config.head_dim, config.state_size)\n",
        "# print(config.num_hidden_layers)\n",
        "x = x.detach().repeat(4,1,1,1) # [batch, num_heads, head_dim, state_size]\n",
        "for i,v in cache.ssm_states.items():\n",
        "    print(i,v.shape)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TRDECggjQymm",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title johnma2006/mamba-minimal\n",
        "# https://github.com/johnma2006/mamba-minimal/blob/master/model.py\n",
        "\"\"\"Simple, minimal implementation of Mamba in one file of PyTorch.\n",
        "    [1] Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Albert Gu and Tri Dao) https://arxiv.org/abs/2312.00752\n",
        "    [2] The Annotated S4 (Sasha Rush and Sidd Karamcheti) https://srush.github.io/annotated-s4\n",
        "\n",
        "Glossary:\n",
        "    b: batch size                       (`B` in Mamba paper [1] Algorithm 2)\n",
        "    l: sequence length                  (`L` in [1] Algorithm 2)\n",
        "    d or d_model: hidden dim\n",
        "    n or d_state: latent state dim      (`N` in [1] Algorithm 2)\n",
        "    expand: expansion factor            (`E` in [1] Section 3.4)\n",
        "    d_in or d_inner: d * expand         (`D` in [1] Algorithm 2)\n",
        "    A, B, C, D: state space parameters  (See any state space representation formula)\n",
        "                                        (B, C are input-dependent (aka selective, a key innovation in Mamba); A, D are not)\n",
        "    Œî or delta: input-dependent step size\n",
        "    dt_rank: rank of Œî                  (See [1] Section 3.6 \"Parameterization of ‚àÜ\")\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Mamba(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "        # self.embedding = nn.Embedding(args.vocab_size, args.d_model)\n",
        "        self.layers = nn.ModuleList([ResidualBlock(args) for _ in range(args.n_layer)])\n",
        "        self.norm_f = nn.RMSNorm(args.d_model)\n",
        "        # self.lm_head = nn.Linear(args.d_model, args.vocab_size, bias=False)\n",
        "        # self.lm_head.weight = self.embedding.weight  # Tie output projection to embedding weights. # See \"Weight Tying\" paper\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\" Args: input_ids (long tensor): shape (b, l)    (See Glossary at top for definitions of b, l, d_in, n...)\n",
        "        Returns: logits: shape (b, l, vocab_size)\n",
        "        Official Implementation: class MambaLMHeadModel, https://github.com/state-spaces/mamba/blob/main/mamba_ssm/models/mixer_seq_simple.py#L173\"\"\"\n",
        "        # x = self.embedding(input_ids)\n",
        "        x = input_ids\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        x = self.norm_f(x)\n",
        "        # logits = self.lm_head(x)\n",
        "        logits = x\n",
        "        return logits\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        \"\"\"Simple block wrapping Mamba block with normalization and residual connection.\"\"\"\n",
        "        super().__init__()\n",
        "        # self.args = args\n",
        "        self.mixer = MambaBlock(args)\n",
        "        self.norm = nn.RMSNorm(args.d_model)\n",
        "\n",
        "    def forward(self, x): # (b, l, d)\n",
        "        \"\"\"Official Implementation: Block.forward(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py#L297\n",
        "            Note: the official repo chains residual blocks that look like\n",
        "                [Add -> Norm -> Mamba] -> [Add -> Norm -> Mamba] -> [Add -> Norm -> Mamba] -> ...\n",
        "            where the first Add is a no-op. This is purely for performance reasons as this\n",
        "            allows them to fuse the Add->Norm.\n",
        "            We instead implement our blocks as the more familiar, simpler, and numerically equivalent\n",
        "                [Norm -> Mamba -> Add] -> [Norm -> Mamba -> Add] -> [Norm -> Mamba -> Add] -> ....\"\"\"\n",
        "        output = x + self.mixer(self.norm(x))\n",
        "        return output # (b, l, d)\n",
        "\n",
        "\n",
        "from einops import rearrange, repeat, einsum\n",
        "class MambaBlock(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        \"\"\"A single Mamba block, as described in Figure 3 in Section 3.4 in the Mamba paper [1].\"\"\"\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "        self.in_proj = nn.Linear(args.d_model, args.d_inner * 2, bias=args.bias)\n",
        "        self.conv1d = nn.Conv1d(in_channels=args.d_inner, out_channels=args.d_inner, bias=args.conv_bias,\n",
        "            kernel_size=args.d_conv, groups=args.d_inner, padding=args.d_conv - 1,)\n",
        "\n",
        "        self.x_proj = nn.Linear(args.d_inner, args.dt_rank + args.d_state * 2, bias=False) # takes in `x` and outputs the input-specific Œî, B, C\n",
        "        self.dt_proj = nn.Linear(args.dt_rank, args.d_inner, bias=True) # project Œî from dt_rank to d_in\n",
        "\n",
        "        A = repeat(torch.arange(1, args.d_state + 1), 'n -> d n', d=args.d_inner)\n",
        "        self.A_log = nn.Parameter(torch.log(A))\n",
        "        self.D = nn.Parameter(torch.ones(args.d_inner))\n",
        "        self.out_proj = nn.Linear(args.d_inner, args.d_model, bias=args.bias)\n",
        "\n",
        "\n",
        "    def forward(self, x): # (b, l, d)\n",
        "        \"\"\"Mamba block forward. This looks the same as Figure 3 in Section 3.4 in the Mamba paper [1].\n",
        "        Official Implementation: class Mamba, https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py#L119\n",
        "            mamba_inner_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L311\"\"\"\n",
        "        b, l, d = x.shape\n",
        "        x_res = self.in_proj(x)  # shape (b, l, 2 * d_in)\n",
        "        x, res = x_res.split([self.args.d_inner, self.args.d_inner], dim=-1)\n",
        "        x = self.conv1d(x.permute(0, 2, 1))[:, :, :l].permute(0, 2, 1) # (b, l, d_in)\n",
        "        y = self.ssm(F.silu(x)) * F.silu(res)\n",
        "        output = self.out_proj(y)\n",
        "        return output # (b, l, d)\n",
        "\n",
        "\n",
        "    def ssm(self, x): # (b, l, d_in)\n",
        "        \"\"\"- Algorithm 2 in Section 3.2 in the Mamba paper [1]\n",
        "            - run_SSM(A, B, C, u) in The Annotated S4 [2]\n",
        "        Official Implementation: mamba_inner_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L311\"\"\"\n",
        "        d_in, n = self.A_log.shape\n",
        "        # Compute ‚àÜ A B C D, the state space parameters.\n",
        "        #     A, D are input independent (see Mamba paper [1] Section 3.5.2 \"Interpretation of A\" for why A isn't selective)\n",
        "        #     ‚àÜ, B, C are input-dependent (this is a key difference between Mamba and the linear time invariant S4, and is why Mamba is called **selective** state spaces)\n",
        "        A = -torch.exp(self.A_log.float())  # shape (d_in, n)\n",
        "        D = self.D.float()\n",
        "\n",
        "        # 2: B : (B, L, N) ‚Üê sB (x)\n",
        "        # 3: C : (B, L, N) ‚Üê sC (x)\n",
        "        # 4: Œî : (B, L, D) ‚Üê ùúèŒî(Parameter+ùë†Œî(x))\n",
        "        # 5: A, B : (B, L, D, N) ‚Üê discretize(Œî, A, B)\n",
        "        # 6: y ‚Üê SSM(A, B, C) (x)\n",
        "\n",
        "        x_dbl = self.x_proj(x)  # (b, l, dt_rank + 2*n)\n",
        "        delta, B, C = x_dbl.split([self.args.dt_rank, n, n], dim=-1)  # delta: (b, l, dt_rank). B, C: (b, l, n)\n",
        "        delta = F.softplus(self.dt_proj(delta)) # (b, l, d_in)\n",
        "        y = self.selective_scan(x, delta, A, B, C, D)  # This is similar to run_SSM(A, B, C, u) in The Annotated S4 [2]\n",
        "        return y # (b, l, d_in)\n",
        "\n",
        "\n",
        "    def selective_scan(self, u, delta, A, B, C, D): # u: (b, l, d_in); delta: (b, l, d_in); A: (d_in, n); B: (b, l, n); C: (b, l, n); D: (d_in,)\n",
        "        \"\"\"- Section 2 State Space Models in the Mamba paper [1]\n",
        "            - Algorithm 2 in Section 3.2 in the Mamba paper [1]\n",
        "            - run_SSM(A, B, C, u) in The Annotated S4 [2]\n",
        "\n",
        "            x(t+1) = Ax(t) + Bu(t)\n",
        "            y(t)   = Cx(t) + Du(t)\n",
        "        except B and C (and the step size delta, which is used for discretization) are dependent on the input x(t).\n",
        "\n",
        "        Official Implementation: selective_scan_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L86\n",
        "            Note: I refactored some parts out of `selective_scan_ref` out, so the functionality doesn't match exactly.\"\"\"\n",
        "        b, l, d_in = u.shape\n",
        "        n = A.shape[1]\n",
        "\n",
        "        # Discretize continuous parameters (A, B)\n",
        "        deltaA = torch.exp(einsum(delta, A, 'b l d_in, d_in n -> b l d_in n')) # - A is discretized using zero-order hold (ZOH) discretization (see Section 2 Equation 4 in the Mamba paper [1])\n",
        "        deltaB_u = einsum(delta, B, u, 'b l d_in, b l n, b l d_in -> b l d_in n') # - B is discretized using a simplified Euler discretization instead of ZOH. From a discussion with authors: \"A is the more important term and the performance doesn't change much with the simplification on B\"\n",
        "\n",
        "        # Perform selective scan (see scan_SSM() in The Annotated S4 [2])\n",
        "        # Note that the below is sequential, while the official implementation does a much faster parallel scan that is additionally hardware-aware (like FlashAttention).\n",
        "        x = torch.zeros((b, d_in, n), device=deltaA.device)\n",
        "        ys = []\n",
        "        for i in range(l):\n",
        "            x = deltaA[:, i] * x + deltaB_u[:, i]\n",
        "            y = einsum(x, C[:, i, :], 'b d_in n, b n -> b d_in')\n",
        "            ys.append(y)\n",
        "        y = torch.stack(ys, dim=1)  # shape (b, l, d_in)\n",
        "        y = y + u * D\n",
        "        return y # (b, l, d_in)\n",
        "\n",
        "\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "@dataclass\n",
        "class ModelArgs:\n",
        "    d_model = 256\n",
        "    n_layer = 1\n",
        "    # vocab_size = 0\n",
        "    d_state = 16\n",
        "    expand = 2\n",
        "    d_conv = 4\n",
        "    # pad_vocab_size_multiple = 8\n",
        "    conv_bias = True\n",
        "    bias = False\n",
        "    d_inner = int(expand * d_model)\n",
        "    dt_rank = math.ceil(d_model / 16)\n",
        "\n",
        "\n",
        "pred = Mamba(ModelArgs)\n",
        "\n",
        "batch=4\n",
        "seq_len=500\n",
        "d_model=256\n",
        "x = torch.randn(batch, seq_len, d_model)\n",
        "y = pred(x)\n",
        "# print(x)\n",
        "# print(y)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HzzZKHYU8wGz",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(h)\n",
        "# for j in h:\n",
        "#     print(j.shape)\n",
        "\n",
        "print(cache)\n"
      ],
      "metadata": {
        "id": "Iz66JEgroHjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test einops\n",
        "# from einops import repeat, einsum\n",
        "# b,l,d_in,n = 4, 500, 256, 256\n",
        "# # einsum(A,B,'b l d, b l d -> b l d')\n",
        "\n",
        "# delta = torch.randn(b,l,d_in)\n",
        "# B = torch.randn(b,l,n)\n",
        "# u = torch.randn(b,l,d_in)\n",
        "# x0=einsum(delta, B, u, 'b l d_in, b l n, b l d_in -> b l d_in n')\n",
        "# x1=torch.einsum('bld,bln,bld->bldn', delta, B, u)\n",
        "# print((x0-x1).sum())\n",
        "\n",
        "# A=torch.randn(b,d_in,n)\n",
        "# C=torch.randn(b,d_in)\n",
        "# y0 = einsum(A, C, 'b d_in n, b n -> b d_in')\n",
        "# y1 = torch.einsum('bdn,bn->bd', A, C)\n",
        "# print((y0-y1).sum())\n",
        "\n",
        "\n",
        "# A = repeat(torch.arange(1, n + 1), 'n -> d n', d=d_in)\n",
        "# print(A)\n",
        "# A1 = torch.arange(1, n + 1).repeat(d_in, 1)\n",
        "# print((A-A1).sum())\n",
        "\n",
        "# %timeit x0=einsum(delta, B, u, 'b l d_in, b l n, b l d_in -> b l d_in n') # 304 ms, 300\n",
        "# %timeit x1=torch.einsum('bld,bln,bld->bldn', delta, B, u) # 303 ms, 301, 301\n",
        "# %timeit y0 = einsum(A, C, 'b d_in n, b n -> b d_in') # 159 ¬µs, 160\n",
        "# %timeit y1 = torch.einsum('bdn,bn->bd', A, C) # 155, 146\n",
        "# %timeit pred(x) # ein 862ms, 866, 977, 984 ; torch 849, 959, 867, 924\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "WcawLsIkRFm6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}