{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/ssm/blob/main/Mamba_S6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bgu3gbQOFnPv"
      },
      "outputs": [],
      "source": [
        "# @title hf byte dataset me\n",
        "!pip install -qU datasets # restart?\n",
        "import torch\n",
        "from torch.utils.data import IterableDataset\n",
        "\n",
        "class StreamDataset(IterableDataset):\n",
        "    def __init__(self, dataset, seq_len=129, buffer_size=1024):\n",
        "        self.vocab_size = 256 # utf-8 # self.enc.n_vocab # gpt2:50257\n",
        "        self.data = iter(dataset)\n",
        "        self.seq_len, self.buffer_size = seq_len, buffer_size  # must be â‰¥ seq_len\n",
        "        self.buffer = []  # token buffer\n",
        "        self.fill_buffer()\n",
        "\n",
        "    def fill_buffer(self):\n",
        "        while len(self.buffer) < self.buffer_size:\n",
        "            x = next(self.data)\n",
        "            # print(x)\n",
        "            self.buffer.extend(x['text'].encode(\"utf-8\"))\n",
        "\n",
        "    def __iter__(self):\n",
        "        while True:\n",
        "            if len(self.buffer) < self.seq_len: self.fill_buffer()\n",
        "            if len(self.buffer) < self.seq_len: return # raise StopIteration\n",
        "            x, self.buffer = self.buffer[:self.seq_len], self.buffer[self.seq_len:]\n",
        "            yield torch.tensor(x, dtype=torch.int32) # uint8 int32\n",
        "\n",
        "from datasets import load_dataset\n",
        "name = 'Skylion007/openwebtext' if torch.cuda.is_available() else 'stas/openwebtext-10k'\n",
        "dataset = load_dataset(name, split=\"train\", streaming=True, revision='refs/convert/parquet', cache_dir=\"/content/hf\")\n",
        "\n",
        "seq_len = 2**7+1 # 128\n",
        "buffer_size = seq_len*1\n",
        "train_data = StreamDataset(dataset, seq_len, buffer_size) # train_data = StreamDataset(dataset[\"train\"], seq_len, buffer_size)\n",
        "# del dataset\n",
        "\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 64 if torch.cuda.is_available() else 16 #64 512\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, pin_memory=True, num_workers=0)\n",
        "# del train_data\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# https://github.com/facebookresearch/blt/blob/main/bytelatent/tokenizers/blt_tokenizer.py#L137\n",
        "# def encode(c): return torch.tensor(list(c.encode(\"utf-8\")), dtype=torch.uint8)#, device=device)#.unsqueeze(0)\n",
        "def encode(c): return torch.tensor(list(c.encode(\"utf-8\")), dtype=torch.int32, device=device).unsqueeze(0)\n",
        "def decode(x): return bytes(x.tolist()).decode(\"utf-8\", errors='replace') # replace ignore\n",
        "\n",
        "# for x in train_loader:\n",
        "#     print(x.shape, x)\n",
        "#     break\n",
        "# print(decode(x[0][:64]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eq-nojFoYYHA"
      },
      "outputs": [],
      "source": [
        "# @title ssd next\n",
        "# https://goombalab.github.io/blog/2024/mamba2-part4-systems/\n",
        "# https://tridao.me/blog/2024/mamba2-part3-algorithm/\n",
        "# # https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/triton/ssd_combined.py\n",
        "# y = mamba_chunk_scan_combined(x.unflatten(-1, (self.n_heads, self.d_head)), dt, A, # chunk_size=256,\n",
        "#     B.unflatten(-1, (self.ngroups, self.d_state)), C.unflatten(-1, (self.ngroups, self.d_state)),\n",
        "#     D=self.D, z=None).flatten(2) # norm before flatten? so norm is within group\n",
        "import torch\n",
        "\n",
        "def segsum(x): # [...,blk] # Naive segment sum calculation. exp(segsum(A)) produces a 1-SS matrix, which is equivalent to a scalar SSM.\n",
        "    T = x.size(-1)\n",
        "    x_cumsum = torch.cumsum(x, dim=-1)\n",
        "    x_segsum = x_cumsum.unsqueeze(-1) - x_cumsum.unsqueeze(-2) # [...,blk,blk]\n",
        "    mask = torch.triu(torch.ones(T, T, device=x.device, dtype=bool), diagonal=1)\n",
        "    return x_segsum.masked_fill(mask, -torch.inf) # [...,blk,blk]\n",
        "\n",
        "# c:t/blk / 1+t/blk\n",
        "# l/s:blk\n",
        "# n:s\n",
        "# p:d\n",
        "# z:1+t/blk\n",
        "def ssd(X, A, B, C, block_len=64, h0=None): # X:[b,t,h,d], A:[b,t,h], B:[b,t,h,s], C:[b,t,h,s], h0:[b,h,d,s]\n",
        "    assert X.dtype == A.dtype == B.dtype == C.dtype\n",
        "    # assert X.shape[1] % block_len == 0\n",
        "    if X.shape[1] % block_len != 0: X, A, B, C = [x.unsqueeze(1) for x in (X, A, B, C)]\n",
        "    else: X, A, B, C = [x.unflatten(1, (-1,block_len)) for x in (X, A, B, C)]\n",
        "    A = A.permute(0,3,1,2) # [b,t/blk,blk,h] -> [b,h,t/blk,blk]\n",
        "    A_cumsum = torch.cumsum(A, dim=-1) # [b,h,t/blk,blk]\n",
        "\n",
        "    # 1. Compute the output for each intra-chunk (diagonal blocks)\n",
        "    L = torch.exp(segsum(A)) # [b,h,t/blk,blk,blk]\n",
        "    Y_diag  = torch.einsum(\"bclhn,bcshn,bhcls,bcshp->bclhp\", C, B, L, X) # [b,t/blk,blk,h,d]\n",
        "\n",
        "    # 2. Compute the state for each intra-chunk # (right term of low-rank factorization of off-diagonal blocks; B terms)\n",
        "    decay_states = torch.exp((A_cumsum[...,-1:] - A_cumsum)) # [b,h,t/blk,blk]\n",
        "    states = torch.einsum(\"bclhn,bhcl,bclhp->bchpn\", B, decay_states, X) # [b,t/blk,h,d,s]\n",
        "\n",
        "    # 3. Compute the inter-chunk SSM recurrence; produces correct SSM states at chunk boundaries # (middle term of factorization of off-diag blocks; A terms)\n",
        "    if h0==None: h0 = torch.zeros_like(states[:,0], device=device) # [b,h,d,s]\n",
        "    states = torch.cat([h0.unsqueeze(1), states], dim=1) # [b,1+t/blk,h,d,s]\n",
        "    decay_chunk = torch.exp(segsum(F.pad(A_cumsum[...,-1], (1,0)))) # [b,h,1+t/blk]-> # [b,h,1+t/blk,1+t/blk]\n",
        "    new_states = torch.einsum(\"bhzc,bchpn->bzhpn\", decay_chunk, states) # [b,1+t/blk,h,d,s]\n",
        "\n",
        "    # 4. Compute state -> output conversion per chunk # (left term of low-rank factorization of off-diagonal blocks; C terms)\n",
        "    Y_off = torch.einsum('bclhn,bchpn,bhcl->bclhp', C, new_states[:,:-1], torch.exp(A_cumsum)) # [b,t/blk,blk,h,d]\n",
        "\n",
        "    # Add output of intra-chunk and inter-chunk terms (diagonal and off-diagonal blocks)\n",
        "    Y = (Y_diag+Y_off).flatten(1,2)\n",
        "    return Y, new_states[:,-1] # [b,t,h,d], [b,h,d,s]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Jv-PWIEInzRp"
      },
      "outputs": [],
      "source": [
        "# @title appendix\n",
        "\n",
        "T Time /target sequence axis\n",
        "S Source sequence axis\n",
        "D d_model Model dimension\n",
        "N d_state State/feature dimension\n",
        "P d_head\n",
        "H n_head\n",
        "G independent K and V heads, where 1 < G and G divides H\n",
        "\n",
        "d_model, 2560\n",
        "d_state=64,\n",
        "d_conv=4,\n",
        "expand=2,\n",
        "headdim=128,\n",
        "ngroups=1,\n",
        "\n",
        "\n",
        "MQA < 1HA ~ MHA < MVA\n",
        "X~V; B~K; C~Q\n",
        "\n",
        "ð´, ðµ,ð¶ parameters have a state dimension N. âˆˆ R (T,T) .\n",
        "operates over ð‘‹ âˆˆ R (T,P) , independently over the P axis. -> is one head\n",
        "H independent heads, for a total model dimension of D = d_model. The parameters may be tied across heads, leading to a head pattern.\n",
        "state size N ~ ð‘„ð¾ head dimension ; head dimension P ~ ð‘‰ head dimension\n",
        " when the model dimension D increases, we increase the number of heads while keeping the head dimensions N and P fixed\n",
        "\n",
        "d_inner = expand * d_model = nheads * headdim\n",
        "\n",
        "\n",
        "X/z: b,l,n_head,d_head    blhp\n",
        "B/C: b,l,ngroups,d_state    blgn\n",
        "dt: nheads\n",
        "A: nheads\n",
        "D: nheads\n",
        "\n",
        "\n",
        "https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/triton/selective_state_update.py#L104\n",
        "state: (batch, dim, dstate) or (batch, nheads, dim, dstate)\n",
        "x: (batch, dim) or (batch, nheads, dim)/\n",
        "dt: (batch, dim) or (batch, nheads, dim) # nheads\n",
        "A: (dim, dstate) or (nheads, dim, dstate) # nheads\n",
        "B: (batch, dstate) or (batch, ngroups, dstate)/\n",
        "C: (batch, dstate) or (batch, ngroups, dstate)/\n",
        "D: (dim,) or (nheads, dim) # nheads\n",
        "z: (batch, dim) or (batch, nheads, dim)/\n",
        "dt_bias: (dim,) or (nheads, dim) # nheads\n",
        "\n",
        "\n",
        "# https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py\n",
        "A = repeat(\n",
        "    torch.arange(1, self.d_state + 1, dtype=torch.float32, device=device),\n",
        "    \"n -> d n\",\n",
        "    d=self.d_inner,\n",
        ").contiguous()\n",
        "\n",
        "ngroups1\n",
        "expand\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6V3B_QM9iVX"
      },
      "outputs": [],
      "source": [
        "# @title mamba_ssm/modules/mamba2_simple.py next\n",
        "# https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba2_simple.py\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class Mamba2(nn.Module):\n",
        "    # def __init__(self, d_model, expand=2, n_heads=8, n_groups=1, d_state=64, d_conv=4):\n",
        "    # def __init__(self, d_model, expand=2, n_heads=8, n_groups=4, d_state=8, d_conv=4):\n",
        "    def __init__(self, d_model, expand=2, n_heads=8, n_groups=8, d_state=8, d_conv=4):\n",
        "        super().__init__()\n",
        "        n_groups = min(n_heads, n_groups)\n",
        "        assert n_heads % n_groups == 0, \"nheads must be divisible by ngroups\"\n",
        "        self.d_model, self.n_groups, self.d_state, self.d_conv = d_model, n_groups, d_state, d_conv\n",
        "        self.d_inner = expand * self.d_model\n",
        "        self.n_heads, self.d_head = n_heads, self.d_inner//n_heads\n",
        "\n",
        "        self.in_proj = nn.Linear(self.d_model, 2* self.d_inner + 2* self.n_groups*self.d_state + self.n_heads, bias=False) # z,x,B,C,dt\n",
        "        conv_dim = self.d_inner + 2* self.n_groups*self.d_state # for x,B,C\n",
        "        self.conv1d = nn.Conv1d(conv_dim, conv_dim, kernel_size=d_conv, groups=conv_dim, bias=True)\n",
        "        # self.conv1d.weight._no_weight_decay = True\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "        # self.h0 = nn.Parameter(torch.zeros(self.n_heads, self.d_head, self.d_state))\n",
        "        # self.h0._no_weight_decay = True\n",
        "        dt_min, dt_max = .001, .1\n",
        "        dt = torch.exp(torch.rand(self.n_heads) * (math.log(dt_max)-math.log(dt_min)) + math.log(dt_min)).clamp(min=1e-4)\n",
        "        self.dt_bias = nn.Parameter(dt + torch.log(-torch.expm1(-dt))) # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n",
        "        self.dt_bias._no_weight_decay = True\n",
        "        A = torch.empty(self.n_heads, dtype=torch.float32).uniform_(1,16) # 1,16 ; .2,2\n",
        "        self.A_log = nn.Parameter(torch.log(A))\n",
        "        self.A_log._no_weight_decay = True\n",
        "        self.D = nn.Parameter(torch.ones(self.n_heads))\n",
        "        self.D._no_weight_decay = True\n",
        "        self.norm = nn.RMSNorm(self.d_inner)\n",
        "        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=False)\n",
        "\n",
        "    def forward(self, u, h=None): # [b,t,d], ([b,k-1,xbc], [b,h,d,s])\n",
        "        # print('Mamba2Simple u', u.shape)\n",
        "        A = -torch.exp(self.A_log) # [n_heads]\n",
        "        z, xBC, dt = self.in_proj(u).split([self.d_inner, self.d_inner + 2* self.n_groups*self.d_state, self.n_heads], dim=-1)\n",
        "        dt = F.softplus(dt+self.dt_bias) # [b,t,h]\n",
        "\n",
        "        if h==None:\n",
        "            h_conv = F.pad(xBC, (0,0,self.d_conv-u.shape[1]-1,0)) # [b,k-1,xbc]\n",
        "            h_ssm = torch.zeros(u.shape[0], self.n_heads, self.d_head, self.d_state, device=device) # [b,h,d,s]\n",
        "            # h_ssm = self.h0.expand(u.size(0),-1,-1,-1) # [b,h,d,s]\n",
        "            xBC = F.pad(xBC, (0,0,self.d_conv-1,0)) # [b,t+d_conv-1,xbc]\n",
        "        else:\n",
        "            h_conv, h_ssm = h\n",
        "            xBC = torch.cat([h_conv, xBC], dim=1) # [b,t+d_conv-1,xbc]\n",
        "\n",
        "        # print(h_conv.shape, xBC.shape)\n",
        "        # xBC = self.act(xBC + self.conv1d(xBC.transpose(-2,-1)).transpose(-2,-1))\n",
        "        xBC = self.act(self.conv1d(xBC.transpose(-2,-1)).transpose(-2,-1)) # [b,t+d_conv-1,xbc]->[b,t,xbc]\n",
        "        x, B, C = xBC.split([self.n_heads*self.d_head, self.n_groups*self.d_state, self.n_groups*self.d_state], dim=-1) # x:[b,t,h,d], B/C:[b,t,g,s] # X, B, C correspond to V, K, Q respectively in the SSM/attention duality\n",
        "        x, B, C = x.unflatten(-1, (self.n_heads, self.d_head)), B.unflatten(-1, (self.n_groups, self.d_state)), C.unflatten(-1, (self.n_groups, self.d_state))\n",
        "        h_g = self.n_heads//self.n_groups\n",
        "        if h_g>1: B, C = B.repeat_interleave(h_g, dim=-2), C.repeat_interleave(h_g, dim=-2) # [b,g,s]->[b,h,s]\n",
        "\n",
        "        # print('x dt a b', x.shape, dt.shape, A.shape, B.shape)\n",
        "        y, h_ssm = ssd(x*dt.unsqueeze(-1), A*dt, B, C, 64, h_ssm) # 256\n",
        "\n",
        "        y = y + x * self.D.unsqueeze(-1)\n",
        "        # y = self.norm(y.flatten(2) * self.act(z)) # [b,t,d_inner] # norm(x)*silu(z) if norm_before_gate, else norm(x*silu(z)) # https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/triton/layernorm_gated.py#L18\n",
        "        y = self.norm(y.flatten(2)) * self.act(z)\n",
        "        out = self.out_proj(y)\n",
        "        return out, (h_conv, h_ssm) # [b,t,in], ([b,k-1,xbc], [b,h,d,s])\n",
        "\n",
        "\n",
        "# b,t,d_model=5,256,32\n",
        "b,t,d_model=5,7,32\n",
        "d_model = 64\n",
        "u = torch.randn(b,t,d_model, device=device)\n",
        "model = Mamba2(d_model).to(device)\n",
        "out, h = model(u)\n",
        "# h0 = torch.randn(b, model.n_heads, model.d_head, model.d_state)\n",
        "# print(out.shape)\n",
        "# print(out.shape, h.shape)\n",
        "# print(out[0,-3:,:5], h[0,:2,:5,:5])\n",
        "out, h = model(u, h)\n",
        "h_conv, h_ssm = h\n",
        "print(h_conv.shape, h_ssm.shape)\n",
        "\n",
        "# u = torch.randn(b,7,d_model, device=device)\n",
        "# # out, h = model(u[:,-1:], h)\n",
        "# print(out.shape)\n",
        "# print(out[0,-3:,:5], h[0,:2,:5,:5])\n",
        "\n",
        "\n",
        "# cpu seq ~40-41s\n",
        "# this is what sherti, epow sour wocks I rmtre herring' Coer alaid it the adoo\n",
        "# 700 time: 43.38966917991638 0.3799727667075251\n",
        "# strain 2.045107126235962\n",
        "# this is whate lilln the Tont to pleesyecise merefes asse sperced is a a cagk\n",
        "\n",
        "# gpu ssd seqlen 2^11 29.5s\n",
        "\n",
        "# n_groups, vs d_state\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iexje6t20KIy"
      },
      "outputs": [],
      "source": [
        "# print(model.blk[0].rnn[0].A_log.exp())\n",
        "# print(model.blk.rnn[1].A_log.exp())\n",
        "print(model.blk[0].rnn.A_log.exp())\n",
        "print(model.blk[1].rnn.A_log.exp())\n",
        "# [0.7099, 0.6012, 0.6362, 0.6542, 0.6469, 0.5790, 0.5655, 0.8270],\n",
        "# [1.1237, 0.4842, 0.8551, 1.5170, 0.5162, 0.9203, 0.7363, 0.9541]\n",
        "# [1.2897, 0.8572, 0.6168, 1.1675, 1.4585, 0.8542, 1.2247, 1.2768],\n",
        "# [1.1875, 0.9908, 1.3625, 1.2116, 1.2327, 0.8914, 1.1673, 1.1140],\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jG8GfZkxkRUj",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# @title Block\n",
        "# import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# def zero_module(module):\n",
        "#     for p in module.parameters():\n",
        "#         p.detach().zero_()\n",
        "#     return module\n",
        "\n",
        "# class SwiGLU(nn.Module): # https://arxiv.org/pdf/2002.05202\n",
        "#     def __init__(self, d_model, ff_dim): # d_model * 3*ff_dim params\n",
        "#         super().__init__()\n",
        "#         self.lin0 = nn.Linear(d_model, 2*ff_dim, bias=False)\n",
        "#         self.lin1 = zero_module(nn.Linear(ff_dim, d_model, bias=False))\n",
        "#         # torch.nn.init.normal_(self.lin0.weight, std=.02)\n",
        "#         torch.nn.init.normal_(self.lin0.weight, std=1/(math.sqrt(d_model)+math.sqrt(ff_dim)))\n",
        "\n",
        "#     def forward(self, x): # [b,t,d]\n",
        "#         x0, x1 = self.lin0(x).chunk(2, dim=-1)\n",
        "#         return self.lin1(x0*F.silu(x1))\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, ff_mult=4, drop=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm1 = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        # self.norm2 = nn.RMSNorm(d_model)\n",
        "        # self.drop = nn.Dropout(drop)\n",
        "        self.rnn = Mamba2(d_model, n_heads=n_heads)\n",
        "        # act = nn.SiLU() # GELU SiLU ReLU\n",
        "        # ff_dim=d_model*ff_mult#4\n",
        "        # # if ff_dim==None: ff_dim=int(d_model*3/2)#4\n",
        "        # self.ff = nn.Sequential(\n",
        "        #     nn.RMSNorm(d_model), nn.Linear(d_model, ff_dim), act,\n",
        "        #     nn.RMSNorm(ff_dim), nn.Dropout(drop), zero_module(nn.Linear(ff_dim, d_model))\n",
        "        # )\n",
        "        # torch.nn.init.normal_(self.ff[1].weight, std=.02)\n",
        "        # # torch.nn.init.normal_(self.ff[1].weight, std=1/(math.sqrt(d_model)+math.sqrt(ff_dim)))\n",
        "        # self.ff = SwiGLU(d_model, ff_dim)\n",
        "\n",
        "    def forward(self, x, h=None): # [b,t,d], (,)\n",
        "        # print('blk fwd', x.shape, h.shape)\n",
        "        x_, h = self.rnn(self.norm1(x), h) #\n",
        "        x = x + x_\n",
        "        # x = x + self.ff(x)\n",
        "        return x, h\n",
        "\n",
        "b,t,d = 2,256,16\n",
        "# b,t,d = 2,5,256\n",
        "x = torch.rand(b,t,d, device=device)\n",
        "model = Block(d_model=d, n_heads=4).to(device)\n",
        "# model = Seq(*[Block(d_model=d, n_heads=4) for _ in range(2)])\n",
        "out, h = model(x)\n",
        "print(out.shape)\n",
        "\n",
        "# import time\n",
        "# start = time.time()\n",
        "# # out = model(x)\n",
        "# print(out.shape)\n",
        "# print(time.time()-start)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxQWouWCvzmk"
      },
      "outputs": [],
      "source": [
        "# @title GPT\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "    def forward(self, x, hid=None):\n",
        "        hidden = []\n",
        "        for i, layer in enumerate(self):\n",
        "            x, h = layer(x, hid[i] if hid!=None else None)\n",
        "            hidden.append(h)\n",
        "        return x, hidden\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, in_dim, d_model=64, out_dim=None, n_heads=8, n_layers=1, ff_dim=256, dropout=0):\n",
        "        super().__init__()\n",
        "        self.d_model, self.n_layers = d_model, n_layers\n",
        "        self.tok_emb = nn.Embedding(in_dim+1, d_model)\n",
        "        # self.blk = Seq(*[Mamba2(d_model, n_heads=n_heads) for _ in range(n_layers)])\n",
        "        self.blk = Seq(*[Block(d_model, n_heads=n_heads) for _ in range(n_layers)])\n",
        "        # self.out = nn.Linear(d_model, out_dim or in_dim)\n",
        "        # self.out = lambda x: x @ self.tok_emb.weight().T  # weight tying\n",
        "        self.out = lambda x: x @ self.tok_emb.weight[:-1].T # [vocab+1,d]->[d,vocab] # weight tying for nn.Embedding\n",
        "        emb_std = d_model**-.5\n",
        "        nn.init.trunc_normal_(self.tok_emb.weight, mean=0, std=emb_std, a=-3*emb_std, b=3*emb_std) # https://github.com/facebookresearch/blt/blob/main/bytelatent/model/local_models.py#L136\n",
        "\n",
        "    def forward(self, x, hid=None): # [b,t], [n_lyr,t,b,d]\n",
        "        x = self.tok_emb(x) # [b,t,d]\n",
        "        x, hid = self.blk(x, hid) # [b,t,d], list[b,t,t]\n",
        "        x = self.out(x)\n",
        "        return x, hid # [b,t,d], [n_lyr,t,b,d]\n",
        "\n",
        "# gpt 2\n",
        "# Parameters Layers dmodel\n",
        "# 117M 12 768 gpt1\n",
        "# 345M 24 1024\n",
        "# 762M 36 1280\n",
        "# 1542M 48 1600\n",
        "\n",
        "try: vocab_size=train_loader.dataset.vocab_size#50\n",
        "except NameError: vocab_size=50\n",
        "# d1024 12lyr h16, 768 8 12, 512 6 8, 256 4 8, , 64 1 8\n",
        "# 768 16 12, 768 12 18\n",
        "# model = GPT(vocab_size, d_model=512, out_dim=vocab_size, n_layers=3, n_heads=16).to(device)\n",
        "model = GPT(vocab_size, d_model=64, out_dim=vocab_size, n_layers=2, n_heads=8).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "# optim = torch.optim.AdamW(model.parameters(), 1e-3) # og betas=(0.9, 0.999), wd.01\n",
        "optim = torch.optim.AdamW(model.parameters(), 1e-3, betas=(0.9, 0.95), weight_decay=1e-1) # og wd.01\n",
        "# optim = torch.optim.AdamW(model.parameters(), 1e-3, weight_decay=1e-1) # og wd.01\n",
        "\n",
        "# gradclip important\n",
        "# betas, scheduler helps\n",
        "# diff num_heads not much diff, ~h_dim32 is good\n",
        "# tokemb init helps significantly\n",
        "# noinit init.02 < stableinit?\n",
        "\n",
        "\n",
        "x = torch.randint(0, vocab_size, (4, 128), device=device)\n",
        "out, hid = model(x)\n",
        "print(out.shape)\n",
        "print(type(hid), len(hid))\n",
        "print(type(hid[0]), len(hid[0]))\n",
        "# print(hid[0].shape)\n",
        "h_conv, h_ssm = hid[0]\n",
        "print(h_conv.shape, h_ssm.shape)\n",
        "\n",
        "# out, hid = model(x, hid)\n",
        "# print(out.shape)\n",
        "# # print(out.shape, hid.shape)\n",
        "# # print(out)\n",
        "\n",
        "\n",
        "\n",
        "# gpt d64 2lyr, 8,8head swiglu2 98624p\n",
        "# this is whatik Acounteats'nltenk IsDplardiopeng-601teack anat A oned alrenow\n",
        "# 700 time: 36.86555862426758 0.37161261685054414\n",
        "# strain 2.3351149559020996\n",
        "# this is whatt sas beme llot the the palerorste the theuf rald pecene has sed\n",
        "# this is what incluer is powerly idaismed said.\n",
        "# We why won woulded, unsers w\n",
        "# 6400 time: 2.9560906887054443 0.029622032612194364\n",
        "# strain 2.0638203620910645\n",
        "# this is what I gooding conreselfle landing of the comberold. Then is in the\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# b64,l128 8.0ram,10.0gpu l128*2:oom\n",
        "\n",
        "# b64,l128 w64 8.2ram,6.8gpu\n",
        "# b64,l128*2 w64 8.1ram,14.0gpu\n",
        "\n",
        "# swa64 128*2 7.4,12.0\n",
        "\n",
        "# d256 6.9,6.7\n",
        "\n",
        "# dim128 3lyr 3.0,.7\n",
        "# this is whats more of ietre'soni, they asari, your very\n",
        "# they vate. This iat\n",
        "# 9600 time: 5.7726781368255615 0.0006012580827081866\n",
        "# strain 1.481871247291565\n",
        "# this is what line wasn't good to both feel\n",
        "# like a delieve able or so foll,\n",
        "\n",
        "# mha\n",
        "# seq128*4+1 3.0,4.3\n",
        "# seq128*8+1 oom\n",
        "\n",
        "\n",
        "# seq128*8+1 w64:oom w32:2.4,7.8\n",
        "\n",
        "# lru 31s, strain 2\n",
        "\n",
        "\n",
        "# mamba2 no n_groups; dmodel512? expand1\n",
        "# this is what the nneed the asregative seem excibitions, politicals have not\n",
        "# 600 time: 103.91108822822571 1.0513688605557663\n",
        "# strain 2.6979057788848877\n",
        "# this is what involved programmess about disclarating diologios indeedoon Att\n",
        "# 700 time: 103.86492347717285 1.051418395933513\n",
        "# strain 1.6710600852966309\n",
        "# this is whating from the boyfrigpters of mind. Itâ€™s Calday Said this oil N\n",
        "# this is whati\n",
        "# Some Inthiamingly Up winder Center Is Total, who zave Lee Ley\n",
        "# 12700 time: 104.1569652557373 1.0526777770648592\n",
        "# strain 1.3009895086288452\n",
        "# this is whather one of the dispatch just requests for right. It also loses t\n",
        "# 12800 time: 104.19663453102112 1.052689873383442\n",
        "# strain 1.530670404434204\n",
        "# this is whathesis, as a shellissfringed, occurs with a higher environment, m\n",
        "# 12900 time: 104.17223763465881 1.052699970502316\n",
        "# strain 1.3385021686553955\n",
        "# this is whative tour. â€œI have here around the peace picking up. Sketch. F\n",
        "\n",
        "\n",
        "# d_model64, expand=2, n_heads=8, n_groups=1, d_state=8\n",
        "# this is what Aolthrest toit trow Wex. Bargreat Ghive vearred (\"But colled to\n",
        "# 700 time: 15.093467950820923 0.14656143011618272\n",
        "# strain 1.8304060697555542\n",
        "# this is whate stdatfor, gasted. Narts disauser atcordivicters meopestes Duc\n",
        "# this is whatop. The proselfense than-heâ€™s consepportants better whiched f\n",
        "# 6400 time: 14.46086049079895 0.15278742198291523\n",
        "# strain 1.930558681488037\n",
        "# this is whate of youâ€™t reenâ€™th his at at the bost effect\" in going disca\n",
        "\n",
        "\n",
        "# d_model64, expand=2, n_heads=8, n_groups=1, d_state=64 85872p\n",
        "# this is whathounce in Goment Kimb Malrangeeds fe) acted tecloods his about f\n",
        "# 700 time: 17.682673692703247 0.1755162530210661\n",
        "# strain 1.9856963157653809\n",
        "# this is whating Evanory doble even seextres sculgy Necres. Each Weatchile o\n",
        "# this is whature li late becaused kate: Hode washing â€™ ducinisating for the\n",
        "# 6400 time: 17.91191840171814 0.18049865636094028\n",
        "# strain 1.9032965898513794\n",
        "# this is what her Ferruminationed Horstaring Inâ€™t Beacthrough the lot dives\n",
        "\n",
        "# this is what a only to two iringly provides to enates and has man informatic\n",
        "# 6400 time: 2.138993501663208 0.023055557905929273\n",
        "# strain 1.705070972442627\n",
        "# this is whats besically governmentalone then a riting on pitch against has a\n",
        "# this is whate pulitary fquality and approtien and judging begged filed at cu\n",
        "# 11400 time: 2.413839817047119 0.02302427980295661\n",
        "# strain 2.1769118309020996\n",
        "# this is whative. Medies realists of brapk, more block object that hut that\n",
        "\n",
        "\n",
        "\n",
        "# d_model64, expand=2, n_heads=8, n_groups=8, d_state=8 85872p\n",
        "# this is whation. Fodicuractiong ligher highesebereptâ€™s Loves,Femmip or are\n",
        "# 700 time: 16.98404598236084 0.16986171767306907\n",
        "# strain 2.0250403881073\n",
        "# this is what buy ovethinaâ€™s not and Os, surves). We pronen: CI seam for fo\n",
        "# this is whating time a yyman stundanchy intelligende: â€œBuck: he, they ogr\n",
        "# 6400 time: 17.89343810081482 0.17402444938405792\n",
        "# strain 2.2177774906158447\n",
        "# this is whation, informer\", fhick during cources, 581 benefirention that sin\n",
        "# this is what poizeers colode. You cliping. '( Thouks 5. Saladmy, as any rec\n",
        "# 133700 time: 2.0944693088531494 0.023651174364440832\n",
        "# strain 1.7085469961166382\n",
        "# this is what Iâ€™m your know (IB2, it is including leaners, although office\n",
        "# 133800 time: 2.1193015575408936 0.02365037872852213\n",
        "# strain 1.8316044807434082\n",
        "# this is what drain the cutool us was cloud in dock Tauk--aQport to fibe othe\n",
        "# 133900 time: 2.1037728786468506 0.023649454599352675\n",
        "# strain 1.5083467960357666\n",
        "# this is what with the aware nashas made anTools-addropped or the covering pr\n",
        "\n",
        "\n",
        "# act(xbc+conv(xbc))\n",
        "# this is whats doliflalows Q nulalin Thare fompale hax re'p to 800ar corss.0\n",
        "# 700 time: 18.463230848312378 0.18301630190197649\n",
        "# strain 2.0806822776794434\n",
        "# this is what nit's as.; fomup Sare mised mally nears port of the pot to he\n",
        "# this is what in chis wurgest about Olrow, you wron ilanit it my for apcaid o\n",
        "# 2000 time: 17.46792197227478 0.18287333448430051\n",
        "# strain 2.216662645339966\n",
        "# this is what hetwar, and cyaker, â€œ-1Vï¿½ N!/ Rovego Pach to cickrep. â€” 2-t\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ngroups1/8/64 dstate 64/8/1\n",
        "# _no_weight_decay conv\n",
        "# _no_weight_decay dt A D\n",
        "# A init unif1,16\n",
        "# expand 1/2\n",
        "# pre/post norm\n",
        "# rms/layer norm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "2Nd-sGe6Ku4S"
      },
      "outputs": [],
      "source": [
        "# @title wandb\n",
        "!pip install -q wandb\n",
        "import wandb # https://docs.wandb.ai/quickstart\n",
        "wandb.login(key='487a2109e55dce4e13fc70681781de9f50f27be7')\n",
        "try: run.finish()\n",
        "except NameError: pass\n",
        "run = wandb.init(project=\"mamba\", config={\"model\": \"mamba\",}) #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrpJDgB3w49H"
      },
      "outputs": [],
      "source": [
        "# @title train generate\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "# torch.backends.cuda.matmul.allow_tf32 = True\n",
        "# torch.backends.cudnn.allow_tf32 = True\n",
        "# scaler = torch.GradScaler()\n",
        "\n",
        "# https://www.comet.com/site/blog/perplexity-for-llm-evaluation/\n",
        "def Perplexity(logits, target): # [b,t,vocab_size], [b,t]\n",
        "    log_probs = F.log_softmax(logits, dim=-1)\n",
        "    nll = -log_probs.gather(dim=-1, index=target.unsqueeze(-1)).squeeze(-1) # [b,t]\n",
        "    perplexity = nll.mean().exp()\n",
        "    return perplexity\n",
        "\n",
        "import time\n",
        "def strain(model, dataloader, optim, scheduler=None): # train function with automatic mixed precision\n",
        "    start = begin = time.time()\n",
        "    model.train()\n",
        "    for i, x in enumerate(dataloader):\n",
        "        x, y = x[:,:-1].to(device), x[:,1:].to(device)\n",
        "        # with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
        "        logits, _ = model(x) #output = [batch size, trg len - 1, output dim]\n",
        "        loss = F.cross_entropy(logits.flatten(0,1), y.flatten().to(int)) # [b*t,d], [b*t]\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        # scaler.scale(loss).backward()\n",
        "        # # scaler.unscale_(optimizer)\n",
        "        # # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
        "        # scaler.step(optimizer)\n",
        "        # scaler.update()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "        if i % 100 == 0:\n",
        "            print(i, 'time:',time.time() - start, (time.time()-begin)/(i+1))\n",
        "            print(\"strain\",loss.item())\n",
        "            print(generate(model, \"this is what\"))\n",
        "            model.train()\n",
        "            # perplexity = Perplexity(logits.detach(), y).item()\n",
        "            start = time.time()\n",
        "        try: wandb.log({\"train loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "\n",
        "def generate(model, context, max_steps=64, temperature=1):\n",
        "    x = encode(context)#.to(device)\n",
        "    model.eval()\n",
        "    hid=None\n",
        "    for n in range(max_steps):\n",
        "        with torch.no_grad(): out, hid = model(x if hid==None else x[:,:-1], hid)\n",
        "        # with torch.no_grad(): out, hid = model(x, hid)\n",
        "        # print('generate', out.shape)\n",
        "        out = out[:,-1] # get logit for last character\n",
        "        out = F.softmax(out/temperature, dim=-1) # vocab_size to char\n",
        "        ix = torch.multinomial(out, num_samples=1) # rand sample by output distribution\n",
        "        x = torch.cat([x, ix], dim=1)\n",
        "    completion = decode(x.squeeze(0))\n",
        "    return completion\n",
        "\n",
        "# scheduler = get_cosine_schedule_with_warmup(optim, num_warmup_steps=400, num_training_steps=40000) # https://docs.pytorch.org/torchtune/0.2/generated/torchtune.modules.get_cosine_schedule_with_warmup.html\n",
        "# import time\n",
        "# start = begin = time.time()\n",
        "for i in range(1):\n",
        "    # train_loss = strain(model, train_loader, optim, scheduler=None)\n",
        "    strain(model, train_loader, optim, scheduler=None)\n",
        "    # strain(model, train_loader, optim, scheduler=scheduler)\n",
        "    # print(generate(model, \"this is what\"))\n",
        "    # print(i, 'time:',time.time() - start, (time.time()-begin)/(i+1))\n",
        "    # start = time.time()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROjbICJjf3Ct"
      },
      "outputs": [],
      "source": [
        "print(generate(model, \"this is what\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPX5CP0PWM6K"
      },
      "source": [
        "### hydra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5ptrY6YB5RU5"
      },
      "outputs": [],
      "source": [
        "# @title hydra me\n",
        "# https://github.com/goombalab/hydra/blob/main/hydra/modules/hydra.py\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class Hydra(nn.Module):\n",
        "    # def __init__(self, d_model, expand=2, n_heads=8, n_groups=1, d_state=64, d_conv=4):\n",
        "    def __init__(self, d_model, expand=2, n_heads=8, n_groups=8, d_state=8, d_conv=7):\n",
        "        super().__init__()\n",
        "        self.d_model, self.n_groups, self.d_state, self.d_conv = d_model, n_groups, d_state, d_conv\n",
        "        self.d_inner = expand * self.d_model\n",
        "        self.n_heads, self.d_head = n_heads, self.d_inner//n_heads\n",
        "\n",
        "        self.in_proj = nn.Linear(self.d_model, 2* self.d_inner + 2* (2*self.n_groups*self.d_state) + 2* self.n_heads, bias=False) # z,x,B,C,dt\n",
        "        conv_dim = self.d_inner + 2* (2*self.n_groups*self.d_state) # for x,B,C\n",
        "        # print(d_conv, conv_dim)\n",
        "        self.conv1d = nn.Conv1d(conv_dim, conv_dim, bias=True, kernel_size=d_conv, groups=conv_dim, padding=d_conv//2)\n",
        "        # self.conv1d.weight._no_weight_decay = True\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "        # self.h0 = nn.Parameter(torch.zeros(self.n_heads, self.d_head, self.d_state))\n",
        "        # self.h0._no_weight_decay = True\n",
        "        dt_min, dt_max = .001, .1\n",
        "        dt = torch.exp(torch.rand(self.n_heads) * (math.log(dt_max)-math.log(dt_min)) + math.log(dt_min)).clamp(min=1e-4)\n",
        "        self.dt_bias = nn.Parameter(dt + torch.log(-torch.expm1(-dt))) # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n",
        "        self.dt_bias._no_weight_decay = True\n",
        "        # A = torch.empty(self.n_heads, dtype=torch.float32).uniform_(1,16)\n",
        "        A = torch.ones(self.n_heads, dtype=torch.float32)\n",
        "        A_log = torch.log(A)\n",
        "        self.A_log = nn.Parameter(A_log)\n",
        "        self.A_log._no_weight_decay = True\n",
        "        self.D = nn.Parameter(torch.ones(self.n_heads))\n",
        "        self.D._no_weight_decay = True\n",
        "        self.fc_D = nn.Linear(self.d_inner, self.n_heads, bias=False)\n",
        "        self.norm = nn.RMSNorm(self.d_inner)\n",
        "        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=False)\n",
        "\n",
        "    def forward(self, u): # [b,t,d]\n",
        "        b = u.shape[0]\n",
        "        # print('Hydra u', u.shape)\n",
        "        A = -torch.exp(self.A_log) # [n_heads]\n",
        "        z, xBC, dt = self.in_proj(u).split([self.d_inner, self.d_inner + 2* (2*self.n_groups*self.d_state), 2* self.n_heads], dim=-1)\n",
        "\n",
        "        dt = torch.cat((dt[:, :, :self.n_heads], torch.flip(dt[:, :, self.n_heads:], (1,))), dim=0)\n",
        "        dt = F.softplus(dt+self.dt_bias) # [b,t,h]\n",
        "\n",
        "        # xBC = self.act(self.conv1d(xBC.transpose(-2,-1)).transpose(-2,-1)[:,:u.size(1),:])  # [b,t, d_inner + 2* n_groups*d_state]\n",
        "        xBC = self.act(self.conv1d(xBC.transpose(-2,-1)).transpose(-2,-1))  # [b,t, d_inner + 2* n_groups*d_state]\n",
        "        # x, B, C = xBC.split([self.n_heads*self.d_head, self.n_groups*self.d_state, self.n_groups*self.d_state], dim=-1) # x:[b,t,h,d], B/C:[b,t,g,s] # X, B, C correspond to V, K, Q respectively in the SSM/attention duality\n",
        "        x, BC = xBC.split([self.d_inner, 2* (2*self.n_groups*self.d_state)], dim=-1)\n",
        "        x_og = x\n",
        "        x = torch.cat((x, torch.flip(x,(1,))), dim=0)\n",
        "        BC = torch.cat((BC[:,:,:2*self.n_groups*self.d_state], torch.flip(BC[:,:,2*self.n_groups*self.d_state:], (1,))), dim=0)\n",
        "        B, C = BC.split([self.n_groups * self.d_state, self.n_groups * self.d_state], dim=-1)\n",
        "        x, B, C = x.unflatten(-1, (self.n_heads, self.d_head)), B.unflatten(-1, (self.n_groups, self.d_state)), C.unflatten(-1, (self.n_groups, self.d_state))\n",
        "\n",
        "\n",
        "        # h0 = self.h0.expand(u.size(0),-1,-1,-1) # [b,n,d,s]\n",
        "        # print('x dt a b', x.shape, dt.shape, A.shape, B.shape)\n",
        "\n",
        "        # y, h_ssm = ssd(x*dt.unsqueeze(-1), A*dt, B, C, 64, h0) # 256\n",
        "        y, h_ssm = ssd(x*dt.unsqueeze(-1), A*dt, B, C, 64) # 256\n",
        "\n",
        "        y = y + x * self.D.unsqueeze(-1)\n",
        "        # y = self.norm(y.flatten(2) * self.act(z)) # [b,t,d_inner] # norm(x)*silu(z) if norm_before_gate, else norm(x*silu(z)) # https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/triton/layernorm_gated.py#L18\n",
        "\n",
        "        y = y.flatten(2) # [b,l,inner]\n",
        "        y = torch.roll(y, shifts=1, dims=1) # 123...l -> l12...l-1\n",
        "        y[:,0,:] = 0. # 012...l-1\n",
        "        y_fw, y_bw = y[:b], torch.flip(y[b:], (1,))\n",
        "        y = y_fw + y_bw + x_og * F.linear(x_og, self.fc_D.weight, bias=self.D).repeat(1,1,self.d_head)\n",
        "\n",
        "\n",
        "        y = self.norm(y.flatten(2)) * self.act(z)\n",
        "        out = self.out_proj(y)\n",
        "        return out # [b,t,in]\n",
        "\n",
        "\n",
        "\n",
        "b,t,d_model=5,256,32\n",
        "# b,t,d_model=5,7,32\n",
        "d_model = 64\n",
        "u = torch.randn(b,t,d_model, device=device)\n",
        "model = Hydra(d_model).to(device)\n",
        "out, h = model(u)\n",
        "# h0 = torch.randn(b, model.n_heads, model.d_head, model.d_state)\n",
        "# print(out.shape)\n",
        "# print(out.shape, h.shape)\n",
        "# print(out[0,-3:,:5], h[0,:2,:5,:5])\n",
        "# out, h = model(u, h)\n",
        "\n",
        "# u = torch.randn(b,7,d_model, device=device)\n",
        "# # out, h = model(u[:,-1:], h)\n",
        "# out, h = model.step(u, h)\n",
        "# out, h = model.step(u)\n",
        "print(out.shape)\n",
        "# print(out[0,-3:,:5], h[0,:2,:5,:5])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XAO3ggmTlXV3"
      },
      "outputs": [],
      "source": [
        "# @title goombalab hydra.py\n",
        "# https://github.com/goombalab/hydra/blob/main/hydra/modules/hydra.py\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from einops import rearrange, repeat\n",
        "try:\n",
        "    from mamba_ssm.ops.triton.layernorm_gated import RMSNorm as RMSNormGated\n",
        "except ImportError:\n",
        "    RMSNormGated = None\n",
        "from mamba_ssm.ops.triton.ssd_combined import mamba_chunk_scan_combined\n",
        "from hydra.modules.ops import hydra_split_conv1d_scan_combined\n",
        "\n",
        "\n",
        "class Hydra(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model,\n",
        "        d_state=64,\n",
        "        d_conv=7,\n",
        "        conv_init=None,\n",
        "        expand=2,\n",
        "        headdim=64,\n",
        "        ngroups=1,\n",
        "        dt_min=0.001,\n",
        "        dt_max=0.1,\n",
        "        dt_init_floor=1e-4,\n",
        "        dt_limit=(0.0, float(\"inf\")),\n",
        "        learnable_init_states=False,\n",
        "        activation=\"swish\",\n",
        "        bias=False,\n",
        "        conv_bias=True,\n",
        "        # Fused kernel and sharding options\n",
        "        chunk_size=256,\n",
        "        layer_idx=None,  # Absorb kwarg for general module\n",
        "        device=None,\n",
        "        dtype=None,\n",
        "    ):\n",
        "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_state = d_state\n",
        "        self.d_conv = d_conv\n",
        "        self.conv_init = conv_init\n",
        "        self.expand = expand\n",
        "        self.d_inner = self.expand * self.d_model\n",
        "        self.headdim = headdim\n",
        "        self.ngroups = ngroups\n",
        "        assert self.d_inner % self.headdim == 0\n",
        "        self.nheads = self.d_inner // self.headdim\n",
        "        self.dt_limit = dt_limit\n",
        "        self.learnable_init_states = learnable_init_states\n",
        "        self.activation = activation\n",
        "        self.chunk_size = chunk_size\n",
        "        self.layer_idx = layer_idx\n",
        "\n",
        "        # Order: [z, x, B, C, dt]\n",
        "        d_in_proj = 2 * self.d_inner + 2 * (2 * self.ngroups * self.d_state) + 2 * self.nheads\n",
        "        self.in_proj = nn.Linear(self.d_model, d_in_proj, bias=bias, **factory_kwargs)\n",
        "\n",
        "        conv_dim = self.d_inner + 2 * (2 * self.ngroups * self.d_state)\n",
        "        self.conv1d = nn.Conv1d(\n",
        "            in_channels=conv_dim,\n",
        "            out_channels=conv_dim,\n",
        "            bias=conv_bias,\n",
        "            kernel_size=d_conv,\n",
        "            groups=conv_dim,\n",
        "            padding=d_conv // 2,\n",
        "            **factory_kwargs,\n",
        "        )\n",
        "        if self.conv_init is not None:\n",
        "            nn.init.uniform_(self.conv1d.weight, -self.conv_init, self.conv_init)\n",
        "        # self.conv1d.weight._no_weight_decay = True\n",
        "\n",
        "        if self.learnable_init_states:\n",
        "            self.init_states = nn.Parameter(torch.zeros(self.nheads, self.headdim, self.d_state, **factory_kwargs))\n",
        "            self.init_states._no_weight_decay = True\n",
        "\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "        # Initialize log dt bias\n",
        "        dt = torch.exp(torch.rand(self.nheads, **factory_kwargs) * (math.log(dt_max) - math.log(dt_min)) + math.log(dt_min))\n",
        "        dt = torch.clamp(dt, min=dt_init_floor)\n",
        "        # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n",
        "        inv_dt = dt + torch.log(-torch.expm1(-dt))\n",
        "        self.dt_bias = nn.Parameter(inv_dt)\n",
        "        # Just to be explicit. Without this we already don't put wd on dt_bias because of the check\n",
        "        # name.endswith(\"bias\") in param_grouping.py\n",
        "        self.dt_bias._no_weight_decay = True\n",
        "\n",
        "        # A parameter\n",
        "        A = torch.ones(self.nheads, dtype=torch.float32, device=device)\n",
        "        A_log = torch.log(A).to(dtype=dtype)\n",
        "        self.A_log = nn.Parameter(A_log)\n",
        "        # self.register_buffer(\"A_log\", torch.zeros(self.nheads, dtype=torch.float32, device=device), persistent=True)\n",
        "        self.A_log._no_weight_decay = True\n",
        "\n",
        "        # D \"skip\" parameter\n",
        "        self.D = nn.Parameter(torch.ones(self.nheads, device=device))\n",
        "        self.D._no_weight_decay = True\n",
        "        self.fc_D = nn.Linear(self.d_inner, self.nheads, bias=False, **factory_kwargs)\n",
        "\n",
        "        # Extra normalization layer right before output projection\n",
        "        assert RMSNormGated is not None\n",
        "        self.norm = RMSNormGated(self.d_inner, eps=1e-5, norm_before_gate=True, **factory_kwargs)\n",
        "\n",
        "        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=bias, **factory_kwargs)\n",
        "\n",
        "    def forward(self, u, seq_idx=None): # [b,l,d]\n",
        "        batch, seqlen, dim = u.shape\n",
        "        zxbcdt = self.in_proj(u)  # (B, L, d_in_proj)\n",
        "        A = -torch.exp(self.A_log.float())  # (nheads) or (d_inner, d_state)\n",
        "        initial_states = repeat(self.init_states, \"... -> b ...\", b=2*batch) if self.learnable_init_states else None\n",
        "        dt_limit_kwargs = {} if self.dt_limit == (0.0, float(\"inf\")) else dict(dt_limit=self.dt_limit)\n",
        "        z, xBC, dt = torch.split(zxbcdt, [self.d_inner, self.d_inner + 2 * (2 * self.ngroups * self.d_state), 2 * self.nheads], dim=-1)\n",
        "\n",
        "        dt = torch.cat((dt[:, :, :self.nheads], torch.flip(dt[:, :, self.nheads:], (1,))), dim=0)\n",
        "        dt = F.softplus(dt + self.dt_bias)  # (2 * B, L, nheads)\n",
        "        assert self.activation in [\"silu\", \"swish\"]\n",
        "\n",
        "        # 1D Convolution\n",
        "        xBC = self.act(\n",
        "            self.conv1d(xBC.transpose(1, 2)).transpose(1, 2)\n",
        "        )  # (B, L, self.d_inner + 2 * (2 * ngroups * d_state))\n",
        "\n",
        "        # Split into 3 main branches: X, B, C\n",
        "        # These correspond to V, K, Q respectively in the SSM/attention duality\n",
        "        x, BC = torch.split(xBC, [self.d_inner, 2 * (2 * self.ngroups * self.d_state)], dim=-1)\n",
        "        x_og = x\n",
        "        x = torch.cat((x, torch.flip(x, (1,))), dim=0)\n",
        "        BC = torch.cat(\n",
        "            (BC[:, :, :2 * self.ngroups * self.d_state],\n",
        "             torch.flip(BC[:, :, 2 * self.ngroups * self.d_state:], (1,))),\n",
        "            dim=0\n",
        "        )\n",
        "        B, C = torch.split(BC, [self.ngroups * self.d_state, self.ngroups * self.d_state], dim=-1)\n",
        "\n",
        "        y = mamba_chunk_scan_combined(\n",
        "            rearrange(x, \"b l (h p) -> b l h p\", p=self.headdim),\n",
        "            dt,\n",
        "            A,\n",
        "            rearrange(B, \"b l (g n) -> b l g n\", g=self.ngroups),\n",
        "            rearrange(C, \"b l (g n) -> b l g n\", g=self.ngroups),\n",
        "            chunk_size=self.chunk_size,\n",
        "            D=None,\n",
        "            z=None,\n",
        "            seq_idx=seq_idx,\n",
        "            initial_states=initial_states,\n",
        "            **dt_limit_kwargs,\n",
        "        )\n",
        "        y = rearrange(y, \"b l h p -> b l (h p)\")\n",
        "        y = torch.roll(y, shifts=1, dims=1)\n",
        "        y[:, 0, :] = 0.0\n",
        "        y_fw, y_bw = y[:batch], torch.flip(y[batch:], (1,))\n",
        "        y = y_fw + y_bw + x_og * repeat(\n",
        "            F.linear(x_og, self.fc_D.weight, bias=self.D), \"b l h -> b l (h p)\", p=self.headdim\n",
        "        )\n",
        "\n",
        "        # Multiply \"gate\" branch and apply extra normalization layer\n",
        "        y = self.norm(y, z)\n",
        "        out = self.out_proj(y)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XQqp-uYIMWs"
      },
      "source": [
        "## mamba implementations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pYvDiHSghoDL"
      },
      "outputs": [],
      "source": [
        "# @title ssd\n",
        "# https://goombalab.github.io/blog/2024/mamba2-part4-systems/\n",
        "# https://tridao.me/blog/2024/mamba2-part3-algorithm/\n",
        "# # https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/triton/ssd_combined.py\n",
        "# y = mamba_chunk_scan_combined(x.unflatten(-1, (self.n_heads, self.d_head)), dt, A, # chunk_size=256,\n",
        "#     B.unflatten(-1, (self.ngroups, self.d_state)), C.unflatten(-1, (self.ngroups, self.d_state)),\n",
        "#     D=self.D, z=None).flatten(2) # norm before flatten? so norm is within group\n",
        "\n",
        "def segsum(x):\n",
        "    \"\"\"Naive segment sum calculation. exp(segsum(A)) produces a 1-SS matrix,\n",
        "       which is equivalent to a scalar SSM.\"\"\"\n",
        "    T = x.size(-1)\n",
        "    x_cumsum = torch.cumsum(x, dim=-1)\n",
        "    x_segsum = x_cumsum[...,:,None] - x_cumsum[...,None,:]\n",
        "    mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool), diagonal=0)\n",
        "    x_segsum = x_segsum.masked_fill(~mask, -torch.inf)\n",
        "    # mask = torch.triu(torch.ones(T, T, device=x.device, dtype=bool))\n",
        "    # x_segsum = x_segsum.masked_fill(mask, -torch.inf)\n",
        "    return x_segsum\n",
        "\n",
        "def ssd(X, A, B, C, block_len=64, h0=None): # X:[b,t,h,d], A:[b,t,h], B:[b,t,g,s], C:[b,t,g,s]\n",
        "    assert X.dtype == A.dtype == B.dtype == C.dtype\n",
        "    assert X.shape[1] % block_len == 0\n",
        "\n",
        "    X, A, B, C = [x.unflatten(1, (-1,block_len)) for x in (X, A, B, C)]\n",
        "    A = A.permute(0,3,1,2) # [b,t/block_len,block_len,h] -> [b,h,t/block_len,block_len]\n",
        "    A_cumsum = torch.cumsum(A, dim=-1)\n",
        "    # 1. Compute the output for each intra-chunk (diagonal blocks)\n",
        "    L = torch.exp(segsum(A))\n",
        "    Y_diag  = torch.einsum(\"bclhn,bcshn,bhcls,bcshp->bclhp\", C, B, L, X)\n",
        "\n",
        "    # 2. Compute the state for each intra-chunk\n",
        "    # (right term of low-rank factorization of off-diagonal blocks; B terms)\n",
        "    decay_states = torch.exp((A_cumsum[:,:,:,-1:] - A_cumsum))\n",
        "    states = torch.einsum(\"bclhn,bhcl,bclhp->bchpn\", B, decay_states, X)\n",
        "\n",
        "    # 3. Compute the inter-chunk SSM recurrence; produces correct SSM states at chunk boundaries\n",
        "    # (middle term of factorization of off-diag blocks; A terms)\n",
        "    # if h0==None: h0 = torch.zeros_like(states[:,:1])\n",
        "    if h0==None: h0 = torch.zeros(states.size(0),*states.shape[2:], device=device) # [b,h]\n",
        "    # print('ssd', h0.shape, states.shape, [states.size(0),1,*states.shape[2:]])\n",
        "    states = torch.cat([h0.unsqueeze(1), states], dim=1)\n",
        "    decay_chunk = torch.exp(segsum(F.pad(A_cumsum[:,:,:,-1], (1,0))))\n",
        "    new_states = torch.einsum(\"bhzc,bchpn->bzhpn\", decay_chunk, states)\n",
        "\n",
        "    # 4. Compute state -> output conversion per chunk\n",
        "    # (left term of low-rank factorization of off-diagonal blocks; C terms)\n",
        "    Y_off = torch.einsum('bclhn,bchpn,bhcl->bclhp', C, new_states[:,:-1], torch.exp(A_cumsum))\n",
        "\n",
        "    # Add output of intra-chunk and inter-chunk terms (diagonal and off-diagonal blocks)\n",
        "    Y = (Y_diag+Y_off).flatten(1,2)\n",
        "    return Y, new_states[:,-1] # [b,t,n_heads,d_head],\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Oup8iBkQ1fa5"
      },
      "outputs": [],
      "source": [
        "# @title mamba_ssm/modules/mamba2_simple.py next\n",
        "# https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba2_simple.py\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class Mamba2(nn.Module):\n",
        "    # def __init__(self, d_model, expand=2, n_heads=8, n_groups=1, d_state=64, d_conv=4):\n",
        "    def __init__(self, d_model, expand=2, n_heads=8, n_groups=8, d_state=8, d_conv=4):\n",
        "        super().__init__()\n",
        "        n_groups = min(n_heads, n_groups)\n",
        "        assert n_heads % n_groups == 0, \"nheads must be divisible by ngroups\"\n",
        "        self.d_model, self.n_groups, self.d_state, self.d_conv = d_model, n_groups, d_state, d_conv\n",
        "        self.d_inner = expand * self.d_model\n",
        "        self.n_heads, self.d_head = n_heads, self.d_inner//n_heads\n",
        "\n",
        "        self.in_proj = nn.Linear(self.d_model, 2* self.d_inner + 2* self.n_groups*self.d_state + self.n_heads, bias=False) # z,x,B,C,dt\n",
        "        conv_dim = self.d_inner + 2* self.n_groups*self.d_state # for x,B,C\n",
        "        self.conv1d = nn.Conv1d(in_channels=conv_dim, out_channels=conv_dim, bias=True, kernel_size=d_conv, groups=conv_dim, padding=d_conv-1)\n",
        "        # self.conv1d.weight._no_weight_decay = True\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "        # self.h0 = nn.Parameter(torch.zeros(self.n_heads, self.d_head, self.d_state))\n",
        "        # self.h0._no_weight_decay = True\n",
        "        dt_min, dt_max = .001, .1\n",
        "        dt = torch.exp(torch.rand(self.n_heads) * (math.log(dt_max)-math.log(dt_min)) + math.log(dt_min)).clamp(min=1e-4)\n",
        "        self.dt_bias = nn.Parameter(dt + torch.log(-torch.expm1(-dt))) # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n",
        "        self.dt_bias._no_weight_decay = True\n",
        "        A = torch.empty(self.n_heads, dtype=torch.float32).uniform_(1,16) # 1,16 ; .2,2\n",
        "        self.A_log = nn.Parameter(torch.log(A))\n",
        "        self.A_log._no_weight_decay = True\n",
        "        self.D = nn.Parameter(torch.ones(self.n_heads))\n",
        "        self.D._no_weight_decay = True\n",
        "        self.norm = nn.RMSNorm(self.d_inner)\n",
        "        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=False)\n",
        "\n",
        "    def forward(self, u, h0=None): # [b,t,d]\n",
        "    # def forward(self, u): # [b,t,d]\n",
        "        # if h0!=None: return self.step(u, h0)\n",
        "        # print('Mamba2Simple u', u.shape)\n",
        "        A = -torch.exp(self.A_log) # [n_heads]\n",
        "        z, xBC, dt = self.in_proj(u).split([self.d_inner, self.d_inner + 2* self.n_groups*self.d_state, self.n_heads], dim=-1)\n",
        "        dt = F.softplus(dt+self.dt_bias) # [b,t,h]\n",
        "\n",
        "        h_conv = F.pad(xBC, (0,0,self.d_conv-u.shape[1],0)) # [b,k,xbc]\n",
        "        # xBC = self.act(self.conv1d(xBC.transpose(-2,-1)).transpose(-2,-1)[:,:u.size(1),:])  # [b,t, d_inner + 2* ngroups*d_state]\n",
        "        # print(xBC[:,:-1].shape, self.act(self.conv1d(xBC.transpose(-2,-1)).transpose(-2,-1)[:,:u.size(1)]).shape)\n",
        "        # xBC = xBC[:,:-1] + self.act(self.conv1d(xBC.transpose(-2,-1)).transpose(-2,-1)[:,:u.size(1)])  # [b,t, d_inner + 2* ngroups*d_state]\n",
        "        x, B, C = xBC.split([self.n_heads*self.d_head, self.n_groups*self.d_state, self.n_groups*self.d_state], dim=-1) # x:[b,t,h,d], B/C:[b,t,g,s] # X, B, C correspond to V, K, Q respectively in the SSM/attention duality\n",
        "        x, B, C = x.unflatten(-1, (self.n_heads, self.d_head)), B.unflatten(-1, (self.n_groups, self.d_state)), C.unflatten(-1, (self.n_groups, self.d_state))\n",
        "        # h0 = self.h0.expand(u.size(0),-1,-1,-1) # [b,n,d,s]\n",
        "\n",
        "        # dt = F.softplus(dt + dt_bias) # [b,h,d]\n",
        "        # dA = torch.exp(dt.unsqueeze(-1) * A)  # [b,h,d,1]*[h,d,s]->[b,h,d,s]\n",
        "        # # B = repeat(B, \"b g n -> b (g h) n\", h=nheads // ngroups)  # (batch, nheads, dstate)\n",
        "        # B = B.repeat(1,nheads//ngroups,1) # [b,g,s]->[b,h,s]\n",
        "        # print(self.n_heads//self.n_groups)\n",
        "        h_g = max(1, self.n_heads//self.n_groups)\n",
        "        B, C = B.repeat_interleave(h_g, dim=-2), C.repeat_interleave(h_g, dim=-2) # [b,g,s]->[b,h,s]\n",
        "        # dB = dt.unsqueeze(-1) * B.unsqueeze(-2) # [b,h,d,1]*[b,h,1,s]->[b,h,d,s]\n",
        "        # state.copy_(state * dA + dB * x.unsqueeze(-1)) # [b,h,d,s]\n",
        "        # out = torch.einsum(\"bhdn,bhn->bhd\", state.to(C.dtype), C) # [b,h,d]\n",
        "        # out += (x * D).to(out.dtype)\n",
        "        # out = out * F.silu(z).to(x.dtype)\n",
        "\n",
        "        # print(h_g)\n",
        "        # print('x dt a b', x.shape, dt.shape, A.shape, B.shape)\n",
        "\n",
        "        # # y, h_ssm = ssd(x*dt.unsqueeze(-1), A*dt, B, C, 64, h0) # 256\n",
        "        y, h_ssm = ssd(x*dt.unsqueeze(-1), A*dt, B, C, 64) # 256\n",
        "\n",
        "\n",
        "        # dA = torch.exp(dt*A)[...,None,None] # [b,t,h,1,1]\n",
        "        # dBx = torch.einsum(\"bth,btgs,bthd->bthds\", dt, B, x)\n",
        "        # # h_ssm = seq(dA, dBx, h0=None) # [b,t,d], [b,t,d], [b,d]\n",
        "        # h_ssm = lcse(dA+0j, dBx+0j, h0=None).real # [b,t,d], [b,t,d], [b,d]\n",
        "        # y = torch.einsum(\"bthds,btgs->bthd\", h_ssm, C) # [b,h,d,s]*[b,h,s]\n",
        "\n",
        "\n",
        "        y = y + x * self.D.unsqueeze(-1)\n",
        "        # y = self.norm(y.flatten(2) * self.act(z)) # [b,t,d_inner] # norm(x)*silu(z) if norm_before_gate, else norm(x*silu(z)) # https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/triton/layernorm_gated.py#L18\n",
        "        y = self.norm(y.flatten(2)) * self.act(z)\n",
        "        out = self.out_proj(y)\n",
        "        return out, (h_conv, h_ssm) # [b,t,in], [b,h,d,s]\n",
        "        # return out, (h_conv, h_ssm[:,-1]) # [b,t,in], [b,h,d,s]\n",
        "\n",
        "    # def step(self, u, h=None): # [b,t,in], (conv[b,k,xbc], ssm[b,h,d,s])\n",
        "    #     u = u[:,-self.d_conv:] # [b,min(k,t),in]\n",
        "    #     A = -torch.exp(self.A_log)  # [n_heads]\n",
        "    #     z, xBC, dt = self.in_proj(u).split([self.d_inner, self.d_inner + 2* self.n_groups*self.d_state, self.n_heads], dim=-1) # [b,min(k,t),_]\n",
        "    #     dt = F.softplus(dt[:,-1]+self.dt_bias) # [b,n_heads]\n",
        "\n",
        "    #     if h==None: h_conv = F.pad(xBC, (0,0,max(0,self.d_conv-u.shape[1]),0))\n",
        "    #     else: h_conv = torch.cat([h[0], xBC], dim=1)[:,-self.d_conv:] # [b,k,xbc]\n",
        "    #     # print(h_conv.shape, self.conv1d.weight.shape, self.conv1d.bias.shape)\n",
        "\n",
        "    #     # xBC = F.silu(torch.einsum(\"bkc,cik->bc\", h_conv, self.conv1d.weight) + self.conv1d.bias) # [b,kernel,xbc], [xbc,1,kernel], +[xbc] -> [b,xbc]\n",
        "    #     # xBC = xBC + F.silu(torch.einsum(\"bkc,cik->bc\", h_conv, self.conv1d.weight) + self.conv1d.bias) # [b,kernel,xbc], [xbc,1,kernel], +[xbc] -> [b,xbc]\n",
        "    #     xBC = h_conv[:,-1]\n",
        "    #     x, B, C = xBC.split([self.n_heads*self.d_head, self.n_groups*self.d_state, self.n_groups*self.d_state], dim=-1) # x:[b,t,h,d], B/C:[b,t,g,s] # X, B, C correspond to V, K, Q respectively in the SSM/attention duality\n",
        "    #     x, B, C = x.unflatten(-1, (self.n_heads, self.d_head)), B.unflatten(-1, (self.n_groups, self.d_state)), C.unflatten(-1, (self.n_groups, self.d_state))\n",
        "    #     h_g = max(1, self.n_heads//self.n_groups)\n",
        "    #     B, C = B.repeat_interleave(h_g, dim=-2), C.repeat_interleave(h_g, dim=-2) # [b,g,s]->[b,h,s]\n",
        "\n",
        "    #     # SSM step\n",
        "    #     dA = torch.exp(dt*A) # [b,n_heads] # https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/triton/selective_state_update.py#L104\n",
        "    #     # print('dt, B, x', dt.shape, B.shape, x.shape)\n",
        "    #     dBx = torch.einsum(\"bh,bhs,bhd->bhds\", dt, B, x)\n",
        "\n",
        "    #     if h==None: h_ssm = dBx\n",
        "    #     else: h_ssm = h[1] * dA[...,None,None] + dBx # [b,h,d,s]*[b,h,1,1]+[b,h,d,s]\n",
        "    #     # print('h.ssm_state, C', h.ssm_state.shape, C.shape)\n",
        "    #     y = torch.einsum(\"bhds,bhs->bhd\", h_ssm, C) # [b,h,d,s]*[b,h,s]\n",
        "\n",
        "    #     y = y + self.D.unsqueeze(-1) * x\n",
        "    #     # print('step', y.shape,z.shape) # [5, 8, 16]) torch.Size([5, 4, 128]\n",
        "    #     # y = self.norm(y.flatten(1) * F.silu(z[:,-1]))\n",
        "    #     y = self.norm(y.flatten(1)) * F.silu(z[:,-1])\n",
        "    #     y = self.out_proj(y)\n",
        "    #     return y.unsqueeze(1), (h_conv, h_ssm)\n",
        "\n",
        "\n",
        "\n",
        "# b,t,d_model=5,256,32\n",
        "b,t,d_model=5,7,32\n",
        "d_model = 64\n",
        "u = torch.randn(b,t,d_model, device=device)\n",
        "model = Mamba2(d_model).to(device)\n",
        "out, h = model(u)\n",
        "# h0 = torch.randn(b, model.n_heads, model.d_head, model.d_state)\n",
        "# print(out.shape)\n",
        "# print(out.shape, h.shape)\n",
        "# print(out[0,-3:,:5], h[0,:2,:5,:5])\n",
        "# out, h = model(u, h)\n",
        "\n",
        "# u = torch.randn(b,7,d_model, device=device)\n",
        "# # out, h = model(u[:,-1:], h)\n",
        "# out, h = model.step(u, h)\n",
        "# # out, h = model.step(u)\n",
        "# print(out.shape)\n",
        "# print(out[0,-3:,:5], h[0,:2,:5,:5])\n",
        "\n",
        "\n",
        "# cpu seq ~40-41s\n",
        "# this is what sherti, epow sour wocks I rmtre herring' Coer alaid it the adoo\n",
        "# 700 time: 43.38966917991638 0.3799727667075251\n",
        "# strain 2.045107126235962\n",
        "# this is whate lilln the Tont to pleesyecise merefes asse sperced is a a cagk\n",
        "\n",
        "# gpu ssd seqlen 2^11 29.5s\n",
        "\n",
        "# lcsm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "u7ZtfZ7IyE4u"
      },
      "outputs": [],
      "source": [
        "# @title mamba_ssm/modules/mamba2_simple.py works\n",
        "# https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba2_simple.py\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class Mamba2Simple(nn.Module):\n",
        "    # def __init__(self, d_model, expand=2, n_heads=8, n_groups=1, d_state=64, d_conv=4):\n",
        "    def __init__(self, d_model, expand=2, n_heads=8, n_groups=8, d_state=8, d_conv=4):\n",
        "        super().__init__()\n",
        "        self.d_model, self.n_groups, self.d_state, self.d_conv = d_model, n_groups, d_state, d_conv\n",
        "        self.d_inner = expand * self.d_model\n",
        "        self.n_heads, self.d_head = n_heads, self.d_inner//n_heads\n",
        "\n",
        "        self.in_proj = nn.Linear(self.d_model, 2* self.d_inner + 2* self.n_groups*self.d_state + self.n_heads, bias=False) # z,x,B,C,dt\n",
        "        conv_dim = self.d_inner + 2* self.n_groups*self.d_state # for x,B,C\n",
        "        self.conv1d = nn.Conv1d(in_channels=conv_dim, out_channels=conv_dim, bias=True, kernel_size=d_conv, groups=conv_dim, padding=d_conv-1)\n",
        "        # self.conv1d.weight._no_weight_decay = True\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "        # self.h0 = nn.Parameter(torch.zeros(self.n_heads, self.d_head, self.d_state))\n",
        "        # self.h0._no_weight_decay = True\n",
        "        dt_min, dt_max = .001, .1\n",
        "        dt = torch.exp(torch.rand(self.n_heads) * (math.log(dt_max)-math.log(dt_min)) + math.log(dt_min)).clamp(min=1e-4)\n",
        "        self.dt_bias = nn.Parameter(dt + torch.log(-torch.expm1(-dt))) # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n",
        "        self.dt_bias._no_weight_decay = True\n",
        "        A = torch.empty(self.n_heads, dtype=torch.float32).uniform_(1,16)\n",
        "        A_log = torch.log(A)\n",
        "        self.A_log = nn.Parameter(A_log)\n",
        "        self.A_log._no_weight_decay = True\n",
        "        self.D = nn.Parameter(torch.ones(self.n_heads))\n",
        "        self.D._no_weight_decay = True\n",
        "        self.norm = nn.RMSNorm(self.d_inner)\n",
        "        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=False)\n",
        "\n",
        "    # def forward(self, u, h0=None): # [b,t,d]\n",
        "    def forward(self, u): # [b,t,d]\n",
        "        # if h0!=None: return self.step(u, h0)\n",
        "        # print('Mamba2Simple u', u.shape)\n",
        "        A = -torch.exp(self.A_log) # [n_heads]\n",
        "        z, xBC, dt = self.in_proj(u).split([self.d_inner, self.d_inner + 2* self.n_groups*self.d_state, self.n_heads], dim=-1)\n",
        "        dt = F.softplus(dt+self.dt_bias) # [b,t,h]\n",
        "\n",
        "        h_conv = F.pad(xBC, (0,0,self.d_conv-u.shape[1],0)) # [b,k,xbc]\n",
        "        xBC = self.act(self.conv1d(xBC.transpose(-2,-1)).transpose(-2,-1)[:,:u.size(1),:])  # [b,t, d_inner + 2* ngroups*d_state]\n",
        "        x, B, C = xBC.split([self.n_heads*self.d_head, self.n_groups*self.d_state, self.n_groups*self.d_state], dim=-1) # x:[b,t,h,d], B/C:[b,t,g,s] # X, B, C correspond to V, K, Q respectively in the SSM/attention duality\n",
        "        x, B, C = x.unflatten(-1, (self.n_heads, self.d_head)), B.unflatten(-1, (self.n_groups, self.d_state)), C.unflatten(-1, (self.n_groups, self.d_state))\n",
        "        # h0 = self.h0.expand(u.size(0),-1,-1,-1) # [b,n,d,s]\n",
        "        # print('x dt a b', x.shape, dt.shape, A.shape, B.shape)\n",
        "\n",
        "        # y, h_ssm = ssd(x*dt.unsqueeze(-1), A*dt, B, C, 64, h0) # 256\n",
        "        y, h_ssm = ssd(x*dt.unsqueeze(-1), A*dt, B, C, 64) # 256\n",
        "\n",
        "        y = y + x * self.D.unsqueeze(-1)\n",
        "        y = self.norm(y.flatten(2) * self.act(z)) # [b,t,d_inner] # norm(x)*silu(z) if norm_before_gate, else norm(x*silu(z)) # https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/triton/layernorm_gated.py#L18\n",
        "        out = self.out_proj(y)\n",
        "        return out, (h_conv, h_ssm) # [b,t,in], [b,h,d,s]\n",
        "\n",
        "    def step(self, u, h=None): # [b,t,in], (conv[b,k,xbc], ssm[b,h,d,s])\n",
        "        u = u[:,-self.d_conv:] # [b,min(k,t),in]\n",
        "        z, xBC, dt = self.in_proj(u).split([self.d_inner, self.d_inner + 2* self.n_groups*self.d_state, self.n_heads], dim=-1) # [b,min(k,t),_]\n",
        "\n",
        "        if h==None: h_conv = F.pad(xBC, (0,0,max(0,self.d_conv-u.shape[1]),0))\n",
        "        else: h_conv = torch.cat([h[0], xBC], dim=1)[:,-self.d_conv:] # [b,k,xbc]\n",
        "        # print(h_conv.shape, self.conv1d.weight.shape, self.conv1d.bias.shape)\n",
        "\n",
        "        xBC = F.silu(torch.einsum(\"bkc,cik->bc\", h_conv, self.conv1d.weight) + self.conv1d.bias) # [b,kernel,xbc], [xbc,1,kernel], +[xbc] -> [b,xbc]\n",
        "\n",
        "\n",
        "        x, B, C = xBC.split([self.n_heads*self.d_head, self.n_groups*self.d_state, self.n_groups*self.d_state], dim=-1) # x:[b,t,h,d], B/C:[b,t,g,s] # X, B, C correspond to V, K, Q respectively in the SSM/attention duality\n",
        "        x, B, C = x.unflatten(-1, (self.n_heads, self.d_head)), B.unflatten(-1, (self.n_groups, self.d_state)), C.unflatten(-1, (self.n_groups, self.d_state))\n",
        "\n",
        "        A = -torch.exp(self.A_log)  # [n_heads]\n",
        "\n",
        "        # SSM step\n",
        "        dt = F.softplus(dt[:,-1]+self.dt_bias) # [b,n_heads]\n",
        "        dA = torch.exp(dt*A) # [b,n_heads] # https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/triton/selective_state_update.py#L104\n",
        "        # print('dt, B, x', dt.shape, B.shape, x.shape)\n",
        "        dBx = torch.einsum(\"bh,bhs,bhd->bhds\", dt, B, x)\n",
        "        if h==None: h_ssm = dBx\n",
        "        else: h_ssm = h[1] * dA[...,None,None] + dBx # [b,h,d,s]*[b,h,1,1]+[b,h,d,s]\n",
        "        # print('h.ssm_state, C', h.ssm_state.shape, C.shape)\n",
        "        y = torch.einsum(\"bhds,bhs->bhd\", h_ssm, C) # [b,h,d,s]*[b,h,s]\n",
        "        y = y + self.D.unsqueeze(-1) * x\n",
        "        y = self.norm(y.flatten(1) * F.silu(z[:,-1]))\n",
        "        y = self.out_proj(y)\n",
        "        return y.unsqueeze(1), (h_conv, h_ssm)\n",
        "\n",
        "\n",
        "\n",
        "b,t,d_model=5,256,32\n",
        "# b,t,d_model=5,7,32\n",
        "d_model = 64\n",
        "u = torch.randn(b,t,d_model, device=device)\n",
        "model = Mamba2Simple(d_model).to(device)\n",
        "out, h = model(u)\n",
        "# h0 = torch.randn(b, model.n_heads, model.d_head, model.d_state)\n",
        "# print(out.shape)\n",
        "# print(out.shape, h.shape)\n",
        "# print(out[0,-3:,:5], h[0,:2,:5,:5])\n",
        "# out, h = model(u, h)\n",
        "\n",
        "# u = torch.randn(b,7,d_model, device=device)\n",
        "# # out, h = model(u[:,-1:], h)\n",
        "out, h = model.step(u, h)\n",
        "# out, h = model.step(u)\n",
        "print(out.shape)\n",
        "# print(out[0,-3:,:5], h[0,:2,:5,:5])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wtY1qNrHynsd"
      },
      "outputs": [],
      "source": [
        "# @title mamba_ssm/modules/mamba2_simple.py next\n",
        "# https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba2_simple.py\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class Mamba2Simple(nn.Module):\n",
        "    def __init__(self, in_dim, d_state=64, expand=1, n_heads=8, d_conv=4):\n",
        "        super().__init__()\n",
        "        self.in_dim, self.d_state, self.d_conv = in_dim, d_state, d_conv\n",
        "        self.d_inner = expand * self.in_dim\n",
        "        self.n_heads, self.d_head = n_heads, self.d_inner//n_heads\n",
        "\n",
        "        self.in_proj = nn.Linear(self.in_dim, 2* self.d_inner + 2* self.n_heads*self.d_state + self.n_heads, bias=False) # z,x,B,C,dt\n",
        "        conv_dim = self.d_inner + 2* self.n_heads*self.d_state # for x,B,C\n",
        "        self.conv1d = nn.Conv1d(in_channels=conv_dim, out_channels=conv_dim, bias=True, kernel_size=d_conv, groups=conv_dim, padding=d_conv-1)\n",
        "        # self.conv1d.weight._no_weight_decay = True\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "        # self.h0 = nn.Parameter(torch.zeros(self.n_heads, self.d_head, self.d_state))\n",
        "        # self.h0._no_weight_decay = True\n",
        "        dt_min, dt_max = .001, .1\n",
        "        dt = torch.exp(torch.rand(self.n_heads) * (math.log(dt_max)-math.log(dt_min)) + math.log(dt_min)).clamp(min=1e-4)\n",
        "        self.dt_bias = nn.Parameter(dt + torch.log(-torch.expm1(-dt))) # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n",
        "        self.dt_bias._no_weight_decay = True\n",
        "        A = torch.empty(self.n_heads, dtype=torch.float32).uniform_(1,16)\n",
        "        A_log = torch.log(A)\n",
        "        self.A_log = nn.Parameter(A_log)\n",
        "        self.A_log._no_weight_decay = True\n",
        "        self.D = nn.Parameter(torch.ones(self.n_heads))\n",
        "        self.D._no_weight_decay = True\n",
        "        self.norm = nn.RMSNorm(self.d_inner)\n",
        "        self.out_proj = nn.Linear(self.d_inner, self.in_dim, bias=False)\n",
        "\n",
        "    # def forward(self, u, h0=None): # [b,t,d]\n",
        "    def forward(self, u): # [b,t,d]\n",
        "        # if h0!=None: return self.step(u, h0)\n",
        "        # print('Mamba2Simple u', u.shape)\n",
        "        A = -torch.exp(self.A_log) # [n_heads]\n",
        "        z, xBC, dt = self.in_proj(u).split([self.d_inner, self.d_inner + 2* self.n_heads*self.d_state, self.n_heads], dim=-1)\n",
        "        dt = F.softplus(dt+self.dt_bias) # [b,t,h]\n",
        "\n",
        "        h_conv = F.pad(xBC, (0,0,self.d_conv-u.shape[1],0)) # [b,k,xbc]\n",
        "        xBC = self.act(self.conv1d(xBC.transpose(-2,-1)).transpose(-2,-1)[:,:u.size(1),:])  # [b,t, d_inner + 2* ngroups*d_state]\n",
        "        x, B, C = xBC.unflatten(-1, (self.n_heads,-1)).split([self.d_head, self.d_state, self.d_state], dim=-1) # x:[b,t,h,d], B/C:[b,t,h,s] # X, B, C correspond to V, K, Q respectively in the SSM/attention duality\n",
        "        # h0 = self.h0.expand(u.size(0),-1,-1,-1) # [b,n,d,s]\n",
        "        # print('x dt a b', x.shape, dt.shape, A.shape, B.shape)\n",
        "\n",
        "        # y, h_ssm = ssd(x*dt.unsqueeze(-1), A*dt, B, C, 64, h0) # 256\n",
        "        y, h_ssm = ssd(x*dt.unsqueeze(-1), A*dt, B, C, 64) # 256\n",
        "\n",
        "        y = y + x * self.D.unsqueeze(-1)\n",
        "        y = self.norm(y.flatten(2) * self.act(z)) # [b,t,d_inner] # norm(x)*silu(z) if norm_before_gate, else norm(x*silu(z)) # https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/triton/layernorm_gated.py#L18\n",
        "        out = self.out_proj(y)\n",
        "        return out, (h_conv, h_ssm) # [b,t,in], [b,h,d,s]\n",
        "\n",
        "    def step(self, u, h=None): # [b,t,in], (conv[b,k,xbc], ssm[b,h,d,s])\n",
        "        u = u[:,-self.d_conv:] # [b,min(k,t),in]\n",
        "        z, xBC, dt = self.in_proj(u).split([self.d_inner, self.d_inner + 2* self.n_heads*self.d_state, self.n_heads], dim=-1) # [b,min(k,t),_]\n",
        "\n",
        "        if h==None: h_conv = F.pad(xBC, (0,0,max(0,self.d_conv-u.shape[1]),0))\n",
        "        else: h_conv = torch.cat([h[0], xBC], dim=1)[:,-self.d_conv:] # [b,k,xbc]\n",
        "        # print(h_conv.shape, self.conv1d.weight.shape, self.conv1d.bias.shape)\n",
        "\n",
        "        xBC = F.silu(torch.einsum(\"bkc,cik->bc\", h_conv, self.conv1d.weight) + self.conv1d.bias) # [b,kernel,xbc], [xbc,1,kernel], +[xbc] -> [b,xbc]\n",
        "        x, B, C = xBC.unflatten(-1, (self.n_heads,-1)).split([self.d_head, self.d_state, self.d_state], dim=-1) # x:[b,t,h,d], B/C:[b,t,h,s] # X, B, C correspond to V, K, Q respectively in the SSM/attention duality\n",
        "        A = -torch.exp(self.A_log)  # [n_heads]\n",
        "\n",
        "        # SSM step\n",
        "        dt = F.softplus(dt[:,-1]+self.dt_bias) # [b,n_heads]\n",
        "        dA = torch.exp(dt*A) # [b,n_heads]\n",
        "        # print('dt, B, x', dt.shape, B.shape, x.shape)\n",
        "        dBx = torch.einsum(\"bh,bhs,bhd->bhds\", dt, B, x)\n",
        "        if h==None: h_ssm = dBx\n",
        "        else: h_ssm = h[1] * dA[...,None,None] + dBx # [b,h,d,s]*[b,h,1,1]+[b,h,d,s]\n",
        "        # print('h.ssm_state, C', h.ssm_state.shape, C.shape)\n",
        "        y = torch.einsum(\"bhds,bhs->bhd\", h_ssm, C) # [b,h,d,s]*[b,h,s]\n",
        "        y = y + self.D.unsqueeze(-1) * x\n",
        "        y = self.norm(y.flatten(1) * F.silu(z[:,-1]))\n",
        "        y = self.out_proj(y)\n",
        "        return y.unsqueeze(1), (h_conv, h_ssm)\n",
        "\n",
        "\n",
        "\n",
        "b,t,in_dim=5,256,32\n",
        "# b,t,in_dim=5,7,32\n",
        "d_model = 64\n",
        "u = torch.randn(b,t,in_dim, device=device)\n",
        "model = Mamba2Simple(in_dim, d_model).to(device)\n",
        "out, h = model(u)\n",
        "# h0 = torch.randn(b, model.n_heads, model.d_head, model.d_state)\n",
        "# print(out.shape)\n",
        "# print(out.shape, h.shape)\n",
        "# print(out[0,-3:,:5], h[0,:2,:5,:5])\n",
        "# out, h = model(u, h)\n",
        "\n",
        "# u = torch.randn(b,7,in_dim, device=device)\n",
        "# # out, h = model(u[:,-1:], h)\n",
        "out, h = model.step(u, h)\n",
        "# out, h = model.step(u)\n",
        "print(out.shape)\n",
        "# print(out[0,-3:,:5], h[0,:2,:5,:5])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-mrujusYX9Cc"
      },
      "outputs": [],
      "source": [
        "# @title from johnma2006/mamba-minimal\n",
        "# https://github.com/johnma2006/mamba-minimal/blob/master/model.py\n",
        "\"\"\"[1] Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Albert Gu and Tri Dao) https://arxiv.org/abs/2312.00752\n",
        "    [2] The Annotated S4 (Sasha Rush and Sidd Karamcheti) https://srush.github.io/annotated-s4\n",
        "    n or d_state: latent state dim      (`N` in [1] Algorithm 2)\n",
        "    d_in or d_inner: d * expansion factor         (`D` in [1] Algorithm 2) ; expansion factor (`E` in [1] Section 3.4)\n",
        "    A, B, C, D: state space parameters  (See any state space representation formula)\n",
        "                                        (B, C are input-dependent (aka selective, a key innovation in Mamba); A, D are not)\n",
        "    Î” or delta: input-dependent step size\n",
        "    dt_rank: rank of Î”                  (See [1] Section 3.6 \"Parameterization of âˆ†\")\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class Mamba(nn.Module):\n",
        "    def __init__(self, d_model=256, d_state=16, d_inner=512, n_layer=3):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([ResidualBlock(d_model, d_state, d_inner) for _ in range(n_layer)])\n",
        "        self.norm_f = nn.RMSNorm(d_model)\n",
        "\n",
        "    def forward(self, x): # (b, l, d)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        x = self.norm_f(x)\n",
        "        return x\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_state, d_inner):\n",
        "        super().__init__()\n",
        "        self.mixer = MambaBlock(d_model, d_state, d_inner)\n",
        "        self.norm = nn.RMSNorm(d_model)\n",
        "\n",
        "    def forward(self, x): # (b, l, d) # Block.forward(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py#L297\n",
        "        output = x + self.mixer(self.norm(x))\n",
        "        return output # (b, l, d)\n",
        "\n",
        "\n",
        "class MambaBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_state, d_inner, dt_rank=16):\n",
        "        super().__init__()\n",
        "        self.dt_rank = dt_rank\n",
        "        self.in_proj = nn.Linear(d_model, d_inner * 2, bias=False)\n",
        "        d_conv = 4\n",
        "        self.conv1d = nn.Conv1d(d_inner, d_inner, d_conv, padding=d_conv-1, groups=d_inner)\n",
        "        self.x_proj = nn.Linear(d_inner, dt_rank + 2*d_state, bias=False) # takes in `x` and outputs the input-specific Î”, B, C\n",
        "        self.dt_proj = nn.Linear(dt_rank, d_inner, bias=True) # project Î” from dt_rank to d_in\n",
        "        self.A_log = nn.Parameter(torch.log(torch.arange(1, d_state + 1).repeat(d_inner, 1)))\n",
        "        self.D = nn.Parameter(torch.ones(d_inner))\n",
        "        self.out_proj = nn.Linear(d_inner, d_model, bias=False)\n",
        "\n",
        "    def forward(self, x): # (b, l, d) # https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py#L119\n",
        "        b, l, d = x.shape\n",
        "        x_res = self.in_proj(x)  # (b, l, 2 * d_in)\n",
        "        x, res = torch.chunk(x_res, 2, dim=-1) # [b, l, d_in]\n",
        "        x = self.conv1d(x.permute(0, 2, 1))[:, :, :l].permute(0, 2, 1) # (b, l, d_in)\n",
        "        y = self.ssm(F.silu(x)) * F.silu(res)\n",
        "        output = self.out_proj(y)\n",
        "        return output # (b, l, d)\n",
        "\n",
        "    def ssm(self, x): # (b, l, d_in)\n",
        "        \"\"\"- Algorithm 2 in Section 3.2 in the Mamba paper [1] - run_SSM(A, B, C, u) in The Annotated S4 [2]\n",
        "        Official Implementation: mamba_inner_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L311\"\"\"\n",
        "        d_in, n = self.A_log.shape\n",
        "        A = -torch.exp(self.A_log.float())  # shape (d_in, n)\n",
        "        # D = self.D.float()\n",
        "\n",
        "        # 2: B : (B, L, N) â† sB (x)\n",
        "        # 3: C : (B, L, N) â† sC (x)\n",
        "        # 4: Î” : (B, L, D) â† ðœÎ”(Parameter+ð‘ Î”(x))\n",
        "        # 5: A, B : (B, L, D, N) â† discretize(Î”, A, B)\n",
        "        # 6: y â† SSM(A, B, C) (x)\n",
        "\n",
        "        delta, B, C = self.x_proj(x).split([self.dt_rank, n, n], dim=-1) # [b,l,dt_rank] [b,l,n], [b,l,n]\n",
        "        delta = F.softplus(self.dt_proj(delta)) # (b, l, d_in)\n",
        "        y = selective_scan(x, delta, A, B, C, self.D)  # This is similar to run_SSM(A, B, C, u) in The Annotated S4 [2]\n",
        "        return y # (b, l, d_in)\n",
        "\n",
        "def selective_scan(u, delta, A, B, C, D): # u: (b, l, d_in); delta: (b, l, d_in); A: (d_in, n); B: (b, l, n); C: (b, l, n); D: (d_in)\n",
        "    \"\"\"- Section 2 State Space Models in the Mamba paper [1] - Algorithm 2 in Section 3.2 in the Mamba paper [1] - run_SSM(A, B, C, u) in The Annotated S4 [2]\n",
        "    x(t+1) = Ax(t) + Bu(t)\n",
        "    y(t)   = Cx(t) + Du(t)\n",
        "    except B and C (and the step size delta, which is used for discretization) are dependent on the input x(t).\n",
        "\n",
        "    Official Implementation: selective_scan_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L86\n",
        "    Note: I refactored some parts out of `selective_scan_ref` out, so the functionality doesn't match exactly.\"\"\"\n",
        "    b, l, d_in = u.shape\n",
        "    n = A.shape[1]\n",
        "\n",
        "    # Discretize continuous parameters (A, B)\n",
        "    deltaA = torch.exp(torch.einsum('bld,dn->bldn', delta, A)) # [b l d_in n] # - A is discretized using zero-order hold (ZOH) discretization (see Section 2 Equation 4 in the Mamba paper [1])\n",
        "    deltaB_u = torch.einsum('bld,bln,bld->bldn', delta, B, u) # [b l d_in n] # - B is discretized using a simplified Euler discretization instead of ZOH. From a discussion with authors: \"A is the more important term and the performance doesn't change much with the simplification on B\"\n",
        "\n",
        "    # selective scan (scan_SSM() in The Annotated S4 [2])\n",
        "    # Note that the below is sequential, while the official implementation does a much faster parallel scan that is additionally hardware-aware (like FlashAttention).\n",
        "    x = torch.zeros((b, d_in, n), device=device)\n",
        "    ys = []\n",
        "    for i in range(l):\n",
        "        x = deltaA[:, i] * x + deltaB_u[:, i]\n",
        "        y = torch.einsum('bdn,bn->bd', x, C[:, i, :]) # b d_in\n",
        "        ys.append(y)\n",
        "    y = torch.stack(ys, dim=1)  # shape (b, l, d_in)\n",
        "    y = y + u * D\n",
        "    return y # (b, l, d_in)\n",
        "\n",
        "\n",
        "pred = Mamba()\n",
        "\n",
        "batch=4\n",
        "seq_len=500\n",
        "d_model=256\n",
        "x = torch.randn(batch, seq_len, d_model)\n",
        "y = pred(x)\n",
        "print(x.shape, y.shape)\n",
        "# print(x)\n",
        "# print(y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "drcz9JmNR9jx"
      },
      "outputs": [],
      "source": [
        "# @title tommyip/mamba2-minimal+\n",
        "# https://github.com/tommyip/mamba2-minimal/blob/main/mamba2.py\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "\n",
        "from torch import Tensor\n",
        "from typing import NamedTuple, cast\n",
        "class InferenceCache(NamedTuple):\n",
        "    conv_state: Tensor  # (batch, d_inner + 2* d_state, d_conv)\n",
        "    ssm_state: Tensor  # (batch, nheads, d_head, d_state)\n",
        "    @staticmethod\n",
        "    def alloc(batch_size, d_inner, n_heads, d_head, d_state, d_conv, device=None):\n",
        "        return InferenceCache(torch.zeros(batch_size, d_inner + 2* d_state, d_conv, device=device),\n",
        "            torch.zeros(batch_size, n_heads, d_head, d_state, device=device))\n",
        "\n",
        "\n",
        "class Mamba2(nn.Module):\n",
        "    # def __init__(self, args: Mamba2Config, device = None):\n",
        "    def __init__(self, d_model=256, d_state=128, n_heads=4, expand=2, d_conv=4, n_layer=2):\n",
        "        super().__init__()\n",
        "        self.d_model, self.d_state, self.d_conv = d_model, d_state, d_conv\n",
        "        self.d_inner = expand * d_model\n",
        "        self.n_heads, self.d_head = n_heads, self.d_inner//n_heads\n",
        "\n",
        "        self.in_proj = nn.Linear(self.d_model, 2* self.d_inner + 2* self.d_state + self.n_heads, bias=False) # z, x, B, C, dt\n",
        "        conv_dim = self.d_inner + 2* self.d_state\n",
        "        self.conv1d = nn.Conv1d(in_channels=conv_dim, out_channels=conv_dim, kernel_size=self.d_conv, groups=conv_dim, padding=self.d_conv-1)\n",
        "\n",
        "        self.dt_bias = nn.Parameter(torch.empty(self.n_heads))\n",
        "        self.A_log = nn.Parameter(torch.empty(self.n_heads))\n",
        "        self.D = nn.Parameter(torch.empty(self.n_heads))\n",
        "        self.norm = nn.RMSNorm(self.d_inner)\n",
        "        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=False)\n",
        "\n",
        "    def forward(self, u, h=None): # (batch, seqlen, d_model)\n",
        "        # print('fwd', u.shape, h.shape if h!=None else None)\n",
        "        # print('fwd', u.shape)\n",
        "        if h: return self.step(u, h)\n",
        "        A = -torch.exp(self.A_log)  # (n_heads,)\n",
        "        zxbcdt = self.in_proj(u)  # (batch, seqlen, d_in_proj)\n",
        "        z, xBC, dt = torch.split(zxbcdt, [self.d_inner, self.d_inner + 2* self.d_state, self.n_heads], dim=-1,)\n",
        "        dt = F.softplus(dt + self.dt_bias)  # (batch, seqlen, n_heads)\n",
        "\n",
        "        # Pad or truncate xBC seqlen to d_conv\n",
        "        conv_state = F.pad(rearrange(xBC, \"b l d -> b d l\"), (self.d_conv - u.shape[1], 0))\n",
        "        xBC = F.silu(self.conv1d(xBC.transpose(1, 2)).transpose(1, 2)[:, : u.shape[1], :])  # (batch, seqlen, d_inner + 2 * d_state))\n",
        "        x, B, C = torch.split(xBC, [self.d_inner, self.d_state, self.d_state], dim=-1)\n",
        "\n",
        "        x = rearrange(x, \"b l (h p) -> b l h p\", p=self.d_head)\n",
        "        y, ssm_state = ssd(x * dt.unsqueeze(-1), A * dt,\n",
        "            rearrange(B, \"b l n -> b l 1 n\"), rearrange(C, \"b l n -> b l 1 n\"), 8)\n",
        "\n",
        "        y = y + x * self.D.unsqueeze(-1)\n",
        "        y = rearrange(y, \"b l h p -> b l (h p)\")\n",
        "        y = self.norm(y * F.silu(z))\n",
        "        y = self.out_proj(y)\n",
        "        h = InferenceCache(conv_state, ssm_state)\n",
        "        return y, h # (batch, seqlen, d_model)\n",
        "\n",
        "    def step(self, u, h): # (batch, 1, d_model)\n",
        "        assert u.shape[1] == 1, \"Only one token can be decoded per inference step\"\n",
        "        A = -torch.exp(self.A_log)  # [n_heads]\n",
        "        zxbcdt = self.in_proj(u.squeeze(1))  # (batch, d_in_proj)\n",
        "        z, xBC, dt = torch.split(zxbcdt, [self.d_inner, self.d_inner + 2* self.d_state, self.n_heads], dim=-1,)\n",
        "        dt = F.softplus(dt+self.dt_bias)  # (batch, n_heads)\n",
        "\n",
        "        # Advance convolution input\n",
        "        h.conv_state.copy_(torch.roll(h.conv_state, shifts=-1, dims=-1))\n",
        "        h.conv_state[:,:,-1] = xBC\n",
        "        xBC = F.silu(torch.sum(h.conv_state * self.conv1d.weight.squeeze(1), dim=-1) + self.conv1d.bias)\n",
        "        x, B, C = xBC.split([self.d_inner, self.d_state, self.d_state], dim=-1)\n",
        "\n",
        "        # SSM step\n",
        "        dA = torch.exp(dt*A)  # (batch, n_heads)\n",
        "        x = x.unflatten(-1, (-1,self.d_head))\n",
        "        dBx = torch.einsum(\"bh,bn,bhp->bhpn\", dt, B, x)\n",
        "        h.ssm_state.copy_(h.ssm_state * dA[...,None,None] + dBx)\n",
        "        y = torch.einsum(\"bhpn,bn->bhp\", h.ssm_state, C)\n",
        "\n",
        "        y = y + self.D.unsqueeze(-1) * x\n",
        "        y = self.norm(y.flatten(1) * F.silu(z))\n",
        "        y = self.out_proj(y)\n",
        "        return y.unsqueeze(1), h # (batch, 1, d_model)\n",
        "\n",
        "\n",
        "# def segsum(x, device = None):\n",
        "#     \"\"\"Stable segment sum calculation.\n",
        "#     `exp(segsum(A))` produces a 1-semiseparable matrix, which is equivalent to a scalar SSM.\n",
        "#     Source: https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/ssd_minimal.py#L23-L32\"\"\"\n",
        "#     T = x.size(-1)\n",
        "#     x = repeat(x, \"... d -> ... d e\", e=T)\n",
        "#     mask = torch.tril(torch.ones(T, T, dtype=torch.bool, device=device), diagonal=-1)\n",
        "#     x = x.masked_fill(~mask, 0)\n",
        "#     x_segsum = torch.cumsum(x, dim=-2)\n",
        "#     mask = torch.tril(torch.ones(T, T, dtype=torch.bool, device=device), diagonal=0)\n",
        "#     x_segsum = x_segsum.masked_fill(~mask, -torch.inf)\n",
        "#     return x_segsum\n",
        "\n",
        "\n",
        "# def ssd(x, A, B, C, chunk_size, initial_states=None, device = None):\n",
        "#     \"\"\"Structed State Space Duality (SSD) - the core of Mamba-2\n",
        "#     This is almost the exact same minimal SSD code from the blog post.\n",
        "#     Arguments\n",
        "#         x: (batch, seqlen, n_heads, d_head)\n",
        "#         A: (batch, seqlen, n_heads)\n",
        "#         B: (batch, seqlen, n_heads, d_state)\n",
        "#         C: (batch, seqlen, n_heads, d_state)\n",
        "#     Return y: (batch, seqlen, n_heads, d_head)\n",
        "\n",
        "#     Source\n",
        "#      1. https://tridao.me/blog/2024/mamba2-part3-algorithm/\n",
        "#      2. https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/ssd_minimal.py#L34-L78\n",
        "#     \"\"\"\n",
        "#     assert x.shape[1] % chunk_size == 0\n",
        "\n",
        "#     # Rearrange into chunks\n",
        "#     # Step 1, 2 and 4 of SSD can be computed in parallel for each chunk across devices (sequence parallel)\n",
        "#     # This is not implemented and left as an exercise for the reader ðŸ˜œ\n",
        "#     x, A, B, C = [rearrange(m, \"b (c l) ... -> b c l ...\", l=chunk_size) for m in (x, A, B, C)]\n",
        "\n",
        "#     A = rearrange(A, \"b c l h -> b h c l\")\n",
        "#     A_cumsum = torch.cumsum(A, dim=-1)\n",
        "\n",
        "#     # 1. Compute the output for each intra-chunk (diagonal blocks)\n",
        "#     L = torch.exp(segsum(A, device=device))\n",
        "#     Y_diag = torch.einsum(\"bclhn, bcshn, bhcls, bcshp -> bclhp\", C, B, L, x)\n",
        "\n",
        "#     # 2. Compute the state for each intra-chunk\n",
        "#     # (right term of low-rank factorization of off-diagonal blocks; B terms)\n",
        "#     decay_states = torch.exp(A_cumsum[:, :, :, -1:] - A_cumsum)\n",
        "#     states = torch.einsum(\"bclhn, bhcl, bclhp -> bchpn\", B, decay_states, x)\n",
        "\n",
        "#     # 3. Compute the inter-chunk SSM recurrence; produces correct SSM states at chunk boundaries\n",
        "#     # (middle term of factorization of off-diag blocks; A terms)\n",
        "#     if initial_states is None:\n",
        "#         initial_states = torch.zeros_like(states[:, :1])\n",
        "#     states = torch.cat([initial_states, states], dim=1)\n",
        "#     decay_chunk = torch.exp(segsum(F.pad(A_cumsum[:, :, :, -1], (1, 0)), device=device))\n",
        "#     new_states = torch.einsum(\"bhzc, bchpn -> bzhpn\", decay_chunk, states)\n",
        "#     states, final_state = new_states[:, :-1], new_states[:, -1]\n",
        "\n",
        "#     # 4. Compute state -> output conversion per chunk\n",
        "#     # (left term of low-rank factorization of off-diagonal blocks; C terms)\n",
        "#     state_decay_out = torch.exp(A_cumsum)\n",
        "#     Y_off = torch.einsum(\"bclhn, bchpn, bhcl -> bclhp\", C, states, state_decay_out)\n",
        "\n",
        "#     # Add output of intra-chunk and inter-chunk terms (diagonal and off-diagonal blocks)\n",
        "#     Y = rearrange(Y_diag + Y_off, \"b c l h p -> b (c l) h p\")\n",
        "#     return Y, final_state\n",
        "\n",
        "\n",
        "batch=4\n",
        "seq_len=16\n",
        "d_model=256\n",
        "\n",
        "pred = Mamba2(d_model)\n",
        "\n",
        "x = torch.randn(batch, seq_len, d_model)\n",
        "y, h = pred(x)\n",
        "# print(x)\n",
        "# print(y)\n",
        "print(x.shape)\n",
        "print(y.shape)\n",
        "\n",
        "x = torch.randn(batch, 1, d_model)\n",
        "y, h = pred(x,h)\n",
        "\n",
        "print(y.shape)\n",
        "# print(y)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DYrzogiIXLa_"
      },
      "outputs": [],
      "source": [
        "# @title tommyip/mamba2-minimal\n",
        "# https://github.com/tommyip/mamba2-minimal/blob/main/mamba2.py\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "from dataclasses import dataclass\n",
        "@dataclass\n",
        "class Mamba2Config:\n",
        "    d_model =256 # model dimension (D)\n",
        "    n_layer = 2  # number of Mamba-2 layers in the language model\n",
        "    d_state = 256  # state dimension (N)\n",
        "    d_conv = 4  # convolution kernel size\n",
        "    headdim = 64  # head dimension (P)\n",
        "    chunk_size = 8  # matrix partition size (Q)\n",
        "    d_inner = 2 * d_model # expansion factor (E) * d_model\n",
        "    nheads = d_inner // headdim\n",
        "\n",
        "from torch import Tensor\n",
        "from typing import NamedTuple, cast\n",
        "class InferenceCache(NamedTuple):\n",
        "    conv_state: Tensor  # (batch, d_inner + 2 * d_state, d_conv)\n",
        "    ssm_state: Tensor  # (batch, nheads, headdim, d_state)\n",
        "\n",
        "    @staticmethod\n",
        "    def alloc(batch_size, args: Mamba2Config, device = None):\n",
        "        return InferenceCache(torch.zeros(batch_size, args.d_inner + 2 * args.d_state, args.d_conv, device=device),\n",
        "            torch.zeros(batch_size, args.nheads, args.headdim, args.d_state, device=device),)\n",
        "\n",
        "\n",
        "class Mamba2LMHeadModel(nn.Module):\n",
        "    def __init__(self, args: Mamba2Config, device = None):\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "        self.device = device\n",
        "        self.backbone = nn.ModuleDict(dict(\n",
        "                layers=nn.ModuleList(\n",
        "                    [nn.ModuleDict(dict(mixer=Mamba2(args, device=device), norm=nn.RMSNorm(args.d_model, device=device),))\n",
        "                        for _ in range(args.n_layer)]),\n",
        "                norm_f=nn.RMSNorm(args.d_model, device=device),\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def forward(self, x, h: list[InferenceCache] | list[None] | None = None): # (batch, seqlen, d_model)\n",
        "        \"\"\"h: hidden states for inference step. If present the constant-time\n",
        "               (wrt sequence length) inference path will be taken, input_ids\n",
        "               should have shape (batch, 1) containing the next batch of prompt token.\"\"\"\n",
        "        seqlen = x.shape[1]\n",
        "        if h is None: h = [None for _ in range(self.args.n_layer)]\n",
        "        for i, layer in enumerate(self.backbone.layers):\n",
        "            y, h[i] = layer.mixer(layer.norm(x), h[i])\n",
        "            x = y + x\n",
        "        x = self.backbone.norm_f(x)\n",
        "        return x[:, :seqlen], cast(list[InferenceCache], h) # (batch, seqlen, d_model)\n",
        "\n",
        "\n",
        "class Mamba2(nn.Module):\n",
        "    def __init__(self, args: Mamba2Config, device = None):\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "        self.device = device\n",
        "        # Order: (z, x, B, C, dt)\n",
        "        d_in_proj = 2 * args.d_inner + 2 * args.d_state + args.nheads\n",
        "        self.in_proj = nn.Linear(args.d_model, d_in_proj, bias=False, device=device)\n",
        "\n",
        "        conv_dim = args.d_inner + 2 * args.d_state\n",
        "        self.conv1d = nn.Conv1d(in_channels=conv_dim, out_channels=conv_dim, kernel_size=args.d_conv,\n",
        "            groups=conv_dim, padding=args.d_conv-1, device=device,)\n",
        "\n",
        "        self.dt_bias = nn.Parameter(torch.empty(args.nheads, device=device))\n",
        "        self.A_log = nn.Parameter(torch.empty(args.nheads, device=device))\n",
        "        self.D = nn.Parameter(torch.empty(args.nheads, device=device))\n",
        "        self.norm = nn.RMSNorm(args.d_inner, device=device)\n",
        "        self.out_proj = nn.Linear(args.d_inner, args.d_model, bias=False, device=device)\n",
        "\n",
        "    def forward(self, u, h=None): # (batch, seqlen, d_model)\n",
        "        if h: return self.step(u, h)\n",
        "\n",
        "        A = -torch.exp(self.A_log)  # (nheads,)\n",
        "        zxbcdt = self.in_proj(u)  # (batch, seqlen, d_in_proj)\n",
        "        z, xBC, dt = torch.split(zxbcdt, [self.args.d_inner, self.args.d_inner + 2 * self.args.d_state, self.args.nheads,], dim=-1,)\n",
        "        dt = F.softplus(dt + self.dt_bias)  # (batch, seqlen, nheads)\n",
        "\n",
        "        # Pad or truncate xBC seqlen to d_conv\n",
        "        conv_state = F.pad(rearrange(xBC, \"b l d -> b d l\"), (self.args.d_conv - u.shape[1], 0))\n",
        "\n",
        "        xBC = F.silu(self.conv1d(xBC.transpose(1, 2)).transpose(1, 2)[:, : u.shape[1], :])  # (batch, seqlen, d_inner + 2 * d_state))\n",
        "        x, B, C = torch.split(xBC, [self.args.d_inner, self.args.d_state, self.args.d_state], dim=-1)\n",
        "        x = rearrange(x, \"b l (h p) -> b l h p\", p=self.args.headdim)\n",
        "        y, ssm_state = ssd(x * dt.unsqueeze(-1), A * dt,\n",
        "            rearrange(B, \"b l n -> b l 1 n\"), rearrange(C, \"b l n -> b l 1 n\"),\n",
        "            self.args.chunk_size, device=self.device,)\n",
        "        y = y + x * self.D.unsqueeze(-1)\n",
        "        y = rearrange(y, \"b l h p -> b l (h p)\")\n",
        "        # y = self.norm(y, z)\n",
        "        y = self.norm(y * F.silu(z))\n",
        "        y = self.out_proj(y)\n",
        "        h = InferenceCache(conv_state, ssm_state)\n",
        "        return y, h # (batch, seqlen, d_model)\n",
        "\n",
        "    def step(self, u, h): # (batch, 1, d_model)\n",
        "        \"\"\"Take a single inference step for the current input and hidden state\n",
        "\n",
        "        Unlike attention-based models, RNN-based models (eg Mamba) does not need\n",
        "        to look back at all the past tokens to generate a new token. Instead a\n",
        "        hidden state (initialized to 0s initially) is updated for each input and\n",
        "        passed to the next inference step. This means that the total inference\n",
        "        time is linear with respect to the sequence length instead of quadratic\n",
        "        in attention's case.\"\"\"\n",
        "        assert u.shape[1] == 1, \"Only one token can be decoded per inference step\"\n",
        "\n",
        "        zxbcdt = self.in_proj(u.squeeze(1))  # (batch, d_in_proj)\n",
        "        z, xBC, dt = torch.split(zxbcdt, [self.args.d_inner, self.args.d_inner + 2 * self.args.d_state, self.args.nheads,], dim=-1,)\n",
        "\n",
        "        # Advance convolution input\n",
        "        h.conv_state.copy_(torch.roll(h.conv_state, shifts=-1, dims=-1))\n",
        "        h.conv_state[:, :, -1] = xBC\n",
        "        # Convolution step\n",
        "        xBC = torch.sum(h.conv_state * rearrange(self.conv1d.weight, \"d 1 w -> d w\"), dim=-1)\n",
        "        xBC += self.conv1d.bias\n",
        "        xBC = F.silu(xBC)\n",
        "\n",
        "        x, B, C = torch.split(xBC, [self.args.d_inner, self.args.d_state, self.args.d_state], dim=-1)\n",
        "        A = -torch.exp(self.A_log)  # (nheads,)\n",
        "\n",
        "        # SSM step\n",
        "        dt = F.softplus(dt + self.dt_bias)  # (batch, nheads)\n",
        "        dA = torch.exp(dt * A)  # (batch, nheads)\n",
        "        x = rearrange(x, \"b (h p) -> b h p\", p=self.args.headdim)\n",
        "        dBx = torch.einsum(\"bh, bn, bhp -> bhpn\", dt, B, x)\n",
        "        h.ssm_state.copy_(h.ssm_state * rearrange(dA, \"b h -> b h 1 1\") + dBx)\n",
        "        y = torch.einsum(\"bhpn, bn -> bhp\", h.ssm_state, C)\n",
        "        y = y + rearrange(self.D, \"h -> h 1\") * x\n",
        "        y = rearrange(y, \"b h p -> b (h p)\")\n",
        "        # y = self.norm(y, z)\n",
        "        y = self.norm(y * F.silu(z))\n",
        "        y = self.out_proj(y)\n",
        "        return y.unsqueeze(1), h # (batch, 1, d_model)\n",
        "\n",
        "\n",
        "def segsum(x, device = None):\n",
        "    \"\"\"Stable segment sum calculation.\n",
        "    `exp(segsum(A))` produces a 1-semiseparable matrix, which is equivalent to a scalar SSM.\n",
        "    Source: https://github.com/state-spaces/mamba/blob/219f03c840d5a44e7d42e4e728134834fddccf45/mamba_ssm/modules/ssd_minimal.py#L23-L32\"\"\"\n",
        "    T = x.size(-1)\n",
        "    x = repeat(x, \"... d -> ... d e\", e=T)\n",
        "    mask = torch.tril(torch.ones(T, T, dtype=torch.bool, device=device), diagonal=-1)\n",
        "    x = x.masked_fill(~mask, 0)\n",
        "    x_segsum = torch.cumsum(x, dim=-2)\n",
        "    mask = torch.tril(torch.ones(T, T, dtype=torch.bool, device=device), diagonal=0)\n",
        "    x_segsum = x_segsum.masked_fill(~mask, -torch.inf)\n",
        "    return x_segsum\n",
        "\n",
        "\n",
        "def ssd(x, A, B, C, chunk_size, initial_states=None, device = None):\n",
        "    \"\"\"Structed State Space Duality (SSD) - the core of Mamba-2\n",
        "    This is almost the exact same minimal SSD code from the blog post.\n",
        "    Arguments\n",
        "        x: (batch, seqlen, n_heads, d_head)\n",
        "        A: (batch, seqlen, n_heads)\n",
        "        B: (batch, seqlen, n_heads, d_state)\n",
        "        C: (batch, seqlen, n_heads, d_state)\n",
        "    Return y: (batch, seqlen, n_heads, d_head)\n",
        "\n",
        "    Source\n",
        "     1. https://tridao.me/blog/2024/mamba2-part3-algorithm/\n",
        "     2. https://github.com/state-spaces/mamba/blob/219f03c840d5a44e7d42e4e728134834fddccf45/mamba_ssm/modules/ssd_minimal.py#L34-L78\n",
        "    \"\"\"\n",
        "    assert x.shape[1] % chunk_size == 0\n",
        "\n",
        "    # Rearrange into chunks\n",
        "    # Step 1, 2 and 4 of SSD can be computed in parallel for each chunk across devices (sequence parallel)\n",
        "    # This is not implemented and left as an exercise for the reader ðŸ˜œ\n",
        "    x, A, B, C = [rearrange(m, \"b (c l) ... -> b c l ...\", l=chunk_size) for m in (x, A, B, C)]\n",
        "\n",
        "    A = rearrange(A, \"b c l h -> b h c l\")\n",
        "    A_cumsum = torch.cumsum(A, dim=-1)\n",
        "\n",
        "    # 1. Compute the output for each intra-chunk (diagonal blocks)\n",
        "    L = torch.exp(segsum(A, device=device))\n",
        "    Y_diag = torch.einsum(\"bclhn, bcshn, bhcls, bcshp -> bclhp\", C, B, L, x)\n",
        "\n",
        "    # 2. Compute the state for each intra-chunk\n",
        "    # (right term of low-rank factorization of off-diagonal blocks; B terms)\n",
        "    decay_states = torch.exp(A_cumsum[:, :, :, -1:] - A_cumsum)\n",
        "    states = torch.einsum(\"bclhn, bhcl, bclhp -> bchpn\", B, decay_states, x)\n",
        "\n",
        "    # 3. Compute the inter-chunk SSM recurrence; produces correct SSM states at chunk boundaries\n",
        "    # (middle term of factorization of off-diag blocks; A terms)\n",
        "    if initial_states is None:\n",
        "        initial_states = torch.zeros_like(states[:, :1])\n",
        "    states = torch.cat([initial_states, states], dim=1)\n",
        "    decay_chunk = torch.exp(segsum(F.pad(A_cumsum[:, :, :, -1], (1, 0)), device=device))\n",
        "    new_states = torch.einsum(\"bhzc, bchpn -> bzhpn\", decay_chunk, states)\n",
        "    states, final_state = new_states[:, :-1], new_states[:, -1]\n",
        "\n",
        "    # 4. Compute state -> output conversion per chunk\n",
        "    # (left term of low-rank factorization of off-diagonal blocks; C terms)\n",
        "    state_decay_out = torch.exp(A_cumsum)\n",
        "    Y_off = torch.einsum(\"bclhn, bchpn, bhcl -> bclhp\", C, states, state_decay_out)\n",
        "\n",
        "    # Add output of intra-chunk and inter-chunk terms (diagonal and off-diagonal blocks)\n",
        "    Y = rearrange(Y_diag + Y_off, \"b c l h p -> b (c l) h p\")\n",
        "    return Y, final_state\n",
        "\n",
        "\n",
        "batch=4\n",
        "seq_len=16\n",
        "d_model=256\n",
        "\n",
        "pred = Mamba2(Mamba2Config)\n",
        "# pred = Mamba2LMHeadModel(Mamba2Config)\n",
        "\n",
        "x = torch.randn(batch, seq_len, d_model)\n",
        "y, h = pred(x)\n",
        "print(x)\n",
        "print(y)\n",
        "print(x.shape)\n",
        "print(y.shape)\n",
        "\n",
        "x = torch.randn(batch, 1, d_model)\n",
        "y, h = pred(x,h)\n",
        "\n",
        "print(y.shape)\n",
        "print(y)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8YM8ybjmVEYP"
      },
      "outputs": [],
      "source": [
        "# @title alxndrTL mamba.py\n",
        "# https://github.com/alxndrTL/mamba.py/blob/main/mambapy/mamba.py\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from typing import Union\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from mambapy.pscan import pscan\n",
        "\n",
        "\"\"\" This file closely follows the mamba_simple.py from the official Mamba implementation, and the mamba-minimal by @johnma2006.\n",
        "The major differences are :\n",
        "-the convolution is done with torch.nn.Conv1d\n",
        "-the selective scan is done in PyTorch\n",
        "\n",
        "A sequential version of the selective scan is also available for comparison. Also, it is possible to use the official Mamba implementation.\n",
        "\n",
        "This is the structure of the torch modules :\n",
        "- A Mamba model is composed of several layers, which are ResidualBlock.\n",
        "- A ResidualBlock is composed of a MambaBlock, a normalization, and a residual connection : ResidualBlock(x) = mamba(norm(x)) + x\n",
        "- This leaves us with the MambaBlock : its input x is (B, L, D) and its outputs y is also (B, L, D) (B=batch size, L=seq len, D=model dim).\n",
        "First, we expand x into (B, L, 2*ED) (where E is usually 2) and split it into x and z, each (B, L, ED).\n",
        "Then, we apply the short 1d conv to x, followed by an activation function (silu), then the SSM.\n",
        "We then multiply it by silu(z).\n",
        "See Figure 3 of the paper (page 8) for a visual representation of a MambaBlock.\"\"\"\n",
        "\n",
        "@dataclass\n",
        "class MambaConfig:\n",
        "    d_model: int #Â D\n",
        "    n_layers: int\n",
        "    dt_rank: Union[int, str] = 'auto'\n",
        "    d_state: int = 16 #Â N in paper/comments\n",
        "    expand_factor: int = 2 #Â E in paper/comments\n",
        "    d_conv: int = 4\n",
        "\n",
        "    dt_min: float = 0.001\n",
        "    dt_max: float = 0.1\n",
        "    dt_init: str = \"random\" #Â \"random\" or \"constant\"\n",
        "    dt_scale: float = 1.0\n",
        "    dt_init_floor = 1e-4\n",
        "\n",
        "    rms_norm_eps: float = 1e-5\n",
        "    base_std: float = 0.02\n",
        "\n",
        "    bias: bool = False\n",
        "    conv_bias: bool = True\n",
        "    inner_layernorms: bool = False # apply layernorms to internal activations\n",
        "\n",
        "    mup: bool = False\n",
        "    mup_base_width: float = 128 # width=d_model\n",
        "\n",
        "    pscan: bool = True #Â use parallel scan mode or sequential mode when training\n",
        "    use_cuda: bool = False # use official CUDA implementation when training (not compatible with (b)float16)\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.d_inner = self.expand_factor * self.d_model # E*D = ED in comments\n",
        "\n",
        "        if self.dt_rank == 'auto':\n",
        "            self.dt_rank = math.ceil(self.d_model / 16)\n",
        "\n",
        "        # muP\n",
        "        if self.mup:\n",
        "            self.mup_width_mult = self.d_model / self.mup_base_width\n",
        "\n",
        "class Mamba(nn.Module):\n",
        "    def __init__(self, config: MambaConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layers = nn.ModuleList([ResidualBlock(config) for _ in range(config.n_layers)])\n",
        "\n",
        "    def forward(self, x): #Â x : (B, L, D)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "    def step(self, x, caches):\n",
        "        #Â x : (B, L, D)\n",
        "        #Â caches : [cache(layer) for all layers], cache : (h, inputs)\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x, caches[i] = layer.step(x, caches[i])\n",
        "        return x, caches\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, config: MambaConfig):\n",
        "        super().__init__()\n",
        "        self.mixer = MambaBlock(config)\n",
        "        self.norm = RMSNorm(config.d_model, config.rms_norm_eps, config.mup)\n",
        "\n",
        "    def forward(self, x): #Â x : (B, L, D)\n",
        "        output = self.mixer(self.norm(x)) + x\n",
        "        return output\n",
        "\n",
        "    def step(self, x, cache):\n",
        "        #Â x : (B, D)\n",
        "        #Â cache : (h, inputs)\n",
        "                # h : (B, ED, N)\n",
        "                #Â inputs: (B, ED, d_conv-1)\n",
        "\n",
        "        #Â output : (B, D)\n",
        "        #Â cache : (h, inputs)\n",
        "        output, cache = self.mixer.step(self.norm(x), cache)\n",
        "        output = output + x\n",
        "        return output, cache\n",
        "\n",
        "class MambaBlock(nn.Module):\n",
        "    def __init__(self, config: MambaConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        #Â projects block input from D to 2*ED (two branches)\n",
        "        self.in_proj = nn.Linear(config.d_model, 2 * config.d_inner, bias=config.bias)\n",
        "\n",
        "        self.conv1d = nn.Conv1d(in_channels=config.d_inner, out_channels=config.d_inner,\n",
        "                              kernel_size=config.d_conv, bias=config.conv_bias,\n",
        "                              groups=config.d_inner,\n",
        "                              padding=config.d_conv - 1)\n",
        "\n",
        "        #Â projects x to input-dependent delta, B, C\n",
        "        self.x_proj = nn.Linear(config.d_inner, config.dt_rank + 2 * config.d_state, bias=False)\n",
        "\n",
        "        #Â projects delta from dt_rank to d_inner\n",
        "        self.dt_proj = nn.Linear(config.dt_rank, config.d_inner, bias=True)\n",
        "\n",
        "        #Â dt initialization\n",
        "        #Â dt weights\n",
        "        dt_init_std = config.dt_rank**-0.5 * config.dt_scale\n",
        "        if config.dt_init == \"constant\":\n",
        "            nn.init.constant_(self.dt_proj.weight, dt_init_std)\n",
        "        elif config.dt_init == \"random\":\n",
        "            nn.init.uniform_(self.dt_proj.weight, -dt_init_std, dt_init_std)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        # delta bias\n",
        "        dt = torch.exp(\n",
        "            torch.rand(config.d_inner) * (math.log(config.dt_max) - math.log(config.dt_min)) + math.log(config.dt_min)\n",
        "        ).clamp(min=config.dt_init_floor)\n",
        "        inv_dt = dt + torch.log(-torch.expm1(-dt)) #Â inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n",
        "        with torch.no_grad():\n",
        "            self.dt_proj.bias.copy_(inv_dt)\n",
        "        #self.dt_proj.bias._no_reinit = True # initialization would set all Linear.bias to zero, need to mark this one as _no_reinit\n",
        "        #Â todo : explain why removed\n",
        "\n",
        "        # S4D real initialization\n",
        "        A = torch.arange(1, config.d_state + 1, dtype=torch.float32).repeat(config.d_inner, 1)\n",
        "        self.A_log = nn.Parameter(torch.log(A)) # why store A in log ? to keep A < 0 (cf -torch.exp(...)) ? for gradient stability ?\n",
        "        self.A_log._no_weight_decay = True\n",
        "\n",
        "        self.D = nn.Parameter(torch.ones(config.d_inner))\n",
        "        self.D._no_weight_decay = True\n",
        "\n",
        "        #Â projects block output from ED back to D\n",
        "        self.out_proj = nn.Linear(config.d_inner, config.d_model, bias=config.bias)\n",
        "\n",
        "        # used in jamba\n",
        "        if self.config.inner_layernorms:\n",
        "            self.dt_layernorm = RMSNorm(self.config.dt_rank, config.rms_norm_eps, config.mup)\n",
        "            self.B_layernorm = RMSNorm(self.config.d_state, config.rms_norm_eps, config.mup)\n",
        "            self.C_layernorm = RMSNorm(self.config.d_state, config.rms_norm_eps, config.mup)\n",
        "        else:\n",
        "            self.dt_layernorm = None\n",
        "            self.B_layernorm = None\n",
        "            self.C_layernorm = None\n",
        "\n",
        "        if self.config.use_cuda:\n",
        "            try:\n",
        "                from mamba_ssm.ops.selective_scan_interface import selective_scan_fn\n",
        "                self.selective_scan_cuda = selective_scan_fn\n",
        "            except ImportError:\n",
        "                print(\"Failed to import mamba_ssm. Falling back to mamba.py.\")\n",
        "                self.config.use_cuda = False\n",
        "\n",
        "    def _apply_layernorms(self, dt, B, C):\n",
        "        if self.dt_layernorm is not None:\n",
        "            dt = self.dt_layernorm(dt)\n",
        "        if self.B_layernorm is not None:\n",
        "            B = self.B_layernorm(B)\n",
        "        if self.C_layernorm is not None:\n",
        "            C = self.C_layernorm(C)\n",
        "        return dt, B, C\n",
        "\n",
        "    def forward(self, x): #Â x : (B, L, D)\n",
        "        _, L, _ = x.shape\n",
        "        xz = self.in_proj(x) # (B, L, 2*ED)\n",
        "        x, z = xz.chunk(2, dim=-1) #Â (B, L, ED), (B, L, ED)\n",
        "\n",
        "        #Â x branch\n",
        "        x = x.transpose(1, 2) #Â (B, ED, L)\n",
        "        x = self.conv1d(x)[:, :, :L] #Â depthwise convolution over time, with a short filter\n",
        "        x = x.transpose(1, 2) #Â (B, L, ED)\n",
        "\n",
        "        x = F.silu(x)\n",
        "        y = self.ssm(x, z)\n",
        "\n",
        "        if self.config.use_cuda:\n",
        "            output = self.out_proj(y) # (B, L, D)\n",
        "            return output # the rest of the operations are done in the ssm function (fused with the CUDA pscan)\n",
        "\n",
        "        #Â z branch\n",
        "        z = F.silu(z)\n",
        "\n",
        "        output = y * z\n",
        "        output = self.out_proj(output) #Â (B, L, D)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def ssm(self, x, z): #Â x : (B, L, ED)\n",
        "        A = -torch.exp(self.A_log.float()) # (ED, N)\n",
        "        D = self.D.float()\n",
        "\n",
        "        deltaBC = self.x_proj(x) #Â (B, L, dt_rank+2*N)\n",
        "        delta, B, C = torch.split(deltaBC, [self.config.dt_rank, self.config.d_state, self.config.d_state], dim=-1) #Â (B, L, dt_rank), (B, L, N), (B, L, N)\n",
        "        delta, B, C = self._apply_layernorms(delta, B, C)\n",
        "        delta = self.dt_proj.weight @ delta.transpose(1, 2) #Â (ED, dt_rank) @ (B, L, dt_rank) -> (B, ED, L)\n",
        "        # here we just apply the matrix mul operation of delta = softplus(dt_proj(delta))\n",
        "        # the rest will be applied later (fused if using cuda)\n",
        "\n",
        "        # choose which selective_scan function to use, according to config\n",
        "        if self.config.use_cuda:\n",
        "            # these are unfortunately needed for the selective_scan_cuda function\n",
        "            x = x.transpose(1, 2)\n",
        "            B = B.transpose(1, 2)\n",
        "            C = C.transpose(1, 2)\n",
        "            z = z.transpose(1, 2)\n",
        "\n",
        "            # \"softplus\" + \"bias\" + \"y * silu(z)\" operations are fused\n",
        "            y = self.selective_scan_cuda(x, delta, A, B, C, D, z=z, delta_softplus=True, delta_bias=self.dt_proj.bias.float())\n",
        "            y = y.transpose(1, 2) # (B, L, ED)\n",
        "        else:\n",
        "            delta = delta.transpose(1, 2)\n",
        "            delta = F.softplus(delta + self.dt_proj.bias)\n",
        "            if self.config.pscan:\n",
        "                y = self.selective_scan(x, delta, A, B, C, D)\n",
        "            else:\n",
        "                y = self.selective_scan_seq(x, delta, A, B, C, D)\n",
        "        return y\n",
        "\n",
        "    def selective_scan(self, x, delta, A, B, C, D):\n",
        "        #Â x : (B, L, ED)\n",
        "        #Â Î” : (B, L, ED)\n",
        "        #Â A : (ED, N)\n",
        "        #Â B : (B, L, N)\n",
        "        #Â C : (B, L, N)\n",
        "        #Â D : (ED)\n",
        "        #Â y : (B, L, ED)\n",
        "        deltaA = torch.exp(delta.unsqueeze(-1) * A) #Â (B, L, ED, N)\n",
        "        deltaB = delta.unsqueeze(-1) * B.unsqueeze(2) #Â (B, L, ED, N)\n",
        "        BX = deltaB * (x.unsqueeze(-1)) #Â (B, L, ED, N)\n",
        "        hs = pscan(deltaA, BX)\n",
        "        y = (hs @ C.unsqueeze(-1)).squeeze(3) #Â (B, L, ED, N) @Â (B, L, N, 1) -> (B, L, ED, 1)\n",
        "        y = y + D * x\n",
        "        return y\n",
        "\n",
        "    def selective_scan_seq(self, x, delta, A, B, C, D):\n",
        "        #Â x : (B, L, ED)\n",
        "        #Â Î” : (B, L, ED)\n",
        "        #Â A : (ED, N)\n",
        "        #Â B : (B, L, N)\n",
        "        #Â C : (B, L, N)\n",
        "        #Â D : (ED)\n",
        "        #Â y : (B, L, ED)\n",
        "        _, L, _ = x.shape\n",
        "        deltaA = torch.exp(delta.unsqueeze(-1) * A) #Â (B, L, ED, N)\n",
        "        deltaB = delta.unsqueeze(-1) * B.unsqueeze(2) #Â (B, L, ED, N)\n",
        "        BX = deltaB * (x.unsqueeze(-1)) #Â (B, L, ED, N)\n",
        "        h = torch.zeros(x.size(0), self.config.d_inner, self.config.d_state, device=deltaA.device) #Â (B, ED, N)\n",
        "        hs = []\n",
        "        for t in range(0, L):\n",
        "            h = deltaA[:, t] * h + BX[:, t]\n",
        "            hs.append(h)\n",
        "        hs = torch.stack(hs, dim=1) #Â (B, L, ED, N)\n",
        "        y = (hs @ C.unsqueeze(-1)).squeeze(3) #Â (B, L, ED, N) @Â (B, L, N, 1) -> (B, L, ED, 1)\n",
        "        y = y + D * x\n",
        "        return y\n",
        "\n",
        "    #Â -------------------------- inference -------------------------- #\n",
        "    \"\"\"\n",
        "    Concerning auto-regressive inference\n",
        "\n",
        "    The cool part of using Mamba : inference is constant wrt to sequence length\n",
        "    We just have to keep in cache, for each layer, two things :\n",
        "    - the hidden state h (which is (B, ED, N)), as you typically would when doing inference with a RNN\n",
        "    - the last d_conv-1 inputs of the layer, to be able to compute the 1D conv which is a convolution over the time dimension\n",
        "      (d_conv is fixed so this doesn't incur a growing cache as we progress on generating the sequence)\n",
        "      (and d_conv is usually very small, like 4, so we just have to \"remember\" the last 3 inputs)\n",
        "\n",
        "    Concretely, these two quantities are put inside a cache tuple, and are named h and inputs respectively.\n",
        "    h is (B, ED, N), and inputs is (B, ED, d_conv-1)\n",
        "    The MambaBlock.step() receives this cache, and, along with outputing the output, alos outputs the updated cache for the next call.\n",
        "\n",
        "    The cache object is initialized as follows : (None, torch.zeros()).\n",
        "    When h is None, the selective scan function detects it and start with h=0.\n",
        "    The torch.zeros() isn't a problem (it's same as just feeding the input, because the conv1d is padded)\n",
        "\n",
        "    As we need one such cache variable per layer, we store a caches object, which is simply a list of cache object. (See mamba_lm.py)\n",
        "    \"\"\"\n",
        "\n",
        "    def step(self, x, cache):\n",
        "        #Â x : (B, D)\n",
        "        #Â cache : (h, inputs)\n",
        "                # h : (B, ED, N)\n",
        "                #Â inputs : (B, ED, d_conv-1)\n",
        "        #Â y : (B, D)\n",
        "        #Â cache : (h, inputs)\n",
        "        h, inputs = cache\n",
        "        xz = self.in_proj(x) # (B, 2*ED)\n",
        "        x, z = xz.chunk(2, dim=1) #Â (B, ED), (B, ED)\n",
        "\n",
        "        #Â x branch\n",
        "        x_cache = x.unsqueeze(2)\n",
        "        x = self.conv1d(torch.cat([inputs, x_cache], dim=2))[:, :, self.config.d_conv-1] #Â (B, ED)\n",
        "\n",
        "        x = F.silu(x)\n",
        "        y, h = self.ssm_step(x, h)\n",
        "\n",
        "        #Â z branch\n",
        "        z = F.silu(z)\n",
        "\n",
        "        output = y * z\n",
        "        output = self.out_proj(output) #Â (B, D)\n",
        "\n",
        "        # prepare cache for next call\n",
        "        inputs = torch.cat([inputs[:, :, 1:], x_cache], dim=2) #Â (B, ED, d_conv-1)\n",
        "        cache = (h, inputs)\n",
        "\n",
        "        return output, cache\n",
        "\n",
        "    def ssm_step(self, x, h): #Â x : (B, ED) #Â h : (B, ED, N)\n",
        "        A = -torch.exp(self.A_log.float()) # (ED, N) #Â todo : ne pas le faire tout le temps, puisque c'est indÃ©pendant de la timestep\n",
        "        D = self.D.float()\n",
        "        deltaBC = self.x_proj(x) #Â (B, dt_rank+2*N)\n",
        "        delta, B, C = torch.split(deltaBC, [self.config.dt_rank, self.config.d_state, self.config.d_state], dim=-1) #Â (B, dt_rank), (B, N), (B, N)\n",
        "        delta, B, C = self._apply_layernorms(delta, B, C)\n",
        "        delta = F.softplus(self.dt_proj(delta)) #Â (B, ED)\n",
        "\n",
        "        deltaA = torch.exp(delta.unsqueeze(-1) * A) #Â (B, ED, N)\n",
        "        deltaB = delta.unsqueeze(-1) * B.unsqueeze(1) #Â (B, ED, N)\n",
        "        BX = deltaB * (x.unsqueeze(-1)) #Â (B, ED, N)\n",
        "        if h is None:\n",
        "            h = torch.zeros(x.size(0), self.config.d_inner, self.config.d_state, device=deltaA.device) #Â (B, ED, N)\n",
        "        h = deltaA * h + BX #Â (B, ED, N)\n",
        "        y = (h @ C.unsqueeze(-1)).squeeze(2) #Â (B, ED, N) @Â (B, N, 1) -> (B, ED, 1)\n",
        "        y = y + D * x\n",
        "        return y, h\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, d_model: int, eps: float = 1e-5, use_mup: bool = False):\n",
        "        super().__init__()\n",
        "        self.use_mup = use_mup\n",
        "        self.eps = eps\n",
        "        #Â https://arxiv.org/abs/2404.05728, RMSNorm gains prevents muTransfer (section 4.2.3)\n",
        "        if not use_mup:\n",
        "            self.weight = nn.Parameter(torch.ones(d_model))\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "        if not self.use_mup:\n",
        "            return output * self.weight\n",
        "        else:\n",
        "            return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "E1wARPnMfjpE"
      },
      "outputs": [],
      "source": [
        "# @title alxndrTL mamba2.py\n",
        "# https://github.com/alxndrTL/mamba.py/blob/main/mambapy/mamba2.py\n",
        "\n",
        "\"\"\"adapted from https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba2_simple.py\n",
        "It implements a Mamba2 model BUT still relies on the official Triton code for Mamba2.\n",
        "(Coming soon hopefully is a full torch version, like in mamba.py and pscan.py)\n",
        "\n",
        "Also, the file implements a caching mecanism, and a config similar to what's being done in mamba.py, as well as supports muP.\n",
        "\n",
        "When passing an input of length 1 in the forward function, the model automatically routes the call to the step function.\n",
        "This step function does one \"classic\" RNN-like step of computation, using the input and the cache provided. It returns the output as well as the new cache.\n",
        "This is used for inference, to generate tokens one by one.\n",
        "\n",
        "Also, this model supports prefilling a prompt and decoding token by token : this is just a mix of forwarding (in parallel) the input, and then decoding step by step.\n",
        "In order to do that, we need the forward/parallel call (also used in training) to output the cache, then used to start the step by step decoding part.\n",
        "To use this mode, you need to call Mamba2.forward() with your input (of shape (B, L, D)) as well as with a non-None cache (like full zeros, it doesnt matter, just not \"None\").\n",
        "The forward call will thus return the output and the cache. From there, you can start your step by step decoding (see just above).\n",
        "\n",
        "The cache is composed of two objects :\n",
        "-h_cache: the last hidden state. Just like an RNN : you have to keep track of only the last h.\n",
        "-conv_state: because Mamba2 uses a convolution over the time sequence, with a filter of length d_conv=4, you have to keep the last d_conv-1=3 inputs of that convolution to be able to run it provided a new input.\n",
        "\n",
        "h_cache is of shape (B, n_heads, d_head, N) and is initialized at 0 (ie no starting hidden state, which is the default behavior in Mamba).\n",
        "conv_state is of shape (B, EDN * 2*n_groups*, d_conv) and is initialized at 0\n",
        "\n",
        "(B=batch_size, L=seq len, E = expand_factor, D=d_model, N=d_state)\n",
        "(TODO: make it full pytorch (ie translate mamba_chunk_scan_combined in pytorch))\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from typing import Union\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "try:\n",
        "    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\n",
        "except ImportError:\n",
        "    causal_conv1d_fn, causal_conv1d_update = None, None\n",
        "\n",
        "try:\n",
        "    from mamba_ssm.ops.triton.layernorm_gated import RMSNorm as RMSNormGated, LayerNorm\n",
        "    from mamba_ssm.ops.triton.ssd_combined import mamba_chunk_scan_combined, mamba_split_conv1d_scan_combined\n",
        "    from mamba_ssm.ops.triton.selective_state_update import selective_state_update\n",
        "\n",
        "except ImportError:\n",
        "    RMSNormGated, LayerNorm = None, None\n",
        "    mamba_chunk_scan_combined, mamba_split_conv1d_scan_combined = None, None\n",
        "    selective_state_update = None\n",
        "\n",
        "@dataclass\n",
        "class Mamba2Config:\n",
        "    d_model: int #Â D\n",
        "    n_layers: int\n",
        "    d_head: int #Â todo : plutot n_heads non ?\n",
        "    d_state: int = 64 #Â N in paper/comments\n",
        "    expand_factor: int = 2 #Â E in paper/comments\n",
        "    d_conv: int = 4\n",
        "    n_groups: int = 1#Â todo : ??\n",
        "\n",
        "    A_init_range: tuple = (1, 16)\n",
        "    dt_min: float = 0.001\n",
        "    dt_max: float = 0.1\n",
        "    dt_init_floor: float = 1e-4\n",
        "    dt_limit: tuple = (0.0, float(\"inf\"))\n",
        "    conv_init = None\n",
        "\n",
        "    learnable_init_states: bool = False\n",
        "    activation: str = \"swish\" #Â \"swish\" or \"silu\"\n",
        "\n",
        "    rms_norm_eps: float = 1e-5\n",
        "    base_std: float = 0.02\n",
        "\n",
        "    bias: bool = False\n",
        "    conv_bias: bool = True\n",
        "\n",
        "    mup: bool = False\n",
        "    mup_base_width: float = 128 # width=d_model\n",
        "\n",
        "    chunk_size: int = 256\n",
        "    use_mem_eff_path: bool = True\n",
        "    dtype=None\n",
        "    device=None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.d_inner = self.expand_factor * self.d_model # E*D = ED in comments\n",
        "        self.n_heads = self.d_inner // self.d_head\n",
        "        assert self.d_inner % self.d_head == 0\n",
        "        assert (self.d_inner / self.d_head) % 8 == 0, \"requierement of causal_conv1d\"\n",
        "\n",
        "        # # muP\n",
        "        # if self.mup:\n",
        "        #     self.mup_width_mult = self.d_model / self.mup_base_width\n",
        "\n",
        "class Mamba2(nn.Module):\n",
        "    def __init__(self, config: Mamba2Config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layers = nn.ModuleList([ResidualBlock(config) for _ in range(config.n_layers)])\n",
        "\n",
        "    def forward(self, x, caches=None): # (B, L, D)\n",
        "        if caches is None:\n",
        "            caches = [None] * self.config.n_layers\n",
        "\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x, caches[i] = layer(x, caches[i])\n",
        "\n",
        "        if caches[0] == None:\n",
        "            return x\n",
        "        else:\n",
        "            return x, caches\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, config: Mamba2Config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.mixer = Mamba2Block(self.config)\n",
        "        self.norm = nn.RMSNorm(self.config.d_model, self.config.rms_norm_eps, self.config.mup)\n",
        "\n",
        "    def forward(self, x, cache=None): # (B, L, D)\n",
        "        output, cache = self.mixer(self.norm(x), cache)\n",
        "        output = output + x # (B, L, D)\n",
        "        return output, cache\n",
        "\n",
        "    def get_empty_cache(self, batch_size):\n",
        "        h_cache = torch.zeros(batch_size, self.config.n_heads, self.config.d_head, self.config.d_state, device=self.mixer.in_proj.weight.device, dtype=self.mixer.in_proj.weight.dtype)\n",
        "        conv_cache = torch.zeros(batch_size, self.mixer.conv1d.weight.shape[0], self.config.d_conv, device=self.mixer.conv1d.weight.device, dtype=self.mixer.conv1d.weight.dtype)\n",
        "        return (h_cache, conv_cache)\n",
        "\n",
        "class Mamba2Block(nn.Module):\n",
        "    def __init__(self, config: Mamba2Config):\n",
        "        super().__init__()\n",
        "        factory_kwargs = {\"device\": config.device, \"dtype\": config.dtype}\n",
        "        self.config = config\n",
        "        # [z, x, B, C, dt]\n",
        "        d_in_proj = 2 * self.config.d_inner + 2 * self.config.n_groups * self.config.d_state + self.config.n_heads\n",
        "        self.in_proj = nn.Linear(self.config.d_model, d_in_proj, bias=self.config.bias, **factory_kwargs)\n",
        "\n",
        "        conv_dim = self.config.d_inner + 2 * self.config.n_groups * self.config.d_state\n",
        "        self.conv1d = nn.Conv1d(conv_dim, conv_dim, self.config.d_conv, padding=self.config.d_conv - 1,\n",
        "            bias=self.config.conv_bias, groups=conv_dim, **factory_kwargs,)\n",
        "\n",
        "        if self.config.conv_init is not None:\n",
        "            nn.init.uniform_(self.conv1d.weight, -self.config.conv_init, self.config.conv_init)\n",
        "        # self.conv1d.weight._no_weight_decay = True\n",
        "\n",
        "        #Â todo : mup init + lr\n",
        "        if self.config.learnable_init_states:\n",
        "            self.init_states = nn.Parameter(torch.zeros(self.config.n_heads, self.config.d_head, self.config.d_state, **factory_kwargs))\n",
        "            self.init_states._no_weight_decay = True\n",
        "\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "        # Initialize log dt bias\n",
        "        dt = torch.exp(\n",
        "            torch.rand(self.config.n_heads, **factory_kwargs) * (math.log(self.config.dt_max) - math.log(self.config.dt_min))\n",
        "            + math.log(self.config.dt_min)\n",
        "        )\n",
        "        dt = torch.clamp(dt, min=self.config.dt_init_floor)\n",
        "        # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n",
        "        inv_dt = dt + torch.log(-torch.expm1(-dt))\n",
        "        self.dt_bias = nn.Parameter(inv_dt)\n",
        "        # Just to be explicit. Without this we already don't put wd on dt_bias because of the check\n",
        "        # name.endswith(\"bias\") in param_grouping.py\n",
        "        self.dt_bias._no_weight_decay = True\n",
        "\n",
        "        # A parameter\n",
        "        assert self.config.A_init_range[0] > 0 and self.config.A_init_range[1] >= self.config.A_init_range[0]\n",
        "        A = torch.empty(self.config.n_heads, dtype=torch.float32, device=self.config.device).uniform_(*self.config.A_init_range)\n",
        "        A_log = torch.log(A).to(dtype=self.config.dtype)\n",
        "        self.A_log = nn.Parameter(A_log)\n",
        "        # self.register_buffer(\"A_log\", torch.zeros(self.nheads, dtype=torch.float32, device=device), persistent=True)\n",
        "        self.A_log._no_weight_decay = True\n",
        "\n",
        "        # D \"skip\" parameter\n",
        "        self.D = nn.Parameter(torch.ones(self.config.n_heads, device=self.config.device))\n",
        "        self.D._no_weight_decay = True\n",
        "\n",
        "        # Extra normalization layer right before output projection\n",
        "        self.norm = RMSNormGated(self.config.d_inner, eps=1e-5, norm_before_gate=False, **factory_kwargs)\n",
        "\n",
        "        self.out_proj = nn.Linear(self.config.d_inner, self.config.d_model, bias=self.config.bias, **factory_kwargs)\n",
        "\n",
        "    def forward(self, u, cache=None, seq_idx=None): # (B, L, D)\n",
        "        batch, length, _ = u.shape\n",
        "        return_cache = False\n",
        "        if cache is not None and length > 1:\n",
        "            cache = None\n",
        "            return_cache = True\n",
        "\n",
        "        if cache is not None:\n",
        "            out, cache = self.step(u, cache)\n",
        "            return out, cache\n",
        "\n",
        "        zxbcdt = self.in_proj(u)  # (B, L, d_in_proj)\n",
        "        A = -torch.exp(self.A_log)  # (nheads) or (d_inner, d_state)\n",
        "        initial_states=repeat(self.init_states, \"... -> b ...\", b=batch) if self.config.learnable_init_states else None\n",
        "        dt_limit_kwargs = {} if self.config.dt_limit == (0.0, float(\"inf\")) else dict(dt_limit=self.config.dt_limit)\n",
        "\n",
        "        if self.config.use_mem_eff_path:\n",
        "            # Fully fused path\n",
        "            out = mamba_split_conv1d_scan_combined(\n",
        "                zxbcdt,\n",
        "                rearrange(self.conv1d.weight, \"d 1 w -> d w\"),\n",
        "                self.conv1d.bias,\n",
        "                self.dt_bias,\n",
        "                A,\n",
        "                D=self.D,\n",
        "                chunk_size=self.config.chunk_size,\n",
        "                seq_idx=seq_idx,\n",
        "                activation=self.config.activation,\n",
        "                rmsnorm_weight=self.norm.weight,\n",
        "                rmsnorm_eps=self.norm.eps,\n",
        "                outproj_weight=self.out_proj.weight,\n",
        "                outproj_bias=self.out_proj.bias,\n",
        "                headdim=self.config.d_head,\n",
        "                ngroups=self.config.n_groups,\n",
        "                norm_before_gate=False,\n",
        "                initial_states=initial_states,\n",
        "                return_final_states=return_cache,\n",
        "                **dt_limit_kwargs,\n",
        "            )\n",
        "\n",
        "            if return_cache:\n",
        "                #Â get h_cache from output\n",
        "                out, h_cache = out\n",
        "\n",
        "                # compute conv_cache with last d_conv entries of xBC\n",
        "                _, xBC, _ = torch.split(zxbcdt, [self.config.d_inner, self.config.d_inner + 2 * self.config.n_groups * self.config.d_state, self.config.n_heads], dim=-1)\n",
        "                conv_cache = xBC[:, -self.config.d_conv:].transpose(1, 2) # (error if seqlen<d_conv)\n",
        "\n",
        "                cache = (h_cache, conv_cache)\n",
        "\n",
        "        else:\n",
        "            z, xBC, dt = torch.split(\n",
        "                zxbcdt, [self.config.d_inner, self.config.d_inner + 2 * self.config.n_groups * self.config.d_state, self.config.n_heads], dim=-1\n",
        "            )\n",
        "            dt = F.softplus(dt + self.dt_bias)  # (B, L, nheads)\n",
        "            assert self.config.activation in [\"silu\", \"swish\"]\n",
        "\n",
        "            # 1D Convolution\n",
        "            if causal_conv1d_fn is None or self.config.activation not in [\"silu\", \"swish\"]:\n",
        "                xBC = self.act(self.conv1d(xBC.transpose(1, 2)).transpose(1, 2)) # (B, L, self.d_inner + 2 * n_groups * d_state)\n",
        "            else:\n",
        "                xBC = causal_conv1d_fn(\n",
        "                    x=xBC.transpose(1, 2),\n",
        "                    weight=rearrange(self.conv1d.weight, \"d 1 w -> d w\"),\n",
        "                    bias=self.conv1d.bias,\n",
        "                    activation=self.config.activation,\n",
        "                ).transpose(1, 2)\n",
        "\n",
        "            # split into 3 main branches: X, B, C\n",
        "            # These correspond to V, K, Q respectively in the SSM/attention duality\n",
        "            x, B, C = torch.split(xBC, [self.config.d_inner, self.config.n_groups * self.config.d_state, self.config.n_groups * self.config.d_state], dim=-1)\n",
        "            y = mamba_chunk_scan_combined(\n",
        "                rearrange(x, \"b l (h p) -> b l h p\", p=self.config.d_head),\n",
        "                dt,\n",
        "                A,\n",
        "                rearrange(B, \"b l (g n) -> b l g n\", g=self.config.n_groups),\n",
        "                rearrange(C, \"b l (g n) -> b l g n\", g=self.config.n_groups),\n",
        "                chunk_size=self.config.chunk_size,\n",
        "                D=self.D,\n",
        "                z=None,\n",
        "                seq_idx=seq_idx,\n",
        "                initial_states=initial_states,\n",
        "                **dt_limit_kwargs,\n",
        "            )\n",
        "            y = rearrange(y, \"b l h p -> b l (h p)\")\n",
        "\n",
        "            # Multiply \"gate\" branch and apply extra normalization layer\n",
        "            y = self.norm(y, z)\n",
        "            out = self.out_proj(y) # (B, L, D)\n",
        "        return out, cache\n",
        "\n",
        "    def step(self, u, cache): # (B, 1, D)\n",
        "        h_cache, conv_cache = cache\n",
        "\n",
        "        zxbcdt = self.in_proj(u.squeeze(1))  # (B, 2D)\n",
        "        d_mlp = (zxbcdt.shape[-1] - 2 * self.config.d_inner - 2 * self.config.n_groups * self.config.d_state - self.config.n_heads) // 2\n",
        "        z0, x0, z, xBC, dt = torch.split(zxbcdt, [d_mlp, d_mlp, self.config.d_inner, self.config.d_inner + 2 * self.config.n_groups * self.config.d_state, self.config.n_heads], dim=-1)\n",
        "\n",
        "        # conv step\n",
        "        if causal_conv1d_update is None:\n",
        "            conv_cache.copy_(torch.roll(conv_cache, shifts=-1, dims=-1)) # update state (B, D, W)\n",
        "            conv_cache[:, :, -1] = xBC\n",
        "            xBC = torch.sum(conv_cache * rearrange(self.conv1d.weight, \"d 1 w -> d w\"), dim=-1) # (B, D)\n",
        "            if self.conv1d.bias is not None:\n",
        "                xBC = xBC + self.conv1d.bias\n",
        "            xBC = self.act(xBC).to(dtype=x.dtype)\n",
        "        else:\n",
        "            xBC = causal_conv1d_update(xBC, conv_cache, rearrange(self.conv1d.weight, \"d 1 w -> d w\"), self.conv1d.bias, self.config.activation)\n",
        "        x, B, C = torch.split(xBC, [self.config.d_inner, self.config.n_groups * self.config.d_state, self.config.n_groups * self.config.d_state], dim=-1)\n",
        "        A = -torch.exp(self.A_log.float()) # (n_heads)\n",
        "\n",
        "        # SSM step\n",
        "        if selective_state_update is None:\n",
        "            assert self.config.n_groups == 1, \"Only support ngroups=1 for this inference code path\"\n",
        "            # discretize A\n",
        "            dt = F.softplus(dt + self.dt_bias.to(dtype=dt.dtype))  # (B, n_heads)\n",
        "            dA = torch.exp(dt * A)  # (B, n_heads)\n",
        "            # discretize B\n",
        "            x = rearrange(x, \"b (h p) -> b h p\", p=self.config.d_head)\n",
        "            dBx = torch.einsum(\"bh,bn,bhp->bhpn\", dt, B, x)\n",
        "            # compute one step\n",
        "            h_cache.copy_(h_cache * rearrange(dA, \"b h -> b h 1 1\") + dBx)\n",
        "            # compute output\n",
        "            y = torch.einsum(\"bhpn,bn->bhp\", h_cache.to(x.dtype), C)\n",
        "            y = y + rearrange(self.D.to(x.dtype), \"h -> h 1\") * x\n",
        "            y = rearrange(y, \"b h p -> b (h p)\")\n",
        "\n",
        "        else:\n",
        "            A = repeat(A, \"h -> h p n\", p=self.config.d_head, n=self.config.d_state).to(dtype=torch.float32)\n",
        "            dt = repeat(dt, \"b h -> b h p\", p=self.config.d_head)\n",
        "            dt_bias = repeat(self.dt_bias, \"h -> h p\", p=self.config.d_head)\n",
        "            D = repeat(self.D, \"h -> h p\", p=self.config.d_head)\n",
        "            B = rearrange(B, \"b (g n) -> b g n\", g=self.config.n_groups)\n",
        "            C = rearrange(C, \"b (g n) -> b g n\", g=self.config.n_groups)\n",
        "            x_reshaped = rearrange(x, \"b (h p) -> b h p\", p=self.config.d_head)\n",
        "\n",
        "            y = selective_state_update(h_cache, x_reshaped, dt, A, B, C, D, z=None, dt_bias=dt_bias, dt_softplus=True)\n",
        "            y = rearrange(y, \"b h p -> b (h p)\")\n",
        "\n",
        "        #if self.rmsnorm:\n",
        "        y = self.norm(y, z)\n",
        "        if d_mlp > 0:\n",
        "            y = torch.cat([F.silu(z0) * x0, y], dim=-1)\n",
        "        out = self.out_proj(y)\n",
        "        return out.unsqueeze(1), (h_cache, conv_cache)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ohp9YS8GZHn8"
      },
      "source": [
        "## state-spaces mamba-ssm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "collapsed": true,
        "id": "kIBNsqJhm0T6"
      },
      "outputs": [],
      "source": [
        "# @title pip install mamba-ssm\n",
        "# !pip install mamba-ssm[causal-conv1d]\n",
        "# !pip install mamba-ssm\n",
        "# !wget https://github.com/state-spaces/mamba/releases/download/v2.2.2/mamba_ssm-2.2.2+cu118torch2.1cxx11abiFALSE-cp310-cp310-linux_x86_64.whl\n",
        "# !pip install mamba_ssm-2.2.2+cu118torch2.1cxx11abiFALSE-cp310-cp310-linux_x86_64.whl\n",
        "\n",
        "# !pip install torch==2.1.1 torchvision==0.16.1 torchaudio==2.1.1 --index-url https://download.pytorch.org/whl/cu118\n",
        "# !pip install causal_conv1d==1.1.1\n",
        "# !pip install mamba-ssm==1.2.0.post1\n",
        "\n",
        "\n",
        "!pip install https://github.com/Dao-AILab/causal-conv1d/releases/download/v1.4.0/causal_conv1d-1.4.0+cu118torch2.0cxx11abiFALSE-cp310-cp310-linux_x86_64.whl\n",
        "!pip install mamba-ssm\n",
        "\n",
        "\n",
        "# !pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu121\n",
        "# !pip uninstall mamba-ssm causal-conv1d\n",
        "# !pip install causal-conv1d && pip install mamba-ssm\n",
        "\n",
        "\n",
        "\n",
        "# !pip install mamba-ssm[causal-conv1d]==1.0.1\n",
        "\n",
        "# !pip install mamba-ssm causal-conv1d\n",
        "\n",
        "# !pip install https://github.com/Dao-AILab/causal-conv1d/releases/download/v1.4.0/causal_conv1d-1.4.0+cu118torch2.0cxx11abiFALSE-cp310-cp310-linux_x86_64.whl\n",
        "# !pip install mamba-ssm\n",
        "\n",
        "# !apt-get install -y cuda-11-8\n",
        "\n",
        "# import os\n",
        "# os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda-11.8/lib64:' + os.environ.get('LD_LIBRARY_PATH', '') # Adds the CUDA library path to LD_LIBRARY_PATH\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8Wzafn6ka8tR"
      },
      "outputs": [],
      "source": [
        "# @title huggingface mamba2\n",
        "# https://huggingface.co/docs/transformers/en/model_doc/mamba2\n",
        "# https://github.com/huggingface/transformers/blob/main/src/transformers/models/mamba2/modeling_mamba2.py#L825\n",
        "import torch\n",
        "# import torch.nn as nn\n",
        "from transformers import Mamba2Config, Mamba2Model\n",
        "\n",
        "in_dim=132\n",
        "b,t,d = 4,5,64#256\n",
        "try: vocab_size=train_loader.dataset.vocab_size#50\n",
        "except NameError: vocab_size=50\n",
        "\n",
        "# num_heads*head_dim=2*hidden_size ?\n",
        "# *6\n",
        "\n",
        "# hidden_size * expand (128) must equal num_heads * head_dim\n",
        "config = Mamba2Config(hidden_size=d, num_heads=8, head_dim=16, state_size=64, num_hidden_layers=2, output_hidden_states=True, vocab_size=vocab_size)\n",
        "model = Mamba2Model(config).to(device)\n",
        "# model.embeddings = nn.Linear(in_dim, d)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "optim = torch.optim.AdamW(model.parameters(), 1e-3) # og betas=(0.9, 0.999), wd.01\n",
        "# optim = torch.optim.AdamW(model.parameters(), 1e-3, betas=(0.9, 0.95), weight_decay=1e-1) # og wd.01\n",
        "\n",
        "\n",
        "# x = torch.randn(batch, seq_len, in_dim)\n",
        "x = torch.randint(vocab_size, (b,t), device=device)\n",
        "\n",
        "# out = model(x, use_cache=True)\n",
        "# # print(out)\n",
        "# print(out.keys())\n",
        "# y, cache = out.last_hidden_state, out.cache_params\n",
        "# # print(y)\n",
        "# print(y.shape) # [b,t,d]\n",
        "# # print(cache)\n",
        "# y = y @ model.embeddings.weight.T\n",
        "# print(y.shape)\n",
        "        # logits = model(x).last_hidden_state\n",
        "        # logits = logits @ model.embeddings.weight.T # for hf mamba2\n",
        "\n",
        "# h = torch.cat(out.hidden_states, dim=-1) # [batch, seq_len, (num_hidden_layers + 1 ?) * d_model]\n",
        "# print(h.shape)\n",
        "# print(model)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AR_ABF62y6SX"
      },
      "outputs": [],
      "source": [
        "# @title from SimplerMambaSSM.ipynb\n",
        "# https://colab.research.google.com/drive/1g9qpeVcFa0ca0cnhmqusO4RZtQdh9umY?usp=sharing\n",
        "# https://www.reddit.com/r/MachineLearning/comments/18d65bz/d_thoughts_on_mamba/?share_id=6IQS8bFpiQNGLsQ-x5xZW\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import  functional as F\n",
        "from torch.nn.parameter import Parameter\n",
        "from tqdm import tqdm\n",
        "from mamba_ssm import Mamba\n",
        "# ---------\n",
        "with open(\"input.txt\", \"r\") as f:\n",
        "  text = f.read()\n",
        "\n",
        "# Unique characters\n",
        "chars = sorted(list(set(text)))\n",
        "print(''.join(chars))\n",
        "vocab_size = len(chars)\n",
        "print(vocab_size)\n",
        "\n",
        "#Tokenizers\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "encode = lambda xx: [stoi[x] for x in xx]\n",
        "decode = lambda xx: ''.join([itos[x] for x in xx])\n",
        "encode(\"Hello!\")\n",
        "print(decode(encode(\"Hello!\")))\n",
        "\n",
        "\n",
        "# train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(len(data)*0.9)\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "  # generate targets and context\n",
        "  if split == \"train\":\n",
        "    data = train_data\n",
        "  else:\n",
        "    data = val_data\n",
        "  index = torch.randint(0,len(data)-block_size,(batch_size,))\n",
        "  x = torch.stack([data[ind:ind+block_size] for ind in index])\n",
        "  y = torch.stack([data[ind+1:ind+block_size+1] for ind in index])\n",
        "  return x.to(device),y.to(device)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'test']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X,Y = get_batch(split)\n",
        "      logits, loss = model(X,Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, n_embed, dropout=.2):\n",
        "    super().__init__()\n",
        "    self.ffn = nn.Sequential(\n",
        "      nn.Linear(n_embed, 4*n_embed), nn.ReLU(),\n",
        "      nn.Linear(4*n_embed, n_embed),\n",
        "      nn.Dropout(dropout),\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    return self.ffn(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, n_embed, n_heads):\n",
        "    super().__init__()\n",
        "    self.head_size = n_embed // n_heads\n",
        "    # self.sa_head = MultiHeadAttention(n_heads, self.head_size)\n",
        "    self.sa_head = Mamba(\n",
        "      # This module uses roughly 3 * expand * d_model^2 parameters\n",
        "      d_model=n_embed, # Model dimension d_model\n",
        "      d_state=16,  # SSM state expansion factor\n",
        "      d_conv=4,    # Local convolution width\n",
        "      expand=2,    # Block expansion factor\n",
        "  ).to(\"cuda\")\n",
        "    self.ffn = FeedForward(n_embed)\n",
        "    self.ln1 = nn.RMSNorm(n_embed)\n",
        "    self.ln2 = nn.RMSNorm(n_embed)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.sa_head(self.ln1(x))\n",
        "    x = x + self.ffn(self.ln2(x))\n",
        "    return x\n",
        "\n",
        "class BigramNeuralNetwork(nn.Module):\n",
        "  def __init__(self,vocab_size, n_layers):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size,n_embed)\n",
        "    self.position_embedding_table = nn.Embedding(block_size,n_embed)\n",
        "    # self.sa_head = MultiHeadAttention(4,int(n_embed/4))\n",
        "    self.lm_head = nn.Linear(n_embed,vocab_size)\n",
        "    # self.ffn = FeedForward(n_embed)\n",
        "    self.blocks = nn.Sequential(*[Block(n_embed,n_heads=n_heads) for _ in range(n_layers)])\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    # idx = idx[:,-block_size:]\n",
        "    B,T = idx.shape\n",
        "    tok_emb = self.token_embedding_table(idx) # (B,T,C_e)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T,device=device)) # (T,C_e)\n",
        "    x = tok_emb + pos_emb # (B,T,C_e)\n",
        "    x = self.blocks(x) # (B,T,C_e)\n",
        "    logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B,T,C = logits.shape\n",
        "      logits = logits.view(B*T,C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "      logits = logits.view(B,T,C)\n",
        "    return logits, loss\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is (B,T)\n",
        "    idx_next = []\n",
        "    for i in range(max_new_tokens):\n",
        "      idx_cond = idx[:,-block_size:]\n",
        "      logits, loss = self(idx_cond)\n",
        "      last_timestep = logits[:,-1,:]\n",
        "      probs = F.softmax(last_timestep, dim=1)\n",
        "      next_index = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, next_index), dim=1)\n",
        "    for arr in idx:\n",
        "      print(decode(arr.cpu().detach().numpy()))\n",
        "    return idx\n",
        "\n",
        "\n",
        "#hyperparams\n",
        "epochs = 100\n",
        "lr = 1e-3\n",
        "batch_size = 64\n",
        "block_size = 256\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "max_iters = 10000\n",
        "print_iters = 100\n",
        "eval_iters = 10\n",
        "eval_interval = 300\n",
        "n_embed=384\n",
        "n_heads = 6\n",
        "n_layers = 6\n",
        "\n",
        "\n",
        "model = BigramNeuralNetwork(vocab_size, n_layers=n_layers)\n",
        "optimizer = torch.optim.AdamW(model.parameters(),lr=lr)\n",
        "\n",
        "# checkpoint = torch.load('model.pt')\n",
        "# model.load_state_dict(checkpoint['model_state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "# epoch = checkpoint['epoch']\n",
        "checkpoint_path = None#\"./differentattention/model_40.pt\"\n",
        "epoch = 0\n",
        "if checkpoint_path:\n",
        "  checkpoint = torch.load(checkpoint_path)\n",
        "  print(checkpoint)\n",
        "  if checkpoint['model_state_dict']:\n",
        "    model.load_state_dict(checkpoint['model_state_dict'].to(device))\n",
        "  if checkpoint['optimizer_state_dict']:\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  epoch = checkpoint['epoch']\n",
        "device = \"cuda\"\n",
        "m = model.to(device)\n",
        "print(\"Uses device \" + device)\n",
        "MODEL_CHECKPOINT = \"./differentattention/model_{iter}.pt\"\n",
        "losses_data = {\"train\":[], \"test\":[]}\n",
        "for iter in tqdm(range(epoch ,max_iters)):\n",
        "  if iter % eval_iters == 0:\n",
        "    losses = estimate_loss()\n",
        "    losses_data['train'].append(losses['train'].cpu().numpy())\n",
        "    losses_data['test'].append(losses['test'].cpu().numpy())\n",
        "    print(f\"Step {iter}, train loss:{losses['train']:.4f}, test loss:{losses['test']:.4f}\")\n",
        "\n",
        "  if iter % print_iters == 0:\n",
        "    losses = estimate_loss()\n",
        "    torch.save({\n",
        "            'epoch': iter,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': losses,\n",
        "            }, MODEL_CHECKPOINT.format(iter=iter))\n",
        "    losses_data['train'].append(losses['train'].cpu().numpy())\n",
        "    losses_data['test'].append(losses['test'].cpu().numpy())\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      #Generate from the model:\n",
        "      output = m.generate(torch.zeros((1,2), dtype=torch.long).to(device).contiguous(), 1000  )[0].tolist()\n",
        "\n",
        "    print(f\"Step {iter}, train loss:{losses['train']:.4f}, test loss:{losses['test']:.4f}\")\n",
        "    model.train()\n",
        "\n",
        "  #Get data\n",
        "  xb,yb = get_batch(\"train\")\n",
        "\n",
        "  #Evaluate loss\n",
        "  logits,loss = model(xb,yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  torch.nn.utils.clip_grad.clip_grad_norm_(model.parameters(), 1.0)\n",
        "  optimizer.step()\n",
        "torch.save(model.state_dict(), \"./differentattention/model.pt\")\n",
        "#Generate from the model:\n",
        "output = m.generate(torch.zeros((1,2), dtype=torch.long).to(device),1000)[0].tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qXaHvdXQ07e1"
      },
      "outputs": [],
      "source": [
        "# @title mamba S6\n",
        "# https://arxiv.org/pdf/2312.00752\n",
        "# https://github.com/state-spaces/mamba\n",
        "\n",
        "from mamba_ssm import Mamba2\n",
        "model = Mamba2(\n",
        "    # This module uses roughly 3 * expand * d_model^2 parameters\n",
        "    d_model=dim, # Model dimension d_model\n",
        "    d_state=64,  # SSM state expansion factor, typically 64 or 128\n",
        "    d_conv=4,    # Local convolution width\n",
        "    expand=2,    # Block expansion factor\n",
        ").to(\"cuda\")\n",
        "y = model(x)\n",
        "assert y.shape == x.shape\n",
        "\n",
        "\n",
        "\n",
        "# https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba2_simple.py\n",
        "\n",
        "# https://github.com/state-spaces/mamba/blob/main/mamba_ssm/models/mixer_seq_simple.py#L118\n",
        "pred = MixerModel(d_model=256, n_layer=1, d_intermediate=1, vocab_size=0,\n",
        "        # ssm_cfg=None,\n",
        "        # attn_layer_idx=None,\n",
        "        # attn_cfg=None,\n",
        "        # norm_epsilon: float = 1e-5,\n",
        "        rms_norm = True,\n",
        "        # initializer_cfg=None,\n",
        "        # fused_add_norm=True,\n",
        "        # residual_in_fp32=False,\n",
        "        # device=None,\n",
        "        # dtype=None,\n",
        "    )\n",
        "\n",
        "\n",
        "pred.embedding = lambda x: x\n",
        "\n",
        "batch=4\n",
        "seq_len=5\n",
        "d_model=256\n",
        "x = torch.randn(batch, seq_len, d_model, device=\"cuda\")\n",
        "y = pred(x)\n",
        "\n",
        "\n",
        "# torch.rand()\n",
        "\n",
        "\n",
        "\n",
        "# # https://github.com/state-spaces/mamba/blob/main/mamba_ssm/models/config_mamba.py\n",
        "# from dataclasses import dataclass, field\n",
        "\n",
        "# dataclass\n",
        "# class MambaConfig:\n",
        "#     d_model = 256\n",
        "#     d_intermediate = 0\n",
        "#     n_layer = 1\n",
        "#     vocab_size = 1\n",
        "#     ssm_cfg: dict = field(default_factory=dict)\n",
        "#     attn_layer_idx: list = field(default_factory=list)\n",
        "#     attn_cfg: dict = field(default_factory=dict)\n",
        "#     rms_norm = True\n",
        "#     residual_in_fp32 = True\n",
        "#     fused_add_norm = True\n",
        "#     pad_vocab_size_multiple = 8\n",
        "#     tie_embeddings = True\n",
        "\n",
        "\n",
        "# https://github.com/state-spaces/mamba/blob/main/mamba_ssm/models/mixer_seq_simple.py#L215\n",
        "# pred = MambaLMHeadModel(config: MambaConfig,\n",
        "#         initializer_cfg=None,\n",
        "#         device=None,\n",
        "#         dtype=None,\n",
        "#     )\n",
        "\n",
        "# pred.backbone.embedding = lambda x: x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmPNbyTn06pq"
      },
      "outputs": [],
      "source": [
        "# training\n",
        "\n",
        "\n",
        "# https://github.com/redotvideo/mamba-chat/blob/main/train_mamba.py\n",
        "# https://github.com/redotvideo/mamba-chat/blob/main/trainer/mamba_trainer.py\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "iXNPTPhMqt8Q"
      },
      "outputs": [],
      "source": [
        "# @title _mamba_chunk_scan_combined_fwd\n",
        "# https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/triton/ssd_combined.py#L331\n",
        "\n",
        "# x/z: [b,t,h,d]\n",
        "# B/C: [b,t,g,s]\n",
        "# dt: [b,t,h]\n",
        "# A: [h]\n",
        "# D: [h,d]/[h]\n",
        "# def _mamba_chunk_scan_combined_fwd(x, dt, A, B, C, chunk_size, D=None, z=None, dt_bias=None, initial_states=None, seq_idx=None, cu_seqlens=None, dt_softplus=False, dt_limit=(0.0, float(\"inf\"))):\n",
        "def _mamba_chunk_scan_combined_fwd(x, dt, A, B, C, chunk_size, D=None, z=None, dt_bias=None, initial_states=None, seq_idx=None, dt_softplus=False, dt_limit=(0.0, float(\"inf\"))):\n",
        "    batch, seqlen, nheads, headdim = x.shape\n",
        "    _, _, ngroups, dstate = B.shape\n",
        "    assert nheads % ngroups == 0\n",
        "    if D is not None:\n",
        "        assert D.shape == (nheads, headdim) or D.shape == (nheads,)\n",
        "    if seq_idx is not None:\n",
        "        assert seq_idx.shape == (batch, seqlen)\n",
        "    if initial_states is not None:\n",
        "        assert initial_states.shape == (batch, nheads, headdim, dstate)\n",
        "    # # (batch, nchunks, chunk_size, chunk_size) or (batch, nchunks, nheads, chunk_size, chunk_size)\n",
        "    dA_cumsum, dt = _chunk_cumsum_fwd(dt, A, chunk_size, dt_bias=dt_bias, dt_softplus=dt_softplus, dt_limit=dt_limit)\n",
        "    states = _chunk_state_fwd(B, x, dt, dA_cumsum, seq_idx=seq_idx, states_in_fp32=True)\n",
        "    states, final_states = _state_passing_fwd(rearrange(states, \"... p n -> ... (p n)\"), dA_cumsum[:, :, :, -1],\n",
        "                                              initial_states=rearrange(initial_states, \"... p n -> ... (p n)\") if initial_states is not None else None,\n",
        "                                              seq_idx=seq_idx, chunk_size=chunk_size, out_dtype=C.dtype)\n",
        "    states, final_states = [rearrange(t, \"... (p n) -> ... p n\", n=dstate) for t in [states, final_states]]\n",
        "    # states_tmp0 = rearrange(_state_passing_fwd(rearrange(states_tmp0, \"... p n -> ... (p n)\"), dA_cumsum_tmp0[:, :, :, -1], chunk_size=chunk_size), \"... (p n) -> ... p n\", n=dstate)\n",
        "    # states_tmp1 = rearrange(_state_passing_fwd(rearrange(states_tmp1, \"... p n -> ... (p n)\"), dA_cumsum_tmp1[:, :, :, -1], chunk_size=chunk_size), \"... (p n) -> ... p n\", n=dstate)\n",
        "    CB = _bmm_chunk_fwd(C, B, chunk_size, seq_idx=seq_idx, output_dtype=torch.float32)\n",
        "    out, out_x = _chunk_scan_fwd(CB, x, dt, dA_cumsum, C, states, D=D, z=z, seq_idx=seq_idx)\n",
        "    # if cu_seqlens is None:\n",
        "    return out, out_x, dt, dA_cumsum, states, final_states\n",
        "    # else:\n",
        "    #     assert batch == 1, \"passing cu_seqlens to get the varlen states is only supported if batch dimension is 1\"\n",
        "    #     varlen_states = chunk_state_varlen(B.squeeze(0), x.squeeze(0), dt.squeeze(0), dA_cumsum.squeeze(0), cu_seqlens, states.squeeze(0))\n",
        "    #     return out, out_x, dt, dA_cumsum, states, final_states, varlen_states\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ahAtg1SnEJ42"
      },
      "outputs": [],
      "source": [
        "# @title selective_state_update_ref\n",
        "# https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/triton/selective_state_update.py\n",
        "\n",
        "# state: [b,h,d,s]\n",
        "# x/z: [b,h,d]\n",
        "# B/C: [b,g,s]\n",
        "# dt: [b,h,d]\n",
        "# dt_bias: [h,d]\n",
        "# A: [h,d,s]\n",
        "# D: [h,d]\n",
        "\n",
        "def selective_state_update_ref(state, x, dt, A, B, C, D=None, z=None, dt_bias=None):\n",
        "    batch, nheads, dim, dstate = state.shape\n",
        "    ngroups = B.shape[1]\n",
        "    assert nheads % ngroups == 0, \"nheads must be divisible by ngroups\"\n",
        "\n",
        "    dt = F.softplus(dt + dt_bias) # [b,h,d]\n",
        "    dA = torch.exp(dt.unsqueeze(-1) * A)  # [b,h,d,1]*[h,d,s]->[b,h,d,s]\n",
        "    # B = repeat(B, \"b g n -> b (g h) n\", h=nheads // ngroups)  # (batch, nheads, dstate)\n",
        "    # B = B.repeat(1,nheads//ngroups,1) # [b,g,s]->[b,h,s]\n",
        "    B = B.repeat_interleave(nheads//ngroups, dim=1) # [b,g,s]->[b,h,s]\n",
        "    C = C.repeat_interleave(nheads//ngroups, dim=1) # [b,g,s]->[b,h,s]\n",
        "    dB = dt.unsqueeze(-1) * B.unsqueeze(-2) # [b,h,d,1]*[b,h,1,s]->[b,h,d,s]\n",
        "    state.copy_(state * dA + dB * x.unsqueeze(-1)) # [b,h,d,s]\n",
        "    out = torch.einsum(\"bhdn,bhn->bhd\", state.to(C.dtype), C) # [b,h,d]\n",
        "    out += (x * D).to(out.dtype)\n",
        "    out = out * F.silu(z).to(x.dtype)\n",
        "    return out # [b,h,d]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5A83K1GCexL6"
      },
      "outputs": [],
      "source": [
        "# @title mamba_ssm/models/mamba2.py\n",
        "# https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba2.py\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange, repeat\n",
        "try: from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\n",
        "except ImportError: causal_conv1d_fn, causal_conv1d_update = None, None\n",
        "try: from causal_conv1d.causal_conv1d_varlen import causal_conv1d_varlen_states\n",
        "except ImportError: causal_conv1d_varlen_states = None\n",
        "try: from mamba_ssm.ops.triton.selective_state_update import selective_state_update\n",
        "except ImportError: selective_state_update = None\n",
        "from mamba_ssm.distributed.tensor_parallel import ColumnParallelLinear, RowParallelLinear\n",
        "from mamba_ssm.distributed.distributed_utils import all_reduce, reduce_scatter\n",
        "from mamba_ssm.ops.triton.ssd_combined import mamba_chunk_scan_combined\n",
        "from mamba_ssm.ops.triton.ssd_combined import mamba_split_conv1d_scan_combined\n",
        "\n",
        "class Mamba2(nn.Module):\n",
        "    def __init__(self, d_model, d_state=128, d_conv=4, expand=2, headdim=64, ngroups=1,\n",
        "        d_ssm=None,  # If not None, we only apply SSM on this many dimensions, the rest uses gated MLP\n",
        "        conv_init=None,\n",
        "        A_init_range=(1, 16),\n",
        "        D_has_hdim=False,\n",
        "        norm_before_gate=False,\n",
        "        dt_min=0.001, dt_max=0.1, dt_init_floor=1e-4, dt_limit=(0.0, float(\"inf\")),\n",
        "        bias=False,\n",
        "        conv_bias=True,\n",
        "        # Fused kernel and sharding options\n",
        "        layer_idx=None,  # Absorb kwarg for general module\n",
        "        sequence_parallel=True, device=None, dtype=None):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_state = d_state\n",
        "        self.d_conv = d_conv\n",
        "        self.conv_init = conv_init\n",
        "        self.expand = expand\n",
        "        self.sequence_parallel = sequence_parallel\n",
        "        self.d_inner = self.expand * self.d_model\n",
        "        assert self.d_inner == self.expand * self.d_model\n",
        "        self.headdim = headdim\n",
        "        self.d_ssm = d_ssm or self.d_inner\n",
        "        self.ngroups = ngroups\n",
        "        assert self.d_ssm % self.headdim == 0\n",
        "        self.nheads = self.d_ssm // self.headdim\n",
        "        self.D_has_hdim = D_has_hdim\n",
        "        self.norm_before_gate = norm_before_gate\n",
        "        self.dt_limit = dt_limit\n",
        "        self.activation = \"silu\"\n",
        "        self.layer_idx = layer_idx\n",
        "\n",
        "        # Order: [z, x, B, C, dt]\n",
        "        d_in_proj = 2 * self.d_inner + 2 * self.ngroups * self.d_state + self.nheads\n",
        "        self.in_proj = nn.Linear(self.d_model, d_in_proj, bias=bias)\n",
        "\n",
        "        conv_dim = self.d_ssm + 2 * self.ngroups * self.d_state\n",
        "        self.conv1d = nn.Conv1d(in_channels=conv_dim, out_channels=conv_dim, bias=conv_bias, kernel_size=d_conv, groups=conv_dim, padding=d_conv-1)\n",
        "        if self.conv_init is not None:\n",
        "            nn.init.uniform_(self.conv1d.weight, -self.conv_init, self.conv_init)\n",
        "\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "        # Initialize log dt bias\n",
        "        dt = torch.exp(torch.rand(self.nheads) * (math.log(dt_max) - math.log(dt_min)) + math.log(dt_min))\n",
        "        dt = torch.clamp(dt, min=dt_init_floor)\n",
        "        # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n",
        "        inv_dt = dt + torch.log(-torch.expm1(-dt))\n",
        "        self.dt_bias = nn.Parameter(inv_dt)\n",
        "        # Just to be explicit. Without this we already don't put wd on dt_bias because of the check\n",
        "        # name.endswith(\"bias\") in param_grouping.py\n",
        "        self.dt_bias._no_weight_decay = True\n",
        "\n",
        "        assert A_init_range[0] > 0 and A_init_range[1] >= A_init_range[0]\n",
        "        A = torch.empty(self.nheads, dtype=torch.float32, device=device).uniform_(*A_init_range)\n",
        "        A_log = torch.log(A).to(dtype=dtype)\n",
        "        self.A_log = nn.Parameter(A_log)\n",
        "        self.A_log._no_weight_decay = True\n",
        "\n",
        "        # D \"skip\" parameter\n",
        "        self.D = nn.Parameter(torch.ones(self.d_ssm if self.D_has_hdim else self.nheads, device=device))\n",
        "        self.D._no_weight_decay = True\n",
        "\n",
        "        self.norm = RMSNormGated(self.d_ssm, eps=1e-5, norm_before_gate=self.norm_before_gate, group_size=self.d_ssm//ngroups)\n",
        "        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=bias)\n",
        "\n",
        "    def forward(self, u, seq_idx=None, cu_seqlens=None, inference_params=None):\n",
        "        batch, seqlen, dim = u.shape\n",
        "        conv_state, ssm_state = None, None\n",
        "        if inference_params is not None:\n",
        "            inference_batch = cu_seqlens.shape[0] - 1 if cu_seqlens is not None else batch\n",
        "            conv_state, ssm_state = self._get_states_from_cache(inference_params, inference_batch)\n",
        "            if inference_params.seqlen_offset > 0:\n",
        "                # The states are updated inplace\n",
        "                out, _, _ = self.step(u, conv_state, ssm_state)\n",
        "                return out\n",
        "\n",
        "        zxbcdt = self.in_proj(u)  # (B, L, d_in_proj) or (B * L, d_in_proj)\n",
        "        # If the model is loaded in fp16, without the .float() here, A might be -inf\n",
        "        A = -torch.exp(self.A_log.float())  # (nheads) or (d_inner, d_state)\n",
        "        dt_limit_kwargs = {} if self.dt_limit == (0.0, float(\"inf\")) else dict(dt_limit=self.dt_limit)\n",
        "\n",
        "        if self.use_mem_eff_path and inference_params is None: # T,\n",
        "            out = mamba_split_conv1d_scan_combined(zxbcdt,\n",
        "                self.conv1d.weight.unsqueeze(1), self.conv1d.bias, self.dt_bias,\n",
        "                A,\n",
        "                D=rearrange(self.D, \"(h p) -> h p\", p=self.headdim) if self.D_has_hdim else self.D,\n",
        "                chunk_size=256,\n",
        "                seq_idx=seq_idx,\n",
        "                activation=self.activation,\n",
        "                rmsnorm_weight=self.norm.weight\n",
        "                rmsnorm_eps=self.norm.eps\n",
        "                outproj_weight=self.out_proj.weight,\n",
        "                outproj_bias=self.out_proj.bias,\n",
        "                headdim=None if self.D_has_hdim else self.headdim,\n",
        "                ngroups=self.ngroups,\n",
        "                norm_before_gate=self.norm_before_gate,\n",
        "                **dt_limit_kwargs,\n",
        "            )\n",
        "        else:\n",
        "            d_mlp = (zxbcdt.shape[-1] - 2* self.d_ssm - 2* self.ngroups*self.d_state - self.nheads) // 2\n",
        "            z0, x0, z, xBC, dt = torch.split(zxbcdt, [d_mlp, d_mlp, self.d_ssm, self.d_ssm + 2 * self.ngroups * self.d_state, self.nheads], dim=-1)\n",
        "            if conv_state is not None:\n",
        "                if cu_seqlens is None:\n",
        "                    # If we just take xBC[:, :, -self.d_conv :], it will error if seqlen < self.d_conv\n",
        "                    # Instead F.pad will pad with zeros if seqlen < self.d_conv, and truncate otherwise.\n",
        "                    xBC_t = rearrange(xBC, \"b l d -> b d l\")\n",
        "                    conv_state.copy_(F.pad(xBC_t, (self.d_conv - xBC_t.shape[-1], 0)))  # Update state (B D W)\n",
        "                else:\n",
        "                    assert causal_conv1d_varlen_states is not None, \"varlen inference requires causal_conv1d package\"\n",
        "                    assert batch == 1, \"varlen inference only supports batch dimension 1\"\n",
        "                    conv_varlen_states = causal_conv1d_varlen_states(xBC.squeeze(0), cu_seqlens, state_len=conv_state.shape[-1])\n",
        "                    conv_state.copy_(conv_varlen_states)\n",
        "            assert self.activation in [\"silu\", \"swish\"]\n",
        "            if causal_conv1d_fn is None or self.activation not in [\"silu\", \"swish\"]:\n",
        "                assert seq_idx is None, \"varlen conv1d requires the causal_conv1d package\"\n",
        "                xBC = self.act(self.conv1d(xBC.transpose(1,2)).transpose(1, 2)[:, :-(self.d_conv-1)])  # (B, L, self.d_ssm + 2 * ngroups * d_state)\n",
        "            else:\n",
        "                xBC = causal_conv1d_fn(xBC.transpose(1, 2), rearrange(self.conv1d.weight, \"d 1 w -> d w\"), bias=self.conv1d.bias, activation=self.activation, seq_idx=seq_idx).transpose(1, 2)\n",
        "\n",
        "            x, B, C = torch.split(xBC, [self.d_ssm, self.ngroups*self.d_state, self.ngroups*self.d_state], dim=-1)\n",
        "\n",
        "            y = mamba_chunk_scan_combined(\n",
        "                x.unflatten(-1, (-1,self.headdim)), dt, A,\n",
        "                B.unflatten(-1, (self.ngroups,-1)), C.unflatten(-1, (self.ngroups,-1)),\n",
        "                chunk_size=256,\n",
        "                D=rearrange(self.D, \"(h p) -> h p\", p=self.headdim) if self.D_has_hdim else self.D,\n",
        "                z=rearrange(z, \"b l (h p) -> b l h p\", p=self.headdim) if not self.rmsnorm else None,\n",
        "                dt_bias=self.dt_bias,\n",
        "                dt_softplus=True,\n",
        "                seq_idx=seq_idx,\n",
        "                cu_seqlens=cu_seqlens,\n",
        "                **dt_limit_kwargs,\n",
        "                return_final_states=ssm_state is not None,\n",
        "                return_varlen_states=cu_seqlens is not None and inference_params is not None,\n",
        "            )\n",
        "\n",
        "            if ssm_state is not None:\n",
        "                y, last_state, *rest = y\n",
        "                if cu_seqlens is None:\n",
        "                    ssm_state.copy_(last_state)\n",
        "                else:\n",
        "                    varlen_states = rest[0]\n",
        "                    ssm_state.copy_(varlen_states)\n",
        "\n",
        "            y = y.flatten(-2) # [b,t,d]\n",
        "            y = self.norm(y, z)\n",
        "            if d_mlp > 0:\n",
        "                y = torch.cat([F.silu(z0) * x0, y], dim=-1)\n",
        "            # if seqlen_og is not None:\n",
        "            #     y = rearrange(y, \"b l d -> (b l) d\")\n",
        "            out = self.out_proj(y)\n",
        "        return out\n",
        "\n",
        "    def step(self, hidden_states, conv_state, ssm_state):\n",
        "        dtype = hidden_states.dtype\n",
        "        assert hidden_states.shape[1] == 1, \"Only support decoding with 1 token at a time for now\"\n",
        "        zxbcdt = self.in_proj(hidden_states.squeeze(1))  # (B 2D)\n",
        "        d_mlp = (zxbcdt.shape[-1] - 2 * self.d_ssm - 2 * self.ngroups * self.d_state - self.nheads) // 2\n",
        "        z0, x0, z, xBC, dt = torch.split(zxbcdt, [d_mlp, d_mlp, self.d_ssm, self.d_ssm + 2* self.ngroups*self.d_state, self.nheads], dim=-1)\n",
        "\n",
        "        # Conv step\n",
        "        if causal_conv1d_update is None:\n",
        "            conv_state.copy_(torch.roll(conv_state, shifts=-1, dims=-1))  # Update state (B D W)\n",
        "            conv_state[:, :, -1] = xBC\n",
        "            xBC = torch.sum(conv_state * rearrange(self.conv1d.weight, \"d 1 w -> d w\"), dim=-1)  # (B D)\n",
        "            if self.conv1d.bias is not None:\n",
        "                xBC = xBC + self.conv1d.bias\n",
        "            xBC = self.act(xBC).to(dtype=dtype)\n",
        "        else:\n",
        "            xBC = causal_conv1d_update(xBC, conv_state, rearrange(self.conv1d.weight, \"d 1 w -> d w\"), self.conv1d.bias, self.activation)\n",
        "\n",
        "        x, B, C = torch.split(xBC, [self.d_ssm, self.ngroups * self.d_state, self.ngroups * self.d_state], dim=-1)\n",
        "        A = -torch.exp(self.A_log.float())  # (nheads,)\n",
        "\n",
        "        # SSM step\n",
        "        if selective_state_update is None:\n",
        "            assert self.ngroups == 1, \"Only support ngroups=1 for this inference code path\"\n",
        "            # Discretize A and B\n",
        "            dt = F.softplus(dt + self.dt_bias.to(dtype=dt.dtype))  # (batch, nheads)\n",
        "            dA = torch.exp(dt * A)  # (batch, nheads)\n",
        "            x = rearrange(x, \"b (h p) -> b h p\", p=self.headdim)\n",
        "            dBx = torch.einsum(\"bh,bn,bhp->bhpn\", dt, B, x)\n",
        "            ssm_state.copy_(ssm_state * rearrange(dA, \"b h -> b h 1 1\") + dBx)\n",
        "            y = torch.einsum(\"bhpn,bn->bhp\", ssm_state.to(dtype), C)\n",
        "            y = y + rearrange(self.D.to(dtype), \"h -> h 1\") * x\n",
        "            y = rearrange(y, \"b h p -> b (h p)\")\n",
        "            # if not self.rmsnorm:\n",
        "            #     y = y * self.act(z)  # (B D)\n",
        "        else:\n",
        "            A = repeat(A, \"h -> h p n\", p=self.headdim, n=self.d_state).to(dtype=torch.float32)\n",
        "            dt = repeat(dt, \"b h -> b h p\", p=self.headdim)\n",
        "            dt_bias = repeat(self.dt_bias, \"h -> h p\", p=self.headdim)\n",
        "            D = repeat(self.D, \"h -> h p\", p=self.headdim)\n",
        "            B = rearrange(B, \"b (g n) -> b g n\", g=self.ngroups)\n",
        "            C = rearrange(C, \"b (g n) -> b g n\", g=self.ngroups)\n",
        "            x_reshaped = rearrange(x, \"b (h p) -> b h p\", p=self.headdim)\n",
        "            # if not self.rmsnorm:\n",
        "            #     z = rearrange(z, \"b (h p) -> b h p\", p=self.headdim)\n",
        "            y = selective_state_update(\n",
        "                ssm_state, x_reshaped, dt, A, B, C, D, z=z if not self.rmsnorm else None,\n",
        "                dt_bias=dt_bias, dt_softplus=True\n",
        "            )\n",
        "            y = rearrange(y, \"b h p -> b (h p)\")\n",
        "\n",
        "        y = self.norm(y, z)\n",
        "        if d_mlp > 0:\n",
        "            y = torch.cat([F.silu(z0) * x0, y], dim=-1)\n",
        "        out = self.out_proj(y)\n",
        "        return out.unsqueeze(1), conv_state, ssm_state\n",
        "\n",
        "    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):\n",
        "        device = self.out_proj.weight.device\n",
        "        conv_dtype = self.conv1d.weight.dtype if dtype is None else dtype\n",
        "        conv_state = torch.zeros(batch_size, self.d_conv, self.conv1d.weight.shape[0], device=device, dtype=conv_dtype).transpose(1, 2)\n",
        "        ssm_dtype = self.in_proj.weight.dtype if dtype is None else dtype\n",
        "        ssm_state = torch.zeros(batch_size, self.nheads, self.headdim, self.d_state, device=device, dtype=ssm_dtype)\n",
        "        return conv_state, ssm_state\n",
        "\n",
        "    def _get_states_from_cache(self, inference_params, batch_size, initialize_states=False):\n",
        "        assert self.layer_idx is not None\n",
        "        if self.layer_idx not in inference_params.key_value_memory_dict:\n",
        "            batch_shape = (batch_size,)\n",
        "            conv_state = torch.zeros(batch_size, self.d_conv, self.conv1d.weight.shape[0],\n",
        "                device=self.conv1d.weight.device, dtype=self.conv1d.weight.dtype).transpose(1, 2)\n",
        "            ssm_state = torch.zeros(batch_size, self.nheads, self.headdim, self.d_state,\n",
        "                device=self.in_proj.weight.device, dtype=self.in_proj.weight.dtype)\n",
        "            inference_params.key_value_memory_dict[self.layer_idx] = (conv_state, ssm_state)\n",
        "        else:\n",
        "            conv_state, ssm_state = inference_params.key_value_memory_dict[self.layer_idx]\n",
        "            # TODO: What if batch size changes between generation, and we reuse the same states?\n",
        "            if initialize_states:\n",
        "                conv_state.zero_()\n",
        "                ssm_state.zero_()\n",
        "        return conv_state, ssm_state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tI7vvGRmtOzK"
      },
      "outputs": [],
      "source": [
        "# @title mamba_ssm/models/block.py\n",
        "# https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/block.py\n",
        "from typing import Optional\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from mamba_ssm.ops.triton.layer_norm import RMSNorm, layer_norm_fn\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(\n",
        "        self, dim, mixer_cls, mlp_cls, norm_cls=nn.LayerNorm, fused_add_norm=False, residual_in_fp32=False\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Simple block wrapping a mixer class with LayerNorm/RMSNorm and residual connection\"\n",
        "\n",
        "        This Block has a slightly different structure compared to a regular\n",
        "        prenorm Transformer block.\n",
        "        The standard block is: LN -> MHA/MLP -> Add.\n",
        "        [Ref: https://arxiv.org/abs/2002.04745]\n",
        "        Here we have: Add -> LN -> Mixer, returning both\n",
        "        the hidden_states (output of the mixer) and the residual.\n",
        "        This is purely for performance reasons, as we can fuse add and LayerNorm.\n",
        "        The residual needs to be provided (except for the very first block).\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.residual_in_fp32 = residual_in_fp32\n",
        "        self.fused_add_norm = fused_add_norm\n",
        "        self.norm = norm_cls(dim)\n",
        "        self.mixer = mixer_cls(dim)\n",
        "        if mlp_cls is not nn.Identity:\n",
        "            self.norm2 = norm_cls(dim)\n",
        "            self.mlp = mlp_cls(dim)\n",
        "        else:\n",
        "            self.mlp = None\n",
        "        if self.fused_add_norm:\n",
        "            assert RMSNorm is not None, \"RMSNorm import fails\"\n",
        "            assert isinstance(\n",
        "                self.norm, (nn.LayerNorm, RMSNorm)\n",
        "            ), \"Only LayerNorm and RMSNorm are supported for fused_add_norm\"\n",
        "\n",
        "    def forward(self, hidden_states: Tensor, residual: Optional[Tensor] = None, inference_params=None, **mixer_kwargs):\n",
        "        r\"\"\"Pass the input through the encoder layer.\n",
        "\n",
        "        Args:\n",
        "            hidden_states: the sequence to the encoder layer (required).\n",
        "            residual: hidden_states = Mixer(LN(residual))\n",
        "        \"\"\"\n",
        "        if not self.fused_add_norm:\n",
        "            residual = (hidden_states + residual) if residual is not None else hidden_states\n",
        "            hidden_states = self.norm(residual.to(dtype=self.norm.weight.dtype))\n",
        "            if self.residual_in_fp32:\n",
        "                residual = residual.to(torch.float32)\n",
        "        else:\n",
        "            hidden_states, residual = layer_norm_fn(\n",
        "                hidden_states,\n",
        "                self.norm.weight,\n",
        "                self.norm.bias,\n",
        "                residual=residual,\n",
        "                prenorm=True,\n",
        "                residual_in_fp32=self.residual_in_fp32,\n",
        "                eps=self.norm.eps,\n",
        "                is_rms_norm=isinstance(self.norm, RMSNorm)\n",
        "            )\n",
        "        hidden_states = self.mixer(hidden_states, inference_params=inference_params, **mixer_kwargs)\n",
        "\n",
        "        if self.mlp is not None:\n",
        "            if not self.fused_add_norm:\n",
        "                residual = hidden_states + residual\n",
        "                hidden_states = self.norm2(residual.to(dtype=self.norm2.weight.dtype))\n",
        "                if self.residual_in_fp32:\n",
        "                    residual = residual.to(torch.float32)\n",
        "            else:\n",
        "                hidden_states, residual = layer_norm_fn(\n",
        "                    hidden_states,\n",
        "                    self.norm2.weight,\n",
        "                    self.norm2.bias,\n",
        "                    residual=residual,\n",
        "                    prenorm=True,\n",
        "                    residual_in_fp32=self.residual_in_fp32,\n",
        "                    eps=self.norm2.eps,\n",
        "                    is_rms_norm=isinstance(self.norm2, RMSNorm)\n",
        "                )\n",
        "            hidden_states = self.mlp(hidden_states)\n",
        "\n",
        "        return hidden_states, residual\n",
        "\n",
        "    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):\n",
        "        return self.mixer.allocate_inference_cache(batch_size, max_seqlen, dtype=dtype, **kwargs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uUOMEha1SOZC"
      },
      "outputs": [],
      "source": [
        "# @title mamba_ssm/models/mamba2.py base\n",
        "# https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba2.py\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange, repeat\n",
        "try: from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\n",
        "except ImportError: causal_conv1d_fn, causal_conv1d_update = None, None\n",
        "try: from causal_conv1d.causal_conv1d_varlen import causal_conv1d_varlen_states\n",
        "except ImportError: causal_conv1d_varlen_states = None\n",
        "try: from mamba_ssm.ops.triton.selective_state_update import selective_state_update\n",
        "except ImportError: selective_state_update = None\n",
        "from mamba_ssm.ops.triton.layernorm_gated import RMSNorm as RMSNormGated\n",
        "from mamba_ssm.distributed.tensor_parallel import ColumnParallelLinear, RowParallelLinear\n",
        "from mamba_ssm.distributed.distributed_utils import all_reduce, reduce_scatter\n",
        "from mamba_ssm.ops.triton.ssd_combined import mamba_chunk_scan_combined\n",
        "from mamba_ssm.ops.triton.ssd_combined import mamba_split_conv1d_scan_combined\n",
        "from huggingface_hub import PyTorchModelHubMixin\n",
        "\n",
        "class Mamba2(nn.Module, PyTorchModelHubMixin):\n",
        "    def __init__(self, d_model, d_state=128, d_conv=4, expand=2, headdim=64, ngroups=1,\n",
        "        d_ssm=None,  # If not None, we only apply SSM on this many dimensions, the rest uses gated MLP\n",
        "        conv_init=None,\n",
        "        A_init_range=(1, 16),\n",
        "        D_has_hdim=False,\n",
        "        rmsnorm=True,\n",
        "        norm_before_gate=False,\n",
        "        dt_min=0.001, dt_max=0.1, dt_init_floor=1e-4, dt_limit=(0.0, float(\"inf\")),\n",
        "        bias=False,\n",
        "        conv_bias=True,\n",
        "        # Fused kernel and sharding options\n",
        "        chunk_size=256,\n",
        "        use_mem_eff_path=True,\n",
        "        layer_idx=None,  # Absorb kwarg for general module\n",
        "        process_group=None, sequence_parallel=True, device=None, dtype=None):\n",
        "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_state = d_state\n",
        "        self.d_conv = d_conv\n",
        "        self.conv_init = conv_init\n",
        "        self.expand = expand\n",
        "        self.process_group = process_group\n",
        "        self.sequence_parallel = sequence_parallel\n",
        "        self.world_size = 1 if process_group is None else process_group.size()\n",
        "        self.local_rank = 0 if process_group is None else process_group.rank()\n",
        "        self.d_inner = (self.expand * self.d_model) // self.world_size\n",
        "        assert self.d_inner * self.world_size == self.expand * self.d_model\n",
        "        self.headdim = headdim\n",
        "        self.d_ssm = self.d_inner if d_ssm is None else d_ssm // self.world_size\n",
        "        assert ngroups % self.world_size == 0\n",
        "        self.ngroups = ngroups // self.world_size\n",
        "        assert self.d_ssm % self.headdim == 0\n",
        "        self.nheads = self.d_ssm // self.headdim\n",
        "        self.D_has_hdim = D_has_hdim\n",
        "        self.rmsnorm = rmsnorm\n",
        "        self.norm_before_gate = norm_before_gate\n",
        "        self.dt_limit = dt_limit\n",
        "        self.activation = \"silu\"\n",
        "        self.chunk_size = chunk_size\n",
        "        self.use_mem_eff_path = use_mem_eff_path\n",
        "        self.layer_idx = layer_idx\n",
        "\n",
        "        # Order: [z, x, B, C, dt]\n",
        "        d_in_proj = 2 * self.d_inner + 2 * self.ngroups * self.d_state + self.nheads\n",
        "        if self.process_group is None:\n",
        "            self.in_proj = nn.Linear(self.d_model, d_in_proj, bias=bias, **factory_kwargs)\n",
        "        else:\n",
        "            self.in_proj = ColumnParallelLinear(self.d_model, d_in_proj * self.world_size, bias=bias, process_group=self.process_group, sequence_parallel=self.sequence_parallel, **factory_kwargs)\n",
        "\n",
        "        conv_dim = self.d_ssm + 2 * self.ngroups * self.d_state\n",
        "        self.conv1d = nn.Conv1d(in_channels=conv_dim, out_channels=conv_dim, bias=conv_bias, kernel_size=d_conv, groups=conv_dim, padding=d_conv-1, **factory_kwargs)\n",
        "        if self.conv_init is not None:\n",
        "            nn.init.uniform_(self.conv1d.weight, -self.conv_init, self.conv_init)\n",
        "\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "        # Initialize log dt bias\n",
        "        dt = torch.exp(torch.rand(self.nheads, **factory_kwargs) * (math.log(dt_max) - math.log(dt_min)) + math.log(dt_min))\n",
        "        dt = torch.clamp(dt, min=dt_init_floor)\n",
        "        # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n",
        "        inv_dt = dt + torch.log(-torch.expm1(-dt))\n",
        "        self.dt_bias = nn.Parameter(inv_dt)\n",
        "        # Just to be explicit. Without this we already don't put wd on dt_bias because of the check\n",
        "        # name.endswith(\"bias\") in param_grouping.py\n",
        "        self.dt_bias._no_weight_decay = True\n",
        "\n",
        "        assert A_init_range[0] > 0 and A_init_range[1] >= A_init_range[0]\n",
        "        A = torch.empty(self.nheads, dtype=torch.float32, device=device).uniform_(*A_init_range)\n",
        "        A_log = torch.log(A).to(dtype=dtype)\n",
        "        self.A_log = nn.Parameter(A_log)\n",
        "        self.A_log._no_weight_decay = True\n",
        "\n",
        "        # D \"skip\" parameter\n",
        "        self.D = nn.Parameter(torch.ones(self.d_ssm if self.D_has_hdim else self.nheads, device=device))\n",
        "        self.D._no_weight_decay = True\n",
        "\n",
        "        if self.rmsnorm:\n",
        "            assert RMSNormGated is not None\n",
        "            self.norm = RMSNormGated(self.d_ssm, eps=1e-5, norm_before_gate=self.norm_before_gate, group_size=self.d_ssm // ngroups, **factory_kwargs)\n",
        "\n",
        "        if self.process_group is None:\n",
        "            self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=bias, **factory_kwargs)\n",
        "        else:\n",
        "            self.out_proj = RowParallelLinear(self.d_inner * self.world_size, self.d_model, bias=bias, process_group=self.process_group, sequence_parallel=self.sequence_parallel, **factory_kwargs)\n",
        "\n",
        "    def forward(self, u, seqlen=None, seq_idx=None, cu_seqlens=None, inference_params=None):\n",
        "        \"\"\"\n",
        "        u: (batch, seqlen, hidden_dim) if seqlen=None.\n",
        "            If seqlen is not None, u is (batch * seqlen, hidden_dim). This is so that when we\n",
        "            split u during sequence parallel, we split the batch * seqlen dimension\n",
        "            (in case batch is small).\n",
        "        Returns: same shape as u\n",
        "        \"\"\"\n",
        "        seqlen_og = seqlen\n",
        "        if seqlen is None:\n",
        "            batch, seqlen, dim = u.shape\n",
        "        else:\n",
        "            batch_seqlen, dim = u.shape\n",
        "            batch = batch_seqlen // seqlen\n",
        "\n",
        "        conv_state, ssm_state = None, None\n",
        "        if inference_params is not None:\n",
        "            inference_batch = cu_seqlens.shape[0] - 1 if cu_seqlens is not None else batch\n",
        "            conv_state, ssm_state = self._get_states_from_cache(inference_params, inference_batch)\n",
        "            if inference_params.seqlen_offset > 0:\n",
        "                # The states are updated inplace\n",
        "                out, _, _ = self.step(u, conv_state, ssm_state)\n",
        "                return out\n",
        "\n",
        "        zxbcdt = self.in_proj(u)  # (B, L, d_in_proj) or (B * L, d_in_proj)\n",
        "        if seqlen_og is not None:\n",
        "            zxbcdt = rearrange(zxbcdt, \"(b l) d -> b l d\", l=seqlen)\n",
        "        # If the model is loaded in fp16, without the .float() here, A might be -inf\n",
        "        A = -torch.exp(self.A_log.float())  # (nheads) or (d_inner, d_state)\n",
        "        dt_limit_kwargs = {} if self.dt_limit == (0.0, float(\"inf\")) else dict(dt_limit=self.dt_limit)\n",
        "        if self.use_mem_eff_path and inference_params is None:\n",
        "            out = mamba_split_conv1d_scan_combined(\n",
        "                zxbcdt,\n",
        "                rearrange(self.conv1d.weight, \"d 1 w -> d w\"),\n",
        "                self.conv1d.bias,\n",
        "                self.dt_bias,\n",
        "                A,\n",
        "                D=rearrange(self.D, \"(h p) -> h p\", p=self.headdim) if self.D_has_hdim else self.D,\n",
        "                chunk_size=self.chunk_size,\n",
        "                seq_idx=seq_idx,\n",
        "                activation=self.activation,\n",
        "                rmsnorm_weight=self.norm.weight if self.rmsnorm else None,\n",
        "                rmsnorm_eps=self.norm.eps if self.rmsnorm else 1e-6,\n",
        "                outproj_weight=self.out_proj.weight,\n",
        "                outproj_bias=self.out_proj.bias,\n",
        "                headdim=None if self.D_has_hdim else self.headdim,\n",
        "                ngroups=self.ngroups,\n",
        "                norm_before_gate=self.norm_before_gate,\n",
        "                **dt_limit_kwargs,\n",
        "            )\n",
        "            if seqlen_og is not None:\n",
        "                out = rearrange(out, \"b l d -> (b l) d\")\n",
        "            if self.process_group is not None:\n",
        "                reduce_fn = reduce_scatter if self.sequence_parallel else all_reduce\n",
        "                out = reduce_fn(out, self.process_group)\n",
        "        else:\n",
        "            d_mlp = (zxbcdt.shape[-1] - 2 * self.d_ssm - 2 * self.ngroups * self.d_state - self.nheads) // 2\n",
        "            z0, x0, z, xBC, dt = torch.split(\n",
        "                zxbcdt,\n",
        "                [d_mlp, d_mlp, self.d_ssm, self.d_ssm + 2 * self.ngroups * self.d_state, self.nheads],\n",
        "                dim=-1\n",
        "            )\n",
        "            if conv_state is not None:\n",
        "                if cu_seqlens is None:\n",
        "                    # If we just take xBC[:, :, -self.d_conv :], it will error if seqlen < self.d_conv\n",
        "                    # Instead F.pad will pad with zeros if seqlen < self.d_conv, and truncate otherwise.\n",
        "                    xBC_t = rearrange(xBC, \"b l d -> b d l\")\n",
        "                    conv_state.copy_(F.pad(xBC_t, (self.d_conv - xBC_t.shape[-1], 0)))  # Update state (B D W)\n",
        "                else:\n",
        "                    assert causal_conv1d_varlen_states is not None, \"varlen inference requires causal_conv1d package\"\n",
        "                    assert batch == 1, \"varlen inference only supports batch dimension 1\"\n",
        "                    conv_varlen_states = causal_conv1d_varlen_states(\n",
        "                        xBC.squeeze(0), cu_seqlens, state_len=conv_state.shape[-1]\n",
        "                    )\n",
        "                    conv_state.copy_(conv_varlen_states)\n",
        "            assert self.activation in [\"silu\", \"swish\"]\n",
        "            if causal_conv1d_fn is None or self.activation not in [\"silu\", \"swish\"]:\n",
        "                assert seq_idx is None, \"varlen conv1d requires the causal_conv1d package\"\n",
        "                xBC = self.act(\n",
        "                    self.conv1d(xBC.transpose(1, 2)).transpose(1, 2)[:, :-(self.d_conv - 1)]\n",
        "                )  # (B, L, self.d_ssm + 2 * ngroups * d_state)\n",
        "            else:\n",
        "                xBC = causal_conv1d_fn(\n",
        "                    xBC.transpose(1, 2),\n",
        "                    rearrange(self.conv1d.weight, \"d 1 w -> d w\"),\n",
        "                    bias=self.conv1d.bias,\n",
        "                    activation=self.activation,\n",
        "                    seq_idx=seq_idx,\n",
        "                ).transpose(1, 2)\n",
        "            x, B, C = torch.split(xBC, [self.d_ssm, self.ngroups * self.d_state, self.ngroups * self.d_state], dim=-1)\n",
        "            y = mamba_chunk_scan_combined(\n",
        "                rearrange(x, \"b l (h p) -> b l h p\", p=self.headdim),\n",
        "                dt,\n",
        "                A,\n",
        "                rearrange(B, \"b l (g n) -> b l g n\", g=self.ngroups),\n",
        "                rearrange(C, \"b l (g n) -> b l g n\", g=self.ngroups),\n",
        "                chunk_size=self.chunk_size,\n",
        "                D=rearrange(self.D, \"(h p) -> h p\", p=self.headdim) if self.D_has_hdim else self.D,\n",
        "                z=rearrange(z, \"b l (h p) -> b l h p\", p=self.headdim) if not self.rmsnorm else None,\n",
        "                dt_bias=self.dt_bias,\n",
        "                dt_softplus=True,\n",
        "                seq_idx=seq_idx,\n",
        "                cu_seqlens=cu_seqlens,\n",
        "                **dt_limit_kwargs,\n",
        "                return_final_states=ssm_state is not None,\n",
        "                return_varlen_states=cu_seqlens is not None and inference_params is not None,\n",
        "            )\n",
        "            if ssm_state is not None:\n",
        "                y, last_state, *rest = y\n",
        "                if cu_seqlens is None:\n",
        "                    ssm_state.copy_(last_state)\n",
        "                else:\n",
        "                    varlen_states = rest[0]\n",
        "                    ssm_state.copy_(varlen_states)\n",
        "            y = rearrange(y, \"b l h p -> b l (h p)\")\n",
        "            if self.rmsnorm:\n",
        "                y = self.norm(y, z)\n",
        "            if d_mlp > 0:\n",
        "                y = torch.cat([F.silu(z0) * x0, y], dim=-1)\n",
        "            if seqlen_og is not None:\n",
        "                y = rearrange(y, \"b l d -> (b l) d\")\n",
        "            out = self.out_proj(y)\n",
        "        return out\n",
        "\n",
        "    def step(self, hidden_states, conv_state, ssm_state):\n",
        "        dtype = hidden_states.dtype\n",
        "        assert hidden_states.shape[1] == 1, \"Only support decoding with 1 token at a time for now\"\n",
        "        zxbcdt = self.in_proj(hidden_states.squeeze(1))  # (B 2D)\n",
        "        d_mlp = (zxbcdt.shape[-1] - 2 * self.d_ssm - 2 * self.ngroups * self.d_state - self.nheads) // 2\n",
        "        z0, x0, z, xBC, dt = torch.split(zxbcdt, [d_mlp, d_mlp, self.d_ssm, self.d_ssm + 2* self.ngroups*self.d_state, self.nheads], dim=-1)\n",
        "\n",
        "        # Conv step\n",
        "        if causal_conv1d_update is None:\n",
        "            conv_state.copy_(torch.roll(conv_state, shifts=-1, dims=-1))  # Update state (B D W)\n",
        "            conv_state[:, :, -1] = xBC\n",
        "            xBC = torch.sum(conv_state * rearrange(self.conv1d.weight, \"d 1 w -> d w\"), dim=-1)  # (B D)\n",
        "            if self.conv1d.bias is not None:\n",
        "                xBC = xBC + self.conv1d.bias\n",
        "            xBC = self.act(xBC).to(dtype=dtype)\n",
        "        else:\n",
        "            xBC = causal_conv1d_update(xBC, conv_state, rearrange(self.conv1d.weight, \"d 1 w -> d w\"), self.conv1d.bias, self.activation)\n",
        "\n",
        "        x, B, C = torch.split(xBC, [self.d_ssm, self.ngroups * self.d_state, self.ngroups * self.d_state], dim=-1)\n",
        "        A = -torch.exp(self.A_log.float())  # (nheads,)\n",
        "\n",
        "        # SSM step\n",
        "        if selective_state_update is None:\n",
        "            assert self.ngroups == 1, \"Only support ngroups=1 for this inference code path\"\n",
        "            # Discretize A and B\n",
        "            dt = F.softplus(dt + self.dt_bias.to(dtype=dt.dtype))  # (batch, nheads)\n",
        "            dA = torch.exp(dt * A)  # (batch, nheads)\n",
        "            x = rearrange(x, \"b (h p) -> b h p\", p=self.headdim)\n",
        "            dBx = torch.einsum(\"bh,bn,bhp->bhpn\", dt, B, x)\n",
        "            ssm_state.copy_(ssm_state * rearrange(dA, \"b h -> b h 1 1\") + dBx)\n",
        "            y = torch.einsum(\"bhpn,bn->bhp\", ssm_state.to(dtype), C)\n",
        "            y = y + rearrange(self.D.to(dtype), \"h -> h 1\") * x\n",
        "            y = rearrange(y, \"b h p -> b (h p)\")\n",
        "            if not self.rmsnorm:\n",
        "                y = y * self.act(z)  # (B D)\n",
        "        else:\n",
        "            A = repeat(A, \"h -> h p n\", p=self.headdim, n=self.d_state).to(dtype=torch.float32)\n",
        "            dt = repeat(dt, \"b h -> b h p\", p=self.headdim)\n",
        "            dt_bias = repeat(self.dt_bias, \"h -> h p\", p=self.headdim)\n",
        "            D = repeat(self.D, \"h -> h p\", p=self.headdim)\n",
        "            B = rearrange(B, \"b (g n) -> b g n\", g=self.ngroups)\n",
        "            C = rearrange(C, \"b (g n) -> b g n\", g=self.ngroups)\n",
        "            x_reshaped = rearrange(x, \"b (h p) -> b h p\", p=self.headdim)\n",
        "            if not self.rmsnorm:\n",
        "                z = rearrange(z, \"b (h p) -> b h p\", p=self.headdim)\n",
        "            y = selective_state_update(\n",
        "                ssm_state, x_reshaped, dt, A, B, C, D, z=z if not self.rmsnorm else None,\n",
        "                dt_bias=dt_bias, dt_softplus=True\n",
        "            )\n",
        "            y = rearrange(y, \"b h p -> b (h p)\")\n",
        "        if self.rmsnorm:\n",
        "            y = self.norm(y, z)\n",
        "        if d_mlp > 0:\n",
        "            y = torch.cat([F.silu(z0) * x0, y], dim=-1)\n",
        "        out = self.out_proj(y)\n",
        "        return out.unsqueeze(1), conv_state, ssm_state\n",
        "\n",
        "    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):\n",
        "        device = self.out_proj.weight.device\n",
        "        conv_dtype = self.conv1d.weight.dtype if dtype is None else dtype\n",
        "        conv_state = torch.zeros(batch_size, self.d_conv, self.conv1d.weight.shape[0], device=device, dtype=conv_dtype).transpose(1, 2)\n",
        "        ssm_dtype = self.in_proj.weight.dtype if dtype is None else dtype\n",
        "        ssm_state = torch.zeros(batch_size, self.nheads, self.headdim, self.d_state, device=device, dtype=ssm_dtype)\n",
        "        return conv_state, ssm_state\n",
        "\n",
        "    def _get_states_from_cache(self, inference_params, batch_size, initialize_states=False):\n",
        "        assert self.layer_idx is not None\n",
        "        if self.layer_idx not in inference_params.key_value_memory_dict:\n",
        "            batch_shape = (batch_size,)\n",
        "            conv_state = torch.zeros(batch_size, self.d_conv, self.conv1d.weight.shape[0],\n",
        "                device=self.conv1d.weight.device, dtype=self.conv1d.weight.dtype).transpose(1, 2)\n",
        "            ssm_state = torch.zeros(batch_size, self.nheads, self.headdim, self.d_state,\n",
        "                device=self.in_proj.weight.device, dtype=self.in_proj.weight.dtype)\n",
        "            inference_params.key_value_memory_dict[self.layer_idx] = (conv_state, ssm_state)\n",
        "        else:\n",
        "            conv_state, ssm_state = inference_params.key_value_memory_dict[self.layer_idx]\n",
        "            # TODO: What if batch size changes between generation, and we reuse the same states?\n",
        "            if initialize_states:\n",
        "                conv_state.zero_()\n",
        "                ssm_state.zero_()\n",
        "        return conv_state, ssm_state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jHsJQqPbt7mo"
      },
      "outputs": [],
      "source": [
        "# @title mamba_ssm/modules/mamba2_simple.py\n",
        "# https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba2_simple.py\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "try: from causal_conv1d import causal_conv1d_fn\n",
        "except ImportError: causal_conv1d_fn = None\n",
        "# from mamba_ssm.ops.triton.layernorm_gated import RMSNorm as RMSNormGated, LayerNorm\n",
        "# from mamba_ssm.ops.triton.ssd_combined import mamba_chunk_scan_combined\n",
        "# from mamba_ssm.ops.triton.ssd_combined import mamba_split_conv1d_scan_combined\n",
        "\n",
        "class Mamba2Simple(nn.Module):\n",
        "    def __init__(self, d_model, d_state=64, d_conv=4,\n",
        "        conv_init=None,\n",
        "        expand=2, headdim=128, ngroups=1,\n",
        "        A_init_range=(1, 16),\n",
        "        dt_min=0.001, dt_max=0.1, dt_init_floor=1e-4, dt_limit=(0.0, float(\"inf\")),\n",
        "        learnable_init_states=False,\n",
        "        activation=\"swish\",\n",
        "        bias=False, conv_bias=True,\n",
        "        # Fused kernel and sharding options\n",
        "        chunk_size=256, use_mem_eff_path=True,\n",
        "        layer_idx=None,  # Absorb kwarg for general module\n",
        "        device=None, dtype=None,):\n",
        "\n",
        "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_state = d_state\n",
        "        self.d_conv = d_conv\n",
        "        self.conv_init = conv_init\n",
        "        self.expand = expand\n",
        "        self.d_inner = self.expand * self.d_model\n",
        "        self.headdim = headdim\n",
        "        self.ngroups = ngroups\n",
        "        assert self.d_inner % self.headdim == 0\n",
        "        self.nheads = self.d_inner // self.headdim\n",
        "        self.dt_limit = dt_limit\n",
        "        self.learnable_init_states = learnable_init_states\n",
        "        self.activation = activation\n",
        "        self.chunk_size = chunk_size\n",
        "        self.use_mem_eff_path = use_mem_eff_path\n",
        "        self.layer_idx = layer_idx\n",
        "\n",
        "        # Order: [z, x, B, C, dt]\n",
        "        d_in_proj = 2 * self.d_inner + 2 * self.ngroups * self.d_state + self.nheads\n",
        "        self.in_proj = nn.Linear(self.d_model, d_in_proj, bias=bias, **factory_kwargs)\n",
        "\n",
        "        conv_dim = self.d_inner + 2 * self.ngroups * self.d_state\n",
        "        self.conv1d = nn.Conv1d(in_channels=conv_dim, out_channels=conv_dim,\n",
        "            bias=conv_bias,\n",
        "            kernel_size=d_conv,\n",
        "            groups=conv_dim, padding=d_conv - 1, **factory_kwargs,)\n",
        "        if self.conv_init is not None:\n",
        "            nn.init.uniform_(self.conv1d.weight, -self.conv_init, self.conv_init)\n",
        "        # self.conv1d.weight._no_weight_decay = True\n",
        "\n",
        "        if self.learnable_init_states:\n",
        "            self.init_states = nn.Parameter(torch.zeros(self.nheads, self.headdim, self.d_state, **factory_kwargs))\n",
        "            self.init_states._no_weight_decay = True\n",
        "\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "        # Initialize log dt bias\n",
        "        dt = torch.exp(torch.rand(self.nheads, **factory_kwargs) * (math.log(dt_max) - math.log(dt_min)) + math.log(dt_min))\n",
        "        dt = torch.clamp(dt, min=dt_init_floor)\n",
        "        # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n",
        "        inv_dt = dt + torch.log(-torch.expm1(-dt))\n",
        "        self.dt_bias = nn.Parameter(inv_dt)\n",
        "        # Just to be explicit. Without this we already don't put wd on dt_bias because of the check\n",
        "        # name.endswith(\"bias\") in param_grouping.py\n",
        "        self.dt_bias._no_weight_decay = True\n",
        "\n",
        "        # A parameter\n",
        "        assert A_init_range[0] > 0 and A_init_range[1] >= A_init_range[0]\n",
        "        A = torch.empty(self.nheads, dtype=torch.float32, device=device).uniform_(*A_init_range)\n",
        "        A_log = torch.log(A).to(dtype=dtype)\n",
        "        self.A_log = nn.Parameter(A_log)\n",
        "        # self.register_buffer(\"A_log\", torch.zeros(self.nheads, dtype=torch.float32, device=device), persistent=True)\n",
        "        self.A_log._no_weight_decay = True\n",
        "\n",
        "        # D \"skip\" parameter\n",
        "        self.D = nn.Parameter(torch.ones(self.nheads, device=device))\n",
        "        self.D._no_weight_decay = True\n",
        "\n",
        "        # Extra normalization layer right before output projection\n",
        "        self.norm = RMSNormGated(self.d_inner, eps=1e-5, norm_before_gate=False, **factory_kwargs)\n",
        "        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=bias, **factory_kwargs)\n",
        "\n",
        "    def forward(self, u, seq_idx=None):\n",
        "        \"\"\"u: (B, L, D)\n",
        "        Returns: same shape as u\"\"\"\n",
        "        batch, seqlen, dim = u.shape\n",
        "\n",
        "        zxbcdt = self.in_proj(u)  # (B, L, d_in_proj)\n",
        "        A = -torch.exp(self.A_log)  # (nheads) or (d_inner, d_state)\n",
        "        initial_states=repeat(self.init_states, \"... -> b ...\", b=batch) if self.learnable_init_states else None\n",
        "        dt_limit_kwargs = {} if self.dt_limit == (0.0, float(\"inf\")) else dict(dt_limit=self.dt_limit)\n",
        "\n",
        "        if self.use_mem_eff_path:\n",
        "            # Fully fused path\n",
        "            out = mamba_split_conv1d_scan_combined(\n",
        "                zxbcdt,\n",
        "                rearrange(self.conv1d.weight, \"d 1 w -> d w\"),\n",
        "                self.conv1d.bias,\n",
        "                self.dt_bias,\n",
        "                A,\n",
        "                D=self.D,\n",
        "                chunk_size=self.chunk_size,\n",
        "                seq_idx=seq_idx,\n",
        "                activation=self.activation,\n",
        "                rmsnorm_weight=self.norm.weight,\n",
        "                rmsnorm_eps=self.norm.eps,\n",
        "                outproj_weight=self.out_proj.weight,\n",
        "                outproj_bias=self.out_proj.bias,\n",
        "                headdim=self.headdim,\n",
        "                ngroups=self.ngroups,\n",
        "                norm_before_gate=False,\n",
        "                initial_states=initial_states,\n",
        "                **dt_limit_kwargs,\n",
        "            )\n",
        "        else:\n",
        "            z, xBC, dt = torch.split(zxbcdt, [self.d_inner, self.d_inner + 2 * self.ngroups * self.d_state, self.nheads], dim=-1)\n",
        "            dt = F.softplus(dt + self.dt_bias)  # (B, L, nheads)\n",
        "            assert self.activation in [\"silu\", \"swish\"]\n",
        "\n",
        "            # 1D Convolution\n",
        "            if causal_conv1d_fn is None or self.activation not in [\"silu\", \"swish\"]:\n",
        "                xBC = self.act(self.conv1d(xBC.transpose(1, 2)).transpose(1, 2))  # (B, L, self.d_inner + 2 * ngroups * d_state)\n",
        "                xBC = xBC[:, :seqlen, :]\n",
        "            else:\n",
        "                xBC = causal_conv1d_fn(\n",
        "                    x=xBC.transpose(1, 2),\n",
        "                    weight=rearrange(self.conv1d.weight, \"d 1 w -> d w\"),\n",
        "                    bias=self.conv1d.bias,\n",
        "                    activation=self.activation,\n",
        "                ).transpose(1, 2)\n",
        "\n",
        "            # Split into 3 main branches: X, B, C\n",
        "            # These correspond to V, K, Q respectively in the SSM/attention duality\n",
        "            x, B, C = torch.split(xBC, [self.d_inner, self.ngroups * self.d_state, self.ngroups * self.d_state], dim=-1)\n",
        "            y = mamba_chunk_scan_combined(\n",
        "                rearrange(x, \"b l (h p) -> b l h p\", p=self.headdim),\n",
        "                dt,\n",
        "                A,\n",
        "                rearrange(B, \"b l (g n) -> b l g n\", g=self.ngroups),\n",
        "                rearrange(C, \"b l (g n) -> b l g n\", g=self.ngroups),\n",
        "                chunk_size=self.chunk_size,\n",
        "                D=self.D,\n",
        "                z=None,\n",
        "                seq_idx=seq_idx,\n",
        "                initial_states=initial_states,\n",
        "                **dt_limit_kwargs,\n",
        "            )\n",
        "            y = rearrange(y, \"b l h p -> b l (h p)\")\n",
        "\n",
        "            # Multiply \"gate\" branch and apply extra normalization layer\n",
        "            y = self.norm(y, z)\n",
        "            out = self.out_proj(y)\n",
        "        return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AqMmHadbuEo-"
      },
      "outputs": [],
      "source": [
        "# @title mamba_ssm/modules/mha.py\n",
        "# https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mha.py\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange\n",
        "\n",
        "try: from flash_attn import flash_attn_with_kvcache\n",
        "except ImportError: flash_attn_with_kvcache = None\n",
        "try: from flash_attn.layers.rotary import RotaryEmbedding\n",
        "except ImportError: RotaryEmbedding = None\n",
        "try: from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\n",
        "except ImportError: causal_conv1d_fn, causal_conv1d_update = None, None\n",
        "\n",
        "\n",
        "def _update_kv_cache(kv, inference_params, layer_idx):\n",
        "    \"\"\"kv: (batch_size, seqlen, 2, nheads, head_dim) or (batch_size, 1, 2, nheads, head_dim)\"\"\"\n",
        "    # Pre-allocate memory for key-values for inference.\n",
        "    num_heads, head_dim = kv.shape[-2:]\n",
        "    assert layer_idx in inference_params.key_value_memory_dict\n",
        "    kv_cache, _ = inference_params.key_value_memory_dict[layer_idx]\n",
        "    # Adjust key and value for inference\n",
        "    batch_start = inference_params.batch_size_offset\n",
        "    batch_end = batch_start + kv.shape[0]\n",
        "    sequence_start = inference_params.seqlen_offset\n",
        "    sequence_end = sequence_start + kv.shape[1]\n",
        "    assert batch_end <= kv_cache.shape[0]\n",
        "    assert sequence_end <= kv_cache.shape[1]\n",
        "    assert kv_cache is not None\n",
        "    kv_cache[batch_start:batch_end, sequence_start:sequence_end, ...] = kv\n",
        "    return kv_cache[batch_start:batch_end, :sequence_end, ...]\n",
        "\n",
        "\n",
        "class MHA(nn.Module):\n",
        "    \"\"\"Multi-head self-attention and cross-attention\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim,\n",
        "        num_heads,\n",
        "        num_heads_kv=None,\n",
        "        head_dim=None,  # If None, use embed_dim // num_heads\n",
        "        mlp_dim=0,\n",
        "        qkv_proj_bias=True,\n",
        "        out_proj_bias=True,\n",
        "        softmax_scale=None,\n",
        "        causal=False,\n",
        "        layer_idx=None,\n",
        "        d_conv=0,\n",
        "        rotary_emb_dim=0,\n",
        "        rotary_emb_base=10000.0,\n",
        "        rotary_emb_interleaved=False,\n",
        "        device=None,\n",
        "        dtype=None,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        num_heads_kv: can be used to toggle MQA / GQA. If None, use num_heads.\n",
        "        return_residual: whether to return the input x along with the output. This is for\n",
        "            performance reason: for post-norm architecture, returning the input allows us\n",
        "            to fuse the backward of nn.Linear with the residual connection.\n",
        "        \"\"\"\n",
        "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.layer_idx = layer_idx\n",
        "        self.d_conv = d_conv\n",
        "        self.rotary_emb_dim = rotary_emb_dim\n",
        "        self.softmax_scale = softmax_scale\n",
        "        self.causal = causal\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.num_heads_kv = num_heads_kv if num_heads_kv is not None else num_heads\n",
        "        assert (\n",
        "            self.num_heads % self.num_heads_kv == 0\n",
        "        ), \"num_heads must be divisible by num_heads_kv\"\n",
        "        if head_dim is None:\n",
        "            assert self.embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
        "        self.head_dim = head_dim if head_dim is not None else self.embed_dim // num_heads\n",
        "        self.mlp_dim = math.ceil(mlp_dim / 256) * 256\n",
        "        qkv_dim = self.head_dim * (self.num_heads + 2 * self.num_heads_kv)\n",
        "        out_dim = self.head_dim * self.num_heads\n",
        "\n",
        "        if self.rotary_emb_dim > 0:\n",
        "            assert RotaryEmbedding is not None, \"rotary requires flash_attn to be installed\"\n",
        "            self.rotary_emb = RotaryEmbedding(\n",
        "                self.rotary_emb_dim,\n",
        "                base=rotary_emb_base,\n",
        "                interleaved=rotary_emb_interleaved,\n",
        "                device=device,\n",
        "            )\n",
        "\n",
        "        self.in_proj = nn.Linear(embed_dim, qkv_dim + self.mlp_dim, bias=qkv_proj_bias, **factory_kwargs)\n",
        "        if self.d_conv > 0:\n",
        "            self.conv1d = nn.Conv1d(\n",
        "                qkv_dim, qkv_dim, kernel_size=self.d_conv, padding=self.d_conv - 1, groups=qkv_dim,\n",
        "                **factory_kwargs\n",
        "            )\n",
        "        self.out_proj = nn.Linear(out_dim + self.mlp_dim // 2, embed_dim, bias=out_proj_bias, **factory_kwargs)\n",
        "\n",
        "    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None):\n",
        "        dtype = self.out_proj.weight.dtype if dtype is None else dtype\n",
        "        device = self.out_proj.weight.device\n",
        "        if self.d_conv > 0:\n",
        "            conv_state = torch.zeros(\n",
        "                batch_size, self.conv1d.weight.shape[0], self.d_conv, device=device, dtype=dtype\n",
        "            )\n",
        "        else:\n",
        "            conv_state = None\n",
        "        kv_cache = torch.empty(\n",
        "            batch_size, max_seqlen, 2, self.num_heads_kv, self.head_dim, dtype=dtype, device=device,\n",
        "        )\n",
        "        return kv_cache, conv_state\n",
        "\n",
        "    def _update_kv_cache(self, kv, inference_params):\n",
        "        \"\"\"kv: (batch_size, seqlen, 2, nheads, head_dim) or (batch_size, 1, 2, nheads, head_dim)\"\"\"\n",
        "        assert self.layer_idx is not None, \"Generation requires layer_idx in the constructor\"\n",
        "        return _update_kv_cache(kv, inference_params, self.layer_idx)\n",
        "\n",
        "    def _apply_rotary_update_kvcache_attention(self, q, kv, inference_params):\n",
        "        \"\"\"\n",
        "        Fast path that combine 3 steps: apply rotary to Q and K, update kv cache, and apply attention.\n",
        "        q: (batch_size, seqlen_q, nheads, head_dim)\n",
        "        kv: (batch_size, seqlen_k, 2, nheads_kv, head_dim)\n",
        "        \"\"\"\n",
        "        assert inference_params is not None and inference_params.seqlen_offset > 0\n",
        "        if self.rotary_emb_dim > 0:\n",
        "            self.rotary_emb._update_cos_sin_cache(\n",
        "                inference_params.max_seqlen, device=q.device, dtype=q.dtype\n",
        "            )\n",
        "            rotary_cos, rotary_sin = self.rotary_emb._cos_cached, self.rotary_emb._sin_cached\n",
        "        else:\n",
        "            rotary_cos, rotary_sin = None, None\n",
        "        batch = q.shape[0]\n",
        "        kv_cache, _ = inference_params.key_value_memory_dict[self.layer_idx]\n",
        "        kv_cache = kv_cache[:batch]\n",
        "        cache_seqlens = (\n",
        "            inference_params.lengths_per_sample[:batch]\n",
        "            if inference_params.lengths_per_sample is not None\n",
        "            else inference_params.seqlen_offset\n",
        "        )\n",
        "        assert flash_attn_with_kvcache is not None, \"flash_attn must be installed\"\n",
        "        context = flash_attn_with_kvcache(\n",
        "            q,\n",
        "            kv_cache[:, :, 0],\n",
        "            kv_cache[:, :, 1],\n",
        "            kv[:, :, 0],\n",
        "            kv[:, :, 1],\n",
        "            rotary_cos=rotary_cos,\n",
        "            rotary_sin=rotary_sin,\n",
        "            cache_seqlens=cache_seqlens,\n",
        "            softmax_scale=self.softmax_scale,\n",
        "            causal=self.causal,\n",
        "            rotary_interleaved=self.rotary_emb.interleaved if self.rotary_emb_dim > 0 else False,\n",
        "        )\n",
        "        return context\n",
        "\n",
        "    def _update_kvcache_attention(self, q, kv, inference_params):\n",
        "        \"\"\"Write kv to inference_params, then do attention\"\"\"\n",
        "        if (\n",
        "            inference_params.seqlen_offset == 0\n",
        "            or flash_attn_with_kvcache is None\n",
        "        ):\n",
        "            # TODO: this only uses seqlen_offset and not lengths_per_sample.\n",
        "            kv = self._update_kv_cache(kv, inference_params)\n",
        "            k, v = kv.unbind(dim=-3)\n",
        "            k = torch.repeat_interleave(k, dim=2, repeats=self.num_heads // self.num_heads_kv)\n",
        "            v = torch.repeat_interleave(v, dim=2, repeats=self.num_heads // self.num_heads_kv)\n",
        "            return F.scaled_dot_product_attention(\n",
        "                q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=self.causal, scale=self.softmax_scale\n",
        "            ).transpose(1, 2)\n",
        "        else:\n",
        "            batch = q.shape[0]\n",
        "            kv_cache, _ = inference_params.key_value_memory_dict[self.layer_idx]\n",
        "            kv_cache = kv_cache[:batch]\n",
        "            cache_seqlens = (\n",
        "                inference_params.lengths_per_sample[:batch]\n",
        "                if inference_params.lengths_per_sample is not None\n",
        "                else inference_params.seqlen_offset\n",
        "            )\n",
        "            return flash_attn_with_kvcache(\n",
        "                q,\n",
        "                kv_cache[:, :, 0],\n",
        "                kv_cache[:, :, 1],\n",
        "                kv[:, :, 0],\n",
        "                kv[:, :, 1],\n",
        "                cache_seqlens=cache_seqlens,\n",
        "                softmax_scale=self.softmax_scale,\n",
        "                causal=self.causal,\n",
        "            )\n",
        "\n",
        "    def forward(self, x, inference_params=None):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: (batch, seqlen, hidden_dim) (where hidden_dim = num heads * head dim) if\n",
        "                cu_seqlens is None and max_seqlen is None, else (total, hidden_dim) where total\n",
        "                is the is the sum of the sequence lengths in the batch.\n",
        "            inference_params: for generation. Adapted from Megatron-LM (and Apex)\n",
        "            https://github.com/NVIDIA/apex/blob/3ff1a10f72ec07067c4e44759442329804ac5162/apex/transformer/testing/standalone_transformer_lm.py#L470\n",
        "        \"\"\"\n",
        "        if inference_params is not None and self.layer_idx not in inference_params.key_value_memory_dict:\n",
        "            inference_params.key_value_memory_dict[self.layer_idx] = self.allocate_inference_cache(\n",
        "                x.shape[0], inference_params.max_seqlen, dtype=x.dtype\n",
        "            )\n",
        "        seqlen_offset = (\n",
        "            0\n",
        "            if inference_params is None\n",
        "            else (\n",
        "                inference_params.lengths_per_sample\n",
        "                if inference_params.lengths_per_sample is not None\n",
        "                else inference_params.seqlen_offset\n",
        "            )\n",
        "        )\n",
        "        rotary_max_seqlen = inference_params.max_seqlen if inference_params is not None else None\n",
        "        qkv = self.in_proj(x)\n",
        "        if self.mlp_dim > 0:\n",
        "            qkv, x_mlp = qkv.split([qkv.shape[-1] - self.mlp_dim, self.mlp_dim], dim=-1)\n",
        "            x_mlp_up, x_mlp_gate = x_mlp.chunk(2, dim=-1)\n",
        "            x_mlp = x_mlp_up * F.silu(x_mlp_gate)\n",
        "        if self.d_conv > 0:\n",
        "            # The inference code for conv1d is pretty messy, should clean it up\n",
        "            if (inference_params is None or inference_params.seqlen_offset == 0):\n",
        "                if causal_conv1d_fn is None:\n",
        "                    qkv = rearrange(\n",
        "                        self.conv1d(rearrange(qkv, \"b s d -> b d s\"))[..., :-(self.d_conv - 1)], \"b d s -> b s d\"\n",
        "                    ).contiguous()\n",
        "                else:\n",
        "                    qkv = causal_conv1d_fn(\n",
        "                        qkv.transpose(1, 2),\n",
        "                        rearrange(self.conv1d.weight, \"d 1 w -> d w\"),\n",
        "                        self.conv1d.bias\n",
        "                    ).transpose(1, 2)\n",
        "                if inference_params is not None:\n",
        "                    _, conv_state = inference_params.key_value_memory_dict[self.layer_idx]\n",
        "                    # If we just take qkv[:, :, -self.d_conv :], it will error if seqlen < self.d_conv\n",
        "                    # Instead F.pad will pad with zeros if seqlen < self.d_conv, and truncate otherwise.\n",
        "                    qkv_t = rearrange(qkv, \"b l d -> b d l\")\n",
        "                    conv_state.copy_(F.pad(qkv_t, (self.d_conv - qkv_t.shape[-1], 0)))  # Update state (B D W)\n",
        "            else:\n",
        "                _, conv_state = inference_params.key_value_memory_dict[self.layer_idx]\n",
        "                assert qkv.shape[1] == 1, \"Only support decoding with 1 token at a time for now\"\n",
        "                qkv = qkv.squeeze(1)\n",
        "                # Conv step\n",
        "                if causal_conv1d_update is None:\n",
        "                    conv_state.copy_(torch.roll(conv_state, shifts=-1, dims=-1))  # Update state (B D W)\n",
        "                    conv_state[:, :, -1] = qkv\n",
        "                    qkv = torch.sum(conv_state * rearrange(self.conv1d.weight, \"d 1 w -> d w\"), dim=-1)  # (B D)\n",
        "                    if self.conv1d.bias is not None:\n",
        "                        qkv = qkv + self.conv1d.bias\n",
        "                else:\n",
        "                    qkv = causal_conv1d_update(\n",
        "                        qkv,\n",
        "                        conv_state,\n",
        "                        rearrange(self.conv1d.weight, \"d 1 w -> d w\"),\n",
        "                        self.conv1d.bias\n",
        "                    )\n",
        "                qkv = qkv.unsqueeze(1)\n",
        "        q, kv = qkv.split([self.num_heads * self.head_dim, self.num_heads_kv * 2 * self.head_dim], dim=-1)\n",
        "        q = rearrange(q, \"... (h d) -> ... h d\", d=self.head_dim)\n",
        "        kv = rearrange(kv, \"... (two hkv d) -> ... two hkv d\", two=2, d=self.head_dim)\n",
        "        if (\n",
        "            inference_params is None\n",
        "            or inference_params.seqlen_offset == 0\n",
        "            or (self.rotary_emb_dim == 0 or self.rotary_emb_dim % 16 != 0)\n",
        "        ):\n",
        "            if self.rotary_emb_dim > 0:\n",
        "                q, kv = self.rotary_emb(\n",
        "                    q, kv, seqlen_offset=seqlen_offset, max_seqlen=rotary_max_seqlen\n",
        "                )\n",
        "            if inference_params is None:\n",
        "                k, v = kv.unbind(dim=-3)\n",
        "                k = torch.repeat_interleave(k, dim=2, repeats=self.num_heads // self.num_heads_kv)\n",
        "                v = torch.repeat_interleave(v, dim=2, repeats=self.num_heads // self.num_heads_kv)\n",
        "                context = F.scaled_dot_product_attention(\n",
        "                    q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=self.causal, scale=self.softmax_scale\n",
        "                ).transpose(1, 2)\n",
        "            else:\n",
        "                context = self._update_kvcache_attention(q, kv, inference_params)\n",
        "        else:\n",
        "            context = self._apply_rotary_update_kvcache_attention(q, kv, inference_params)\n",
        "        context = rearrange(context, \"... h d -> ... (h d)\")\n",
        "        if self.mlp_dim > 0:\n",
        "            context = torch.cat([context, x_mlp], dim=-1)\n",
        "        out = self.out_proj(context)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lf4BD3_duGl9"
      },
      "outputs": [],
      "source": [
        "# @title mamba_ssm/modules/mlp.py\n",
        "# https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mlp.py\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class GatedMLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None,\n",
        "        activation=F.silu, bias=False,\n",
        "        multiple_of=128, device=None, dtype=None,):\n",
        "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
        "        super().__init__()\n",
        "        out_features = out_features if out_features is not None else in_features\n",
        "        hidden_features = (hidden_features if hidden_features is not None else int(8 * in_features / 3))\n",
        "        hidden_features = (hidden_features + multiple_of - 1) // multiple_of * multiple_of\n",
        "        self.fc1 = nn.Linear(in_features, 2 * hidden_features, bias=bias, **factory_kwargs)\n",
        "        self.activation = activation\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias, **factory_kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.fc1(x)\n",
        "        y, gate = y.chunk(2, dim=-1)\n",
        "        y = y * self.activation(gate)\n",
        "        y = self.fc2(y)\n",
        "        return y\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vulD1EcSs9xc"
      },
      "outputs": [],
      "source": [
        "# @title mamba_ssm/models/mixer_seq_simple.py\n",
        "# https://github.com/state-spaces/mamba/blob/main/mamba_ssm/models/mixer_seq_simple.py\n",
        "import math\n",
        "from functools import partial\n",
        "import json\n",
        "import os\n",
        "import copy\n",
        "from collections import namedtuple\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from mamba_ssm.models.config_mamba import MambaConfig\n",
        "from mamba_ssm.modules.mamba_simple import Mamba\n",
        "from mamba_ssm.modules.mamba2 import Mamba2\n",
        "from mamba_ssm.modules.mha import MHA\n",
        "from mamba_ssm.modules.mlp import GatedMLP\n",
        "from mamba_ssm.modules.block import Block\n",
        "from mamba_ssm.utils.generation import GenerationMixin\n",
        "from mamba_ssm.utils.hf import load_config_hf, load_state_dict_hf\n",
        "\n",
        "try:\n",
        "    from mamba_ssm.ops.triton.layer_norm import RMSNorm, layer_norm_fn, rms_norm_fn\n",
        "except ImportError:\n",
        "    RMSNorm, layer_norm_fn, rms_norm_fn = None, None, None\n",
        "\n",
        "\n",
        "def create_block(\n",
        "    d_model,\n",
        "    d_intermediate,\n",
        "    ssm_cfg=None,\n",
        "    attn_layer_idx=None,\n",
        "    attn_cfg=None,\n",
        "    norm_epsilon=1e-5,\n",
        "    rms_norm=False,\n",
        "    residual_in_fp32=False,\n",
        "    fused_add_norm=False,\n",
        "    layer_idx=None,\n",
        "    device=None,\n",
        "    dtype=None,\n",
        "):\n",
        "    if ssm_cfg is None:\n",
        "        ssm_cfg = {}\n",
        "    if attn_layer_idx is None:\n",
        "        attn_layer_idx = []\n",
        "    if attn_cfg is None:\n",
        "        attn_cfg = {}\n",
        "    factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
        "    if layer_idx not in attn_layer_idx:\n",
        "        # Create a copy of the config to modify\n",
        "        ssm_cfg = copy.deepcopy(ssm_cfg) if ssm_cfg is not None else {}\n",
        "        ssm_layer = ssm_cfg.pop(\"layer\", \"Mamba1\")\n",
        "        if ssm_layer not in [\"Mamba1\", \"Mamba2\"]:\n",
        "            raise ValueError(f\"Invalid ssm_layer: {ssm_layer}, only support Mamba1 and Mamba2\")\n",
        "        mixer_cls = partial(\n",
        "            Mamba2 if ssm_layer == \"Mamba2\" else Mamba,\n",
        "            layer_idx=layer_idx,\n",
        "            **ssm_cfg,\n",
        "            **factory_kwargs\n",
        "        )\n",
        "    else:\n",
        "        mixer_cls = partial(MHA, layer_idx=layer_idx, **attn_cfg, **factory_kwargs)\n",
        "    norm_cls = partial(nn.LayerNorm if not rms_norm else RMSNorm, eps=norm_epsilon, **factory_kwargs)\n",
        "    if d_intermediate == 0:\n",
        "        mlp_cls = nn.Identity\n",
        "    else:\n",
        "        mlp_cls = partial(GatedMLP, hidden_features=d_intermediate, out_features=d_model, **factory_kwargs)\n",
        "    block = Block(\n",
        "        d_model,\n",
        "        mixer_cls,\n",
        "        mlp_cls,\n",
        "        norm_cls=norm_cls,\n",
        "        fused_add_norm=fused_add_norm,\n",
        "        residual_in_fp32=residual_in_fp32,\n",
        "    )\n",
        "    block.layer_idx = layer_idx\n",
        "    return block\n",
        "\n",
        "\n",
        "# https://github.com/huggingface/transformers/blob/c28d04e9e252a1a099944e325685f14d242ecdcd/src/transformers/models/gpt2/modeling_gpt2.py#L454\n",
        "def _init_weights(\n",
        "    module,\n",
        "    n_layer,\n",
        "    initializer_range=0.02,  # Now only used for embedding layer.\n",
        "    rescale_prenorm_residual=True,\n",
        "    n_residuals_per_layer=1,  # Change to 2 if we have MLP\n",
        "):\n",
        "    if isinstance(module, nn.Linear):\n",
        "        if module.bias is not None:\n",
        "            if not getattr(module.bias, \"_no_reinit\", False):\n",
        "                nn.init.zeros_(module.bias)\n",
        "    elif isinstance(module, nn.Embedding):\n",
        "        nn.init.normal_(module.weight, std=initializer_range)\n",
        "\n",
        "    if rescale_prenorm_residual:\n",
        "        # Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:\n",
        "        #   > A modified initialization which accounts for the accumulation on the residual path with model depth. Scale\n",
        "        #   > the weights of residual layers at initialization by a factor of 1/âˆšN where N is the # of residual layers.\n",
        "        #   >   -- GPT-2 :: https://openai.com/blog/better-language-models/\n",
        "        #\n",
        "        # Reference (Megatron-LM): https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py\n",
        "        for name, p in module.named_parameters():\n",
        "            if name in [\"out_proj.weight\", \"fc2.weight\"]:\n",
        "                # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block\n",
        "                # Following Pytorch init, except scale by 1/sqrt(2 * n_layer)\n",
        "                # We need to reinit p since this code could be called multiple times\n",
        "                # Having just p *= scale would repeatedly scale it down\n",
        "                nn.init.kaiming_uniform_(p, a=math.sqrt(5))\n",
        "                with torch.no_grad():\n",
        "                    p /= math.sqrt(n_residuals_per_layer * n_layer)\n",
        "\n",
        "\n",
        "class MixerModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model,\n",
        "        n_layer,\n",
        "        d_intermediate,\n",
        "        vocab_size,\n",
        "        ssm_cfg=None,\n",
        "        attn_layer_idx=None,\n",
        "        attn_cfg=None,\n",
        "        norm_epsilon: float = 1e-5,\n",
        "        rms_norm: bool = False,\n",
        "        initializer_cfg=None,\n",
        "        fused_add_norm=False,\n",
        "        residual_in_fp32=False,\n",
        "        device=None,\n",
        "        dtype=None,\n",
        "    ) -> None:\n",
        "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
        "        super().__init__()\n",
        "        self.residual_in_fp32 = residual_in_fp32\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model, **factory_kwargs)\n",
        "\n",
        "        # We change the order of residual and layer norm:\n",
        "        # Instead of LN -> Attn / MLP -> Add, we do:\n",
        "        # Add -> LN -> Attn / MLP / Mixer, returning both the residual branch (output of Add) and\n",
        "        # the main branch (output of MLP / Mixer). The model definition is unchanged.\n",
        "        # This is for performance reason: we can fuse add + layer_norm.\n",
        "        self.fused_add_norm = fused_add_norm\n",
        "        if self.fused_add_norm:\n",
        "            if layer_norm_fn is None or rms_norm_fn is None:\n",
        "                raise ImportError(\"Failed to import Triton LayerNorm / RMSNorm kernels\")\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "                create_block(\n",
        "                    d_model,\n",
        "                    d_intermediate=d_intermediate,\n",
        "                    ssm_cfg=ssm_cfg,\n",
        "                    attn_layer_idx=attn_layer_idx,\n",
        "                    attn_cfg=attn_cfg,\n",
        "                    norm_epsilon=norm_epsilon,\n",
        "                    rms_norm=rms_norm,\n",
        "                    residual_in_fp32=residual_in_fp32,\n",
        "                    fused_add_norm=fused_add_norm,\n",
        "                    layer_idx=i,\n",
        "                    **factory_kwargs,\n",
        "                )\n",
        "                for i in range(n_layer)])\n",
        "\n",
        "        self.norm_f = (nn.LayerNorm if not rms_norm else RMSNorm)(\n",
        "            d_model, eps=norm_epsilon, **factory_kwargs\n",
        "        )\n",
        "\n",
        "        self.apply(\n",
        "            partial(\n",
        "                _init_weights,\n",
        "                n_layer=n_layer,\n",
        "                **(initializer_cfg if initializer_cfg is not None else {}),\n",
        "                n_residuals_per_layer=1 if d_intermediate == 0 else 2,  # 2 if we have MLP\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):\n",
        "        return {\n",
        "            i: layer.allocate_inference_cache(batch_size, max_seqlen, dtype=dtype, **kwargs)\n",
        "            for i, layer in enumerate(self.layers)\n",
        "        }\n",
        "\n",
        "    def forward(self, input_ids, inference_params=None, **mixer_kwargs):\n",
        "        hidden_states = self.embedding(input_ids)\n",
        "        residual = None\n",
        "        for layer in self.layers:\n",
        "            hidden_states, residual = layer(hidden_states, residual, inference_params=inference_params, **mixer_kwargs)\n",
        "        if not self.fused_add_norm:\n",
        "            residual = (hidden_states + residual) if residual is not None else hidden_states\n",
        "            hidden_states = self.norm_f(residual.to(dtype=self.norm_f.weight.dtype))\n",
        "        else:\n",
        "            # Set prenorm=False here since we don't need the residual\n",
        "            hidden_states = layer_norm_fn(\n",
        "                hidden_states,\n",
        "                self.norm_f.weight,\n",
        "                self.norm_f.bias,\n",
        "                eps=self.norm_f.eps,\n",
        "                residual=residual,\n",
        "                prenorm=False,\n",
        "                residual_in_fp32=self.residual_in_fp32,\n",
        "                is_rms_norm=isinstance(self.norm_f, RMSNorm)\n",
        "            )\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class MambaLMHeadModel(nn.Module, GenerationMixin):\n",
        "    def __init__(self, config: MambaConfig, initializer_cfg=None, device=None, dtype=None,) -> None:\n",
        "        self.config = config\n",
        "        d_model = config.d_model\n",
        "        n_layer = config.n_layer\n",
        "        d_intermediate = config.d_intermediate\n",
        "        vocab_size = config.vocab_size\n",
        "        ssm_cfg = config.ssm_cfg\n",
        "        attn_layer_idx = config.attn_layer_idx\n",
        "        attn_cfg = config.attn_cfg\n",
        "        rms_norm = config.rms_norm\n",
        "        residual_in_fp32 = config.residual_in_fp32\n",
        "        fused_add_norm = config.fused_add_norm\n",
        "        pad_vocab_size_multiple = config.pad_vocab_size_multiple\n",
        "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
        "\n",
        "        super().__init__()\n",
        "        if vocab_size % pad_vocab_size_multiple != 0:\n",
        "            vocab_size += pad_vocab_size_multiple - (vocab_size % pad_vocab_size_multiple)\n",
        "        self.backbone = MixerModel(\n",
        "            d_model=d_model,\n",
        "            n_layer=n_layer,\n",
        "            d_intermediate=d_intermediate,\n",
        "            vocab_size=vocab_size,\n",
        "            ssm_cfg=ssm_cfg,\n",
        "            attn_layer_idx=attn_layer_idx,\n",
        "            attn_cfg=attn_cfg,\n",
        "            rms_norm=rms_norm,\n",
        "            initializer_cfg=initializer_cfg,\n",
        "            fused_add_norm=fused_add_norm,\n",
        "            residual_in_fp32=residual_in_fp32,\n",
        "            **factory_kwargs,\n",
        "        )\n",
        "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False, **factory_kwargs)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.apply(\n",
        "            partial(\n",
        "                _init_weights,\n",
        "                n_layer=n_layer,\n",
        "                **(initializer_cfg if initializer_cfg is not None else {}),\n",
        "            )\n",
        "        )\n",
        "        self.tie_weights()\n",
        "\n",
        "    def tie_weights(self):\n",
        "        if self.config.tie_embeddings:\n",
        "            self.lm_head.weight = self.backbone.embedding.weight\n",
        "\n",
        "    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):\n",
        "        return self.backbone.allocate_inference_cache(batch_size, max_seqlen, dtype=dtype, **kwargs)\n",
        "\n",
        "    def forward(self, input_ids, position_ids=None, inference_params=None, num_last_tokens=0, **mixer_kwargs):\n",
        "        \"\"\"\n",
        "        \"position_ids\" is just to be compatible with Transformer generation. We don't use it.\n",
        "        num_last_tokens: if > 0, only return the logits for the last n tokens\n",
        "        \"\"\"\n",
        "        hidden_states = self.backbone(input_ids, inference_params=inference_params, **mixer_kwargs)\n",
        "        if num_last_tokens > 0:\n",
        "            hidden_states = hidden_states[:, -num_last_tokens:]\n",
        "        lm_logits = self.lm_head(hidden_states)\n",
        "        CausalLMOutput = namedtuple(\"CausalLMOutput\", [\"logits\"])\n",
        "        return CausalLMOutput(logits=lm_logits)\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, pretrained_model_name, device=None, dtype=None, **kwargs):\n",
        "        config_data = load_config_hf(pretrained_model_name)\n",
        "        config = MambaConfig(**config_data)\n",
        "        model = cls(config, device=device, dtype=dtype, **kwargs)\n",
        "        model.load_state_dict(load_state_dict_hf(pretrained_model_name, device=device, dtype=dtype))\n",
        "        return model\n",
        "\n",
        "    def save_pretrained(self, save_directory):\n",
        "        \"\"\"\n",
        "        Minimal implementation of save_pretrained for MambaLMHeadModel.\n",
        "        Save the model and its configuration file to a directory.\n",
        "        \"\"\"\n",
        "        # Ensure save_directory exists\n",
        "        os.makedirs(save_directory, exist_ok=True)\n",
        "\n",
        "        # Save the model's state_dict\n",
        "        model_path = os.path.join(save_directory, 'pytorch_model.bin')\n",
        "        torch.save(self.state_dict(), model_path)\n",
        "\n",
        "        # Save the configuration of the model\n",
        "        config_path = os.path.join(save_directory, 'config.json')\n",
        "        with open(config_path, 'w') as f:\n",
        "            json.dump(self.config.__dict__, f, indent=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SorwaCX4wo8t"
      },
      "outputs": [],
      "source": [
        "# @title mamba_ssm/utils/generation.py#L247\n",
        "# https://github.com/state-spaces/mamba/blob/main/mamba_ssm/utils/generation.py#L247\n",
        "\n",
        "class GenerationMixin:\n",
        "    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def generate(\n",
        "        self,\n",
        "        input_ids,\n",
        "        max_length,\n",
        "        top_k=1,\n",
        "        top_p=0.0,\n",
        "        min_p=0.0,\n",
        "        temperature=1.0,\n",
        "        return_dict_in_generate=False,\n",
        "        output_scores=False,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        output = decode(\n",
        "            input_ids, self, max_length, top_k=top_k, top_p=top_p, min_p = min_p, temperature=temperature, output_scores=output_scores, **kwargs\n",
        "        )\n",
        "        if not output_scores:\n",
        "            output.scores = None\n",
        "        return output if return_dict_in_generate else output.sequences\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "dtbn54KmHm79"
      },
      "outputs": [],
      "source": [
        "# @title mamba_ssm/modules/ssd_minimal.py\n",
        "# https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/ssd_minimal.py\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange, repeat\n",
        "from mamba_ssm.ops.triton.ssd_combined import mamba_chunk_scan_combined\n",
        "\n",
        "\n",
        "def segsum_unstable(x):\n",
        "    \"\"\"Naive segment sum calculation.\"\"\"\n",
        "    T = x.size(-1)\n",
        "    x_cumsum = torch.cumsum(x, dim=-1)\n",
        "    x_segsum = x_cumsum[..., :, None] - x_cumsum[..., None, :]\n",
        "    mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool), diagonal=0)\n",
        "    x_segsum = x_segsum.masked_fill(~mask, -torch.inf)\n",
        "    return x_segsum\n",
        "\n",
        "def segsum(x):\n",
        "    \"\"\"More stable segment sum calculation.\"\"\"\n",
        "    T = x.size(-1)\n",
        "    x = repeat(x, \"... d -> ... d e\", e=T)\n",
        "    mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool), diagonal=-1)\n",
        "    x = x.masked_fill(~mask, 0)\n",
        "    x_segsum = torch.cumsum(x, dim=-2)\n",
        "    mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool), diagonal=0)\n",
        "    x_segsum = x_segsum.masked_fill(~mask, -torch.inf)\n",
        "    return x_segsum\n",
        "\n",
        "def ssd_minimal_discrete(X, A, B, C, block_len, initial_states=None):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "        X: (batch, length, n_heads, d_head)\n",
        "        A: (batch, length, n_heads)\n",
        "        B: (batch, length, n_heads, d_state)\n",
        "        C: (batch, length, n_heads, d_state)\n",
        "    Return:\n",
        "        Y: (batch, length, n_heads, d_head)\n",
        "    \"\"\"\n",
        "    assert X.dtype == A.dtype == B.dtype == C.dtype\n",
        "    assert X.shape[1] % block_len == 0\n",
        "\n",
        "    # Rearrange into blocks/chunks\n",
        "    X, A, B, C = [rearrange(x, \"b (c l) ... -> b c l ...\", l=block_len) for x in (X, A, B, C)]\n",
        "\n",
        "    A = rearrange(A, \"b c l h -> b h c l\")\n",
        "    A_cumsum = torch.cumsum(A, dim=-1)\n",
        "\n",
        "    # 1. Compute the output for each intra-chunk (diagonal blocks)\n",
        "    L = torch.exp(segsum(A))\n",
        "    Y_diag  = torch.einsum(\"bclhn,bcshn,bhcls,bcshp->bclhp\", C, B, L, X)\n",
        "\n",
        "    # 2. Compute the state for each intra-chunk\n",
        "    # (right term of low-rank factorization of off-diagonal blocks; B terms)\n",
        "    decay_states = torch.exp((A_cumsum[:, :, :, -1:] - A_cumsum))\n",
        "    states = torch.einsum(\"bclhn,bhcl,bclhp->bchpn\", B, decay_states, X)\n",
        "\n",
        "    # 3. Compute the inter-chunk SSM recurrence; produces correct SSM states at chunk boundaries\n",
        "    # (middle term of factorization of off-diag blocks; A terms)\n",
        "    if initial_states is None:\n",
        "        initial_states = torch.zeros_like(states[:, :1])\n",
        "    states = torch.cat([initial_states, states], dim=1)\n",
        "    decay_chunk = torch.exp(segsum(F.pad(A_cumsum[:, :, :, -1], (1, 0))))\n",
        "    new_states = torch.einsum(\"bhzc,bchpn->bzhpn\", decay_chunk, states)\n",
        "    states, final_state = new_states[:, :-1], new_states[:, -1]\n",
        "\n",
        "    # 4. Compute state -> output conversion per chunk\n",
        "    # (left term of low-rank factorization of off-diagonal blocks; C terms)\n",
        "    state_decay_out = torch.exp(A_cumsum)\n",
        "    Y_off = torch.einsum('bclhn,bchpn,bhcl->bclhp', C, states, state_decay_out)\n",
        "\n",
        "    # Add output of intra-chunk and inter-chunk terms (diagonal and off-diagonal blocks)\n",
        "    Y = rearrange(Y_diag+Y_off, \"b c l h p -> b (c l) h p\")\n",
        "    return Y, final_state\n",
        "\n",
        "\n",
        "# Simple test\n",
        "def test_correctness():\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    ## Dimensions\n",
        "    # Denoted (B, T, Q, D, P) in the paper\n",
        "    batch, seqlen, chunk_size, dim, headdim = 1, 2048, 64, 2048, 64\n",
        "    nheads = dim // headdim  # (H) in the paper\n",
        "    ngroups = 1 # (G) in the paper\n",
        "    dstate = 64  # (N) in the paper\n",
        "    dtype = torch.float32\n",
        "    device = \"cuda\"\n",
        "\n",
        "    x = torch.randn(batch, seqlen, nheads, headdim, dtype=dtype, device=device)\n",
        "    dt = F.softplus(torch.randn(batch, seqlen, nheads, dtype=torch.float32, device=device) - 4).requires_grad_()\n",
        "    A = (-torch.exp(torch.rand(nheads, dtype=torch.float32, device=device))).requires_grad_()\n",
        "    B = torch.randn(batch, seqlen, ngroups, dstate, dtype=dtype, device=device)\n",
        "    C = torch.randn(batch, seqlen, ngroups, dstate, dtype=dtype, device=device)\n",
        "    D = torch.randn(nheads, dtype=dtype, device=device)\n",
        "\n",
        "    # Comparing fused version and minimal version\n",
        "    y = mamba_chunk_scan_combined(x, dt, A, B, C, chunk_size, D=None)\n",
        "    y_min, _ = ssd_minimal_discrete(x*dt.unsqueeze(-1), A*dt, B, C, chunk_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "A4-OhAewvjom"
      },
      "outputs": [],
      "source": [
        "# @title mixer_seq_simple\n",
        "# https://github.com/state-spaces/mamba/blob/main/mamba_ssm/models/mixer_seq_simple.py\n",
        "import math\n",
        "from functools import partial\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from mamba_ssm.models.config_mamba import MambaConfig\n",
        "from mamba_ssm.modules.mamba_simple import Mamba\n",
        "from mamba_ssm.modules.mamba2 import Mamba2\n",
        "# from mamba_ssm.modules.mha import MHA\n",
        "# from mamba_ssm.modules.mlp import GatedMLP\n",
        "# from mamba_ssm.modules.block import Block\n",
        "from mamba_ssm.utils.generation import GenerationMixin\n",
        "# from mamba_ssm.utils.hf import load_config_hf, load_state_dict_hf\n",
        "\n",
        "# try: from mamba_ssm.ops.triton.layer_norm import RMSNorm, layer_norm_fn, rms_norm_fn\n",
        "# except ImportError: RMSNorm, layer_norm_fn, rms_norm_fn = None, None, None\n",
        "\n",
        "import copy\n",
        "def create_block(\n",
        "    d_model,\n",
        "    d_intermediate,\n",
        "    ssm_cfg=None,\n",
        "    attn_layer_idx=None,\n",
        "    attn_cfg=None,\n",
        "    norm_epsilon=1e-5,\n",
        "    rms_norm=False,\n",
        "    residual_in_fp32=False,\n",
        "    fused_add_norm=False,\n",
        "    layer_idx=None,\n",
        "    device=None,\n",
        "    dtype=None,\n",
        "):\n",
        "    if ssm_cfg is None:\n",
        "        ssm_cfg = {}\n",
        "    if attn_layer_idx is None:\n",
        "        attn_layer_idx = []\n",
        "    if attn_cfg is None:\n",
        "        attn_cfg = {}\n",
        "    factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
        "    if layer_idx not in attn_layer_idx:\n",
        "        # Create a copy of the config to modify\n",
        "        ssm_cfg = copy.deepcopy(ssm_cfg) if ssm_cfg is not None else {}\n",
        "        ssm_layer = ssm_cfg.pop(\"layer\", \"Mamba1\")\n",
        "        if ssm_layer not in [\"Mamba1\", \"Mamba2\"]:\n",
        "            raise ValueError(f\"Invalid ssm_layer: {ssm_layer}, only support Mamba1 and Mamba2\")\n",
        "        mixer_cls = partial(\n",
        "            Mamba2 if ssm_layer == \"Mamba2\" else Mamba,\n",
        "            layer_idx=layer_idx,\n",
        "            **ssm_cfg,\n",
        "            **factory_kwargs\n",
        "        )\n",
        "    else:\n",
        "        mixer_cls = partial(MHA, layer_idx=layer_idx, **attn_cfg, **factory_kwargs)\n",
        "    norm_cls = partial(\n",
        "        nn.LayerNorm if not rms_norm else RMSNorm, eps=norm_epsilon, **factory_kwargs\n",
        "    )\n",
        "    if d_intermediate == 0:\n",
        "        mlp_cls = nn.Identity\n",
        "    else:\n",
        "        mlp_cls = partial(\n",
        "            GatedMLP, hidden_features=d_intermediate, out_features=d_model, **factory_kwargs\n",
        "        )\n",
        "    block = Block(\n",
        "        d_model,\n",
        "        mixer_cls,\n",
        "        mlp_cls,\n",
        "        norm_cls=norm_cls,\n",
        "        fused_add_norm=fused_add_norm,\n",
        "        residual_in_fp32=residual_in_fp32,\n",
        "    )\n",
        "    block.layer_idx = layer_idx\n",
        "    return block\n",
        "\n",
        "\n",
        "# https://github.com/huggingface/transformers/blob/c28d04e9e252a1a099944e325685f14d242ecdcd/src/transformers/models/gpt2/modeling_gpt2.py#L454\n",
        "def _init_weights(\n",
        "    module,\n",
        "    n_layer,\n",
        "    initializer_range=0.02,  # Now only used for embedding layer.\n",
        "    rescale_prenorm_residual=True,\n",
        "    n_residuals_per_layer=1,  # Change to 2 if we have MLP\n",
        "):\n",
        "    if isinstance(module, nn.Linear):\n",
        "        if module.bias is not None:\n",
        "            if not getattr(module.bias, \"_no_reinit\", False):\n",
        "                nn.init.zeros_(module.bias)\n",
        "    elif isinstance(module, nn.Embedding):\n",
        "        nn.init.normal_(module.weight, std=initializer_range)\n",
        "\n",
        "    if rescale_prenorm_residual:\n",
        "        # Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:\n",
        "        #   > A modified initialization which accounts for the accumulation on the residual path with model depth. Scale\n",
        "        #   > the weights of residual layers at initialization by a factor of 1/âˆšN where N is the # of residual layers.\n",
        "        #   >   -- GPT-2 :: https://openai.com/blog/better-language-models/\n",
        "        #\n",
        "        # Reference (Megatron-LM): https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py\n",
        "        for name, p in module.named_parameters():\n",
        "            if name in [\"out_proj.weight\", \"fc2.weight\"]:\n",
        "                # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block\n",
        "                # Following Pytorch init, except scale by 1/sqrt(2 * n_layer)\n",
        "                # We need to reinit p since this code could be called multiple times\n",
        "                # Having just p *= scale would repeatedly scale it down\n",
        "                nn.init.kaiming_uniform_(p, a=math.sqrt(5))\n",
        "                with torch.no_grad():\n",
        "                    p /= math.sqrt(n_residuals_per_layer * n_layer)\n",
        "\n",
        "\n",
        "class MixerModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model,\n",
        "        n_layer,\n",
        "        d_intermediate,\n",
        "        vocab_size,\n",
        "        ssm_cfg=None,\n",
        "        attn_layer_idx=None,\n",
        "        attn_cfg=None,\n",
        "        norm_epsilon: float = 1e-5,\n",
        "        rms_norm: bool = False,\n",
        "        initializer_cfg=None,\n",
        "        fused_add_norm=False,\n",
        "        residual_in_fp32=False,\n",
        "        device=None,\n",
        "        dtype=None,\n",
        "    ) -> None:\n",
        "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
        "        super().__init__()\n",
        "        self.residual_in_fp32 = residual_in_fp32\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model, **factory_kwargs)\n",
        "\n",
        "        # We change the order of residual and layer norm:\n",
        "        # Instead of LN -> Attn / MLP -> Add, we do:\n",
        "        # Add -> LN -> Attn / MLP / Mixer, returning both the residual branch (output of Add) and\n",
        "        # the main branch (output of MLP / Mixer). The model definition is unchanged.\n",
        "        # This is for performance reason: we can fuse add + layer_norm.\n",
        "        self.fused_add_norm = fused_add_norm\n",
        "        if self.fused_add_norm:\n",
        "            if layer_norm_fn is None or rms_norm_fn is None:\n",
        "                raise ImportError(\"Failed to import Triton LayerNorm / RMSNorm kernels\")\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "                create_block(\n",
        "                    d_model,\n",
        "                    d_intermediate=d_intermediate,\n",
        "                    ssm_cfg=ssm_cfg,\n",
        "                    attn_layer_idx=attn_layer_idx,\n",
        "                    attn_cfg=attn_cfg,\n",
        "                    norm_epsilon=norm_epsilon,\n",
        "                    rms_norm=rms_norm,\n",
        "                    residual_in_fp32=residual_in_fp32,\n",
        "                    fused_add_norm=fused_add_norm,\n",
        "                    layer_idx=i,\n",
        "                    **factory_kwargs,\n",
        "                )\n",
        "                for i in range(n_layer)])\n",
        "\n",
        "        self.norm_f = (nn.LayerNorm if not rms_norm else RMSNorm)(\n",
        "            d_model, eps=norm_epsilon, **factory_kwargs\n",
        "        )\n",
        "\n",
        "        self.apply(partial(_init_weights, n_layer=n_layer,\n",
        "                **(initializer_cfg if initializer_cfg is not None else {}),\n",
        "                n_residuals_per_layer=1 if d_intermediate == 0 else 2,  # 2 if we have MLP\n",
        "            ))\n",
        "\n",
        "    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):\n",
        "        return {\n",
        "            i: layer.allocate_inference_cache(batch_size, max_seqlen, dtype=dtype, **kwargs)\n",
        "            for i, layer in enumerate(self.layers)\n",
        "        }\n",
        "\n",
        "    def forward(self, input_ids, inference_params=None, **mixer_kwargs):\n",
        "        hidden_states = self.embedding(input_ids)\n",
        "        residual = None\n",
        "        for layer in self.layers:\n",
        "            hidden_states, residual = layer(hidden_states, residual, inference_params=inference_params, **mixer_kwargs)\n",
        "        if not self.fused_add_norm:\n",
        "            residual = (hidden_states + residual) if residual is not None else hidden_states\n",
        "            hidden_states = self.norm_f(residual.to(dtype=self.norm_f.weight.dtype))\n",
        "        else:\n",
        "            # Set prenorm=False here since we don't need the residual\n",
        "            hidden_states = layer_norm_fn(\n",
        "                hidden_states,\n",
        "                self.norm_f.weight,\n",
        "                self.norm_f.bias,\n",
        "                eps=self.norm_f.eps,\n",
        "                residual=residual,\n",
        "                prenorm=False,\n",
        "                residual_in_fp32=self.residual_in_fp32,\n",
        "                is_rms_norm=isinstance(self.norm_f, RMSNorm)\n",
        "            )\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "from collections import namedtuple\n",
        "class MambaLMHeadModel(nn.Module, GenerationMixin):\n",
        "    def __init__(self, config: MambaConfig, initializer_cfg=None, device=None, dtype=None,) -> None:\n",
        "        self.config = config\n",
        "        d_model = config.d_model\n",
        "        n_layer = config.n_layer\n",
        "        d_intermediate = config.d_intermediate\n",
        "        vocab_size = config.vocab_size\n",
        "        ssm_cfg = config.ssm_cfg\n",
        "        attn_layer_idx = config.attn_layer_idx\n",
        "        attn_cfg = config.attn_cfg\n",
        "        rms_norm = config.rms_norm\n",
        "        residual_in_fp32 = config.residual_in_fp32\n",
        "        fused_add_norm = config.fused_add_norm\n",
        "        pad_vocab_size_multiple = config.pad_vocab_size_multiple\n",
        "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
        "\n",
        "        super().__init__()\n",
        "        if vocab_size % pad_vocab_size_multiple != 0:\n",
        "            vocab_size += pad_vocab_size_multiple - (vocab_size % pad_vocab_size_multiple)\n",
        "        self.backbone = MixerModel(\n",
        "            d_model=d_model,\n",
        "            n_layer=n_layer,\n",
        "            d_intermediate=d_intermediate,\n",
        "            vocab_size=vocab_size,\n",
        "            ssm_cfg=ssm_cfg,\n",
        "            attn_layer_idx=attn_layer_idx,\n",
        "            attn_cfg=attn_cfg,\n",
        "            rms_norm=rms_norm,\n",
        "            initializer_cfg=initializer_cfg,\n",
        "            fused_add_norm=fused_add_norm,\n",
        "            residual_in_fp32=residual_in_fp32,\n",
        "            **factory_kwargs,\n",
        "        )\n",
        "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False, **factory_kwargs)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.apply(partial(_init_weights, n_layer=n_layer,\n",
        "                **(initializer_cfg if initializer_cfg is not None else {}),\n",
        "            ))\n",
        "        self.tie_weights()\n",
        "\n",
        "    def tie_weights(self):\n",
        "        if self.config.tie_embeddings:\n",
        "            self.lm_head.weight = self.backbone.embedding.weight\n",
        "\n",
        "    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):\n",
        "        return self.backbone.allocate_inference_cache(batch_size, max_seqlen, dtype=dtype, **kwargs)\n",
        "\n",
        "    def forward(self, input_ids, position_ids=None, inference_params=None, num_last_tokens=0, **mixer_kwargs):\n",
        "        \"\"\"\n",
        "        \"position_ids\" is just to be compatible with Transformer generation. We don't use it.\n",
        "        num_last_tokens: if > 0, only return the logits for the last n tokens\n",
        "        \"\"\"\n",
        "        hidden_states = self.backbone(input_ids, inference_params=inference_params, **mixer_kwargs)\n",
        "        if num_last_tokens > 0:\n",
        "            hidden_states = hidden_states[:, -num_last_tokens:]\n",
        "        lm_logits = self.lm_head(hidden_states)\n",
        "        CausalLMOutput = namedtuple(\"CausalLMOutput\", [\"logits\"])\n",
        "        return CausalLMOutput(logits=lm_logits)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# model_path = os.path.join(save_directory, 'pytorch_model.bin')\n",
        "# torch.save(self.state_dict(), model_path)\n",
        "# model.load_state_dict(load_state_dict_hf(pretrained_model_name, device=device, dtype=dtype))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npuIYJEhUie_"
      },
      "source": [
        "## basement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "B-CEHIj35vPb"
      },
      "outputs": [],
      "source": [
        "# @title seq conv lcse\n",
        "# ht = at*ht-1 + bt\n",
        "\n",
        "def seq(at, bt, h0=None): # [t,b,d], [t,b,d], [b,d]\n",
        "    h = 0 if h0==None else h0\n",
        "    ht = []\n",
        "    # for t in range(at.size(0)):\n",
        "    for t in range(at.size(1)):\n",
        "        # h = at[t] * h + bt[t]\n",
        "        h = at[:,t] * h + bt[:,t]\n",
        "        ht.append(h)\n",
        "    # ht = torch.stack(ht, dim=0)\n",
        "    ht = torch.stack(ht, dim=1)\n",
        "    return ht\n",
        "\n",
        "# conv\n",
        "# h1=a1*h0+b1 =       a1*h0 +       b1\n",
        "# h2=a2*h1+b2 =    a2*a1*h0 +    a2*b1 +    b2\n",
        "# h3=a3*h2+b3 = a3*a2*a1*h0 + a3*a2*b1 + a3*b2 + b3\n",
        "\n",
        "# h1/a1   = h0 + b1/a1\n",
        "# h2/a12  = h0 + b1/a1 + b2/a12\n",
        "# h3/a123 = h0 + b1/a1 + b2/a12 + b3/a123\n",
        "\n",
        "def conv(at, bt, h0=None): # [t,b,d], [t,b,d], [b,d]\n",
        "    # a123 = torch.cumprod(at, dim=0) # [t,b,d]\n",
        "    # ht = torch.cumsum(bt/a123, dim=0) # cusum b1/a1, b2/a12, b3/a123, ...\n",
        "    a123 = torch.cumprod(at, dim=1) # [t,b,d]\n",
        "    ht = torch.cumsum(bt/a123, dim=1) # cusum b1/a1, b2/a12, b3/a123, ...\n",
        "    ht = ((h0 if h0!=None else 0)+ht)*a123\n",
        "    return ht\n",
        "\n",
        "# ht = A* ht-1 + B*ut ; yt = C *ht + D*ut\n",
        "# let kernel K: Km = CA^mB\n",
        "\n",
        "# K = C @ A**torch.arange(1,t+1) @ B\n",
        "# u = F.pad(u, (t-1, 0))\n",
        "# y = F.conv1d(u, K) + D*u\n",
        "\n",
        "# fft\n",
        "# L_fft = 2 * T\n",
        "# uf = torch.fft.rfft(u, n=L_fft)            # (B, Din, L_fft//2+1)\n",
        "# kf = torch.fft.rfft(K, n=L_fft)            # (Dout, Din, L_fft//2+1)\n",
        "# # Multiply + sum over input channels\n",
        "# # out[b, dout, f] = sum_{din} uf[b,din,f] * kf[dout,din,f]\n",
        "# yf = torch.einsum(\"bdf, odf -> bof\", uf, kf)\n",
        "# y = torch.fft.irfft(yf, n=L_fft)[..., :T]   # causal output\n",
        "\n",
        "\n",
        "# lcse\n",
        "# computing the sequence ht = at*ht-1 + bt\n",
        "# xt = e^(a*_t +tail(LCSE(cat(log x0, log b_t -a*_t))))\n",
        "# b*_t = Sum t] e^( log bt - a*_t)\n",
        "# a*_t = Sum t] log a_t\n",
        "def lcse(at, bt, h0=None): # [t,b,d], [t,b,d], [b,d]\n",
        "    if h0!=None:\n",
        "        # x_in = torch.cat([h0.log().unsqueeze(0), bt.log()], dim=0) # [1+t,b,d]\n",
        "        # at = torch.cat([torch.zeros(1,*at.shape[1:]), torch.cumsum(at.log(), dim=0)], dim=0) # a*_t # [1+t,b,d]\n",
        "        # ht = torch.exp(at[1:] + torch.logcumsumexp(x_in - at, dim=0)[1:])\n",
        "        x_in = torch.cat([h0.log().unsqueeze(1), bt.log()], dim=1) # [b,1+t,d]\n",
        "        at = torch.cat([torch.zeros(at.shape[0],1,*at.shape[2:]), torch.cumsum(at.log(), dim=1)], dim=1) # a*_t # [1+t,b,d]\n",
        "        ht = torch.exp(at[:,1:] + torch.logcumsumexp(x_in - at, dim=1)[:,1:])\n",
        "    else:\n",
        "        # at = torch.cumsum(at.log(), dim=0) # a*_t # [t,b,d]\n",
        "        # ht = torch.exp(at + torch.logcumsumexp(bt.log() - at, dim=0))\n",
        "        at = torch.cumsum(at.log(), dim=1) # a*_t # [t,b,d]\n",
        "        ht = torch.exp(at + torch.logcumsumexp(bt.log() - at, dim=1))\n",
        "    return ht\n",
        "\n",
        "\n",
        "t, b, d = 30, 4, 5\n",
        "# dA = torch.exp(dt*A)[...,None,None] # [b,t,h,1,1]\n",
        "# dBx = torch.einsum(\"bth,btgs,bthd->bthds\", dt, B, x)\n",
        "# at = torch.randn(b,t,d)\n",
        "# bt = torch.randn(b,t,d)\n",
        "at = torch.randn(b,t,d,1,1).exp()\n",
        "bt = torch.randn(b,t,d,d,d)\n",
        "\n",
        "h0 = torch.randn(b,d, dtype=torch.complex64)\n",
        "h0 = None\n",
        "ht = seq(at, bt, h0)\n",
        "# ht = seq(at+0j, bt+0j, h0)\n",
        "# ht1 = conv(at, bt, h0)\n",
        "# ht1 = lcse(at, bt, h0)\n",
        "ht1 = lcse(at+0j, bt+0j, h0).real\n",
        "print(abs(ht-ht1).sum())\n",
        "# print(ht[:3,0,:5])\n",
        "# print(ht1[:3,0,:5])\n",
        "\n",
        "# cpu; t, b, d = 200, 64, 64\n",
        "# %timeit seq(at, bt, h0) # 6.16 ms 6.09 6.25\n",
        "# %timeit conv(at, bt, h0) # 25.6 ms 24.2\n",
        "# %timeit lcse(at, bt, h0) # 344 ms 348 ms\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DriHnUWNB9wH"
      },
      "outputs": [],
      "source": [
        "# @title test lcse\n",
        "def lcse(at, bt, h0=None): # [t,b,d], [t,b,d], [b,d]\n",
        "    if h0!=None:\n",
        "        # x_in = torch.cat([h0.log().unsqueeze(1), bt.log()], dim=1) # [b,1+t,d]\n",
        "        # at = torch.cat([torch.zeros(at.shape[0],1,*at.shape[2:]), torch.cumsum(at.log(), dim=1)], dim=1) # a*_t # [1+t,b,d]\n",
        "        # ht = torch.exp(at[:,1:] + torch.logcumsumexp(x_in - at, dim=1)[:,1:])\n",
        "        at = torch.cumsum(at.log(), dim=1) # a*_t # [b,t,d]\n",
        "        ht = torch.exp(at + torch.logcumsumexp(torch.cat([h0.log(), bt.log() - at], dim=1))[:,1:])\n",
        "    else:\n",
        "        at = torch.cumsum(at.log(), dim=1) # a*_t # [b,t,d]\n",
        "        # print(at[:3,0,:5])\n",
        "        ht = torch.exp(at + torch.logcumsumexp(bt.log() - at, dim=1))\n",
        "        # ht = torch.exp(at + torch.logcumsumexp(bt.log() - at))\n",
        "    return ht\n",
        "\n",
        "\n",
        "t, b, d = 30, 4, 5\n",
        "# t, b, d = 200, 64, 64\n",
        "at = torch.randn(b,t,d)*10\n",
        "bt = torch.randn(b,t,d)*10\n",
        "\n",
        "h0 = torch.randn(b,d, dtype=torch.complex64)\n",
        "h0 = None\n",
        "ht = seq(at, bt, h0)\n",
        "# ht = seq(at+0j, bt+0j, h0)\n",
        "# ht1 = lcse(at, bt, h0)\n",
        "ht1 = lcse(at+0j, bt+0j, h0).real\n",
        "print(abs(ht-ht1).sum())\n",
        "print(ht[:3,0,:5])\n",
        "print(ht1[:3,0,:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kQesLf6pvBV5"
      },
      "outputs": [],
      "source": [
        "# @title mamba seq lcse\n",
        "# x:[b,t,h,d], B/C:[b,t,g,s]\n",
        "# A:[h], dt:[b,t,h]\n",
        "\n",
        "b,t,h,d = 3,256,4,4\n",
        "g,s = 4,4\n",
        "x = torch.randn(b,t,h,d)\n",
        "B = torch.randn(b,t,g,s)\n",
        "C = torch.randn(b,t,g,s)\n",
        "A = torch.randn(h)\n",
        "dt = torch.randn(b,t,h)\n",
        "\n",
        "\n",
        "# y, h_ssm = ssd(x*dt.unsqueeze(-1), A*dt, B, C, 64, h0) # 256\n",
        "# y, h_ssm = ssd(x*dt.unsqueeze(-1), A*dt, B, C, 64) # 256\n",
        "# print(y.shape, h_ssm.shape)\n",
        "\n",
        "# dA = torch.exp(dt*A) # [b,n_heads] # https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/triton/selective_state_update.py#L104\n",
        "# # print('dt, B, x', dt.shape, B.shape, x.shape)\n",
        "# dBx = torch.einsum(\"bh,bhs,bhd->bhds\", dt, B, x)\n",
        "# if h==None: h_ssm = dBx\n",
        "# else: h_ssm = h[1] * dA[...,None,None] + dBx # [b,h,d,s]*[b,h,1,1]+[b,h,d,s]\n",
        "# # print('h.ssm_state, C', h.ssm_state.shape, C.shape)\n",
        "\n",
        "# h = at[t] * h + bt[t]\n",
        "\n",
        "\n",
        "dA = torch.exp(dt*A)[...,None,None] # [b,t,h,1,1]\n",
        "dBx = torch.einsum(\"bth,btgs,bthd->bthds\", dt, B, x)\n",
        "# print(dA.shape, dBx.shape)\n",
        "print(dA[0,:15].squeeze())\n",
        "# dA = torch.randn(b,t,h,1,1)\n",
        "# dBx = torch.randn(b,t,h,d,s)\n",
        "\n",
        "# h: [b,h,d,s]\n",
        "# h = at[t] * h + bt[t]\n",
        "# print(dA[:,:5])\n",
        "# print(dBx[:,:5])\n",
        "\n",
        "\n",
        "h_ssm = seq(dA, dBx, h0=None) # [b,t,d], [b,t,d], [b,d]\n",
        "# h_ssm = seq(dA+0j, dBx+0j, h0=None) # [b,t,d], [b,t,d], [b,d]\n",
        "print(h_ssm.shape)\n",
        "# h_ssm1 = lcse(dA+0j, dBx+0j, h0=None) # [b,t,d], [b,t,d], [b,d]\n",
        "h_ssm1 = lcse(dA+0j, dBx+0j, h0=None).real # [b,t,d], [b,t,d], [b,d]\n",
        "# print(h_ssm.shape)\n",
        "# print(h_ssm[0,:3])\n",
        "# print(h_ssm1[0,:3])\n",
        "print(abs(h_ssm-h_ssm1).sum())\n",
        "# print((h_ssm-h_ssm1)[0,:3])\n",
        "# print((h_ssm-h_ssm1)[0])\n",
        "\n",
        "# # y = torch.einsum(\"bhds,bgs->bhd\", h_ssm, C) # [b,h,d,s]*[b,h,s]\n",
        "y1 = torch.einsum(\"bthds,btgs->bthd\", h_ssm, C) # [b,h,d,s]*[b,h,s]\n",
        "# print(y1.shape)\n",
        "# print(y[0,:3])\n",
        "# print(y1[0,:3])\n",
        "# print(abs(y-y1).sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRDECggjQymm"
      },
      "outputs": [],
      "source": [
        "# @title huggingface mamba2\n",
        "# https://huggingface.co/docs/transformers/en/model_doc/mamba2\n",
        "# https://github.com/huggingface/transformers/blob/main/src/transformers/models/mamba2/modeling_mamba2.py#L825\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import Mamba2Config, Mamba2Model\n",
        "\n",
        "# import torch.nn as nn\n",
        "# class Id(nn.Module):\n",
        "#     def __init__(self): super().__init__()\n",
        "#     def forward(self, x): return x\n",
        "# pred.embeddings = Id()\n",
        "\n",
        "batch=4\n",
        "seq_len=5\n",
        "in_dim=132\n",
        "d_model=128#256\n",
        "# num_heads*head_dim=2*hidden_size ?\n",
        "\n",
        "config = Mamba2Config(hidden_size=d_model, num_heads=8, head_dim=32, state_size=128, num_hidden_layers=1, output_hidden_states=True, vocab_size=0)\n",
        "pred = Mamba2Model(config)\n",
        "pred.embeddings = nn.Linear(in_dim, d_model)\n",
        "\n",
        "x = torch.randn(batch, seq_len, in_dim)\n",
        "\n",
        "# out = pred(inputs_embeds=x, use_cache=True)\n",
        "out = pred(x, use_cache=True)\n",
        "y, cache = out.last_hidden_state, out.cache_params\n",
        "# print(x)\n",
        "# print(y)\n",
        "print(x.shape) # [batch, seq_len, d_model]\n",
        "print(y.shape) # [batch, seq_len, d_model]\n",
        "# print(len(h)) # num_hidden_layers + 1 ?\n",
        "# h = torch.cat(out.hidden_states, dim=-1) # [batch, seq_len, (num_hidden_layers + 1 ?) * d_model]\n",
        "# print(h.shape)\n",
        "print(cache)\n",
        "\n",
        "#  if key/value cache contains 10 tokens (no matter how many of it is a pad token), the cache position for the next token should be torch.tensor([10]) https://huggingface.co/docs/transformers/main/en/kv_cache\n",
        "# out = pred(inputs_embeds=x, cache_params=cache, use_cache=True, cache_position=x.shape[1]) # torch.tensor([10])\n",
        "out = pred(x, cache_params=cache, use_cache=True, cache_position=x.shape[1]) # torch.tensor([10])\n",
        "y, cache = out.last_hidden_state, out.cache_params\n",
        "\n",
        "print(y.shape)\n",
        "print(len(h)) # num_hidden_layers + 1 ?\n",
        "# print(h.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "x = torch.randn(batch, seq_len, in_dim)\n",
        "out = pred(x, use_cache=True)\n",
        "y, cache = out.last_hidden_state, out.cache_params\n",
        "h = torch.cat(out.hidden_states, dim=-1) # [batch, seq_len, (num_hidden_layers + 1 ?) * d_model]\n",
        "out = pred(x, cache_params=cache, use_cache=True, cache_position=x.shape[1]) # torch.tensor([10])\n",
        "y, cache = out.last_hidden_state, out.cache_params\n",
        "\n",
        "\n",
        "# Mamba2Output\n",
        "# Mamba2Cache\n",
        "pred = Mamba2Model(Mamba2Config(hidden_size=d_model, num_heads=8, head_dim=32, state_size=128, num_hidden_layers=1, output_hidden_states=True, vocab_size=0))\n",
        "\n",
        "self.ssm_states = {\n",
        "    i: torch.zeros(batch_size, config.num_heads, config.head_dim, config.state_size, device=device, dtype=dtype)\n",
        "    for i in range(config.num_hidden_layers)\n",
        "}\n",
        "\n",
        "\n",
        "# x = torch.randn(4*batch, seq_len, in_dim)\n",
        "# out = pred(x, cache_params=cache, use_cache=True, cache_position=x.shape[1]) # torch.tensor([10])\n",
        "# y, cache = out.last_hidden_state, out.cache_params\n",
        "\n",
        "# print(cache)\n",
        "# print(cache.ssm_states)\n",
        "# print(len(cache.ssm_states))\n",
        "\n",
        "# config = Mamba2Config(hidden_size=d_model, num_heads=8, head_dim=32, state_size=128, num_hidden_layers=1, output_hidden_states=True, vocab_size=0)\n",
        "# print(config.num_heads, config.head_dim, config.state_size)\n",
        "# print(config.num_hidden_layers)\n",
        "x = x.detach().repeat(4,1,1,1) # [batch, num_heads, head_dim, state_size]\n",
        "for i,v in cache.ssm_states.items():\n",
        "    print(i,v.shape)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HzzZKHYU8wGz"
      },
      "outputs": [],
      "source": [
        "# @title johnma2006/mamba-minimal\n",
        "# https://github.com/johnma2006/mamba-minimal/blob/master/model.py\n",
        "\"\"\"Simple, minimal implementation of Mamba in one file of PyTorch.\n",
        "    [1] Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Albert Gu and Tri Dao) https://arxiv.org/abs/2312.00752\n",
        "    [2] The Annotated S4 (Sasha Rush and Sidd Karamcheti) https://srush.github.io/annotated-s4\n",
        "\n",
        "Glossary:\n",
        "    b: batch size                       (`B` in Mamba paper [1] Algorithm 2)\n",
        "    l: sequence length                  (`L` in [1] Algorithm 2)\n",
        "    d or d_model: hidden dim\n",
        "    n or d_state: latent state dim      (`N` in [1] Algorithm 2)\n",
        "    expand: expansion factor            (`E` in [1] Section 3.4)\n",
        "    d_in or d_inner: d * expand         (`D` in [1] Algorithm 2)\n",
        "    A, B, C, D: state space parameters  (See any state space representation formula)\n",
        "                                        (B, C are input-dependent (aka selective, a key innovation in Mamba); A, D are not)\n",
        "    Î” or delta: input-dependent step size\n",
        "    dt_rank: rank of Î”                  (See [1] Section 3.6 \"Parameterization of âˆ†\")\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Mamba(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "        # self.embedding = nn.Embedding(args.vocab_size, args.d_model)\n",
        "        self.layers = nn.ModuleList([ResidualBlock(args) for _ in range(args.n_layer)])\n",
        "        self.norm_f = nn.RMSNorm(args.d_model)\n",
        "        # self.lm_head = nn.Linear(args.d_model, args.vocab_size, bias=False)\n",
        "        # self.lm_head.weight = self.embedding.weight  # Tie output projection to embedding weights. # See \"Weight Tying\" paper\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\" Args: input_ids (long tensor): shape (b, l)    (See Glossary at top for definitions of b, l, d_in, n...)\n",
        "        Returns: logits: shape (b, l, vocab_size)\n",
        "        Official Implementation: class MambaLMHeadModel, https://github.com/state-spaces/mamba/blob/main/mamba_ssm/models/mixer_seq_simple.py#L173\"\"\"\n",
        "        # x = self.embedding(input_ids)\n",
        "        x = input_ids\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        x = self.norm_f(x)\n",
        "        # logits = self.lm_head(x)\n",
        "        logits = x\n",
        "        return logits\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        \"\"\"Simple block wrapping Mamba block with normalization and residual connection.\"\"\"\n",
        "        super().__init__()\n",
        "        # self.args = args\n",
        "        self.mixer = MambaBlock(args)\n",
        "        self.norm = nn.RMSNorm(args.d_model)\n",
        "\n",
        "    def forward(self, x): # (b, l, d)\n",
        "        \"\"\"Official Implementation: Block.forward(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py#L297\n",
        "            Note: the official repo chains residual blocks that look like\n",
        "                [Add -> Norm -> Mamba] -> [Add -> Norm -> Mamba] -> [Add -> Norm -> Mamba] -> ...\n",
        "            where the first Add is a no-op. This is purely for performance reasons as this\n",
        "            allows them to fuse the Add->Norm.\n",
        "            We instead implement our blocks as the more familiar, simpler, and numerically equivalent\n",
        "                [Norm -> Mamba -> Add] -> [Norm -> Mamba -> Add] -> [Norm -> Mamba -> Add] -> ....\"\"\"\n",
        "        output = x + self.mixer(self.norm(x))\n",
        "        return output # (b, l, d)\n",
        "\n",
        "\n",
        "from einops import rearrange, repeat, einsum\n",
        "class MambaBlock(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        \"\"\"A single Mamba block, as described in Figure 3 in Section 3.4 in the Mamba paper [1].\"\"\"\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "        self.in_proj = nn.Linear(args.d_model, args.d_inner * 2, bias=args.bias)\n",
        "        self.conv1d = nn.Conv1d(in_channels=args.d_inner, out_channels=args.d_inner, bias=args.conv_bias,\n",
        "            kernel_size=args.d_conv, groups=args.d_inner, padding=args.d_conv - 1,)\n",
        "\n",
        "        self.x_proj = nn.Linear(args.d_inner, args.dt_rank + args.d_state * 2, bias=False) # takes in `x` and outputs the input-specific Î”, B, C\n",
        "        self.dt_proj = nn.Linear(args.dt_rank, args.d_inner, bias=True) # project Î” from dt_rank to d_in\n",
        "\n",
        "        A = repeat(torch.arange(1, args.d_state + 1), 'n -> d n', d=args.d_inner)\n",
        "        self.A_log = nn.Parameter(torch.log(A))\n",
        "        self.D = nn.Parameter(torch.ones(args.d_inner))\n",
        "        self.out_proj = nn.Linear(args.d_inner, args.d_model, bias=args.bias)\n",
        "\n",
        "\n",
        "    def forward(self, x): # (b, l, d)\n",
        "        \"\"\"Mamba block forward. This looks the same as Figure 3 in Section 3.4 in the Mamba paper [1].\n",
        "        Official Implementation: class Mamba, https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py#L119\n",
        "            mamba_inner_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L311\"\"\"\n",
        "        b, l, d = x.shape\n",
        "        x_res = self.in_proj(x)  # shape (b, l, 2 * d_in)\n",
        "        x, res = x_res.split([self.args.d_inner, self.args.d_inner], dim=-1)\n",
        "        x = self.conv1d(x.permute(0, 2, 1))[:, :, :l].permute(0, 2, 1) # (b, l, d_in)\n",
        "        y = self.ssm(F.silu(x)) * F.silu(res)\n",
        "        output = self.out_proj(y)\n",
        "        return output # (b, l, d)\n",
        "\n",
        "\n",
        "    def ssm(self, x): # (b, l, d_in)\n",
        "        \"\"\"- Algorithm 2 in Section 3.2 in the Mamba paper [1]\n",
        "            - run_SSM(A, B, C, u) in The Annotated S4 [2]\n",
        "        Official Implementation: mamba_inner_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L311\"\"\"\n",
        "        d_in, n = self.A_log.shape\n",
        "        # Compute âˆ† A B C D, the state space parameters.\n",
        "        #     A, D are input independent (see Mamba paper [1] Section 3.5.2 \"Interpretation of A\" for why A isn't selective)\n",
        "        #     âˆ†, B, C are input-dependent (this is a key difference between Mamba and the linear time invariant S4, and is why Mamba is called **selective** state spaces)\n",
        "        A = -torch.exp(self.A_log.float())  # shape (d_in, n)\n",
        "        D = self.D.float()\n",
        "\n",
        "        # 2: B : (B, L, N) â† sB (x)\n",
        "        # 3: C : (B, L, N) â† sC (x)\n",
        "        # 4: Î” : (B, L, D) â† ðœÎ”(Parameter+ð‘ Î”(x))\n",
        "        # 5: A, B : (B, L, D, N) â† discretize(Î”, A, B)\n",
        "        # 6: y â† SSM(A, B, C) (x)\n",
        "\n",
        "        x_dbl = self.x_proj(x)  # (b, l, dt_rank + 2*n)\n",
        "        delta, B, C = x_dbl.split([self.args.dt_rank, n, n], dim=-1)  # delta: (b, l, dt_rank). B, C: (b, l, n)\n",
        "        delta = F.softplus(self.dt_proj(delta)) # (b, l, d_in)\n",
        "        y = self.selective_scan(x, delta, A, B, C, D)  # This is similar to run_SSM(A, B, C, u) in The Annotated S4 [2]\n",
        "        return y # (b, l, d_in)\n",
        "\n",
        "\n",
        "    def selective_scan(self, u, delta, A, B, C, D): # u: (b, l, d_in); delta: (b, l, d_in); A: (d_in, n); B: (b, l, n); C: (b, l, n); D: (d_in,)\n",
        "        \"\"\"- Section 2 State Space Models in the Mamba paper [1]\n",
        "            - Algorithm 2 in Section 3.2 in the Mamba paper [1]\n",
        "            - run_SSM(A, B, C, u) in The Annotated S4 [2]\n",
        "\n",
        "            x(t+1) = Ax(t) + Bu(t)\n",
        "            y(t)   = Cx(t) + Du(t)\n",
        "        except B and C (and the step size delta, which is used for discretization) are dependent on the input x(t).\n",
        "\n",
        "        Official Implementation: selective_scan_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L86\n",
        "            Note: I refactored some parts out of `selective_scan_ref` out, so the functionality doesn't match exactly.\"\"\"\n",
        "        b, l, d_in = u.shape\n",
        "        n = A.shape[1]\n",
        "\n",
        "        # Discretize continuous parameters (A, B)\n",
        "        deltaA = torch.exp(einsum(delta, A, 'b l d_in, d_in n -> b l d_in n')) # - A is discretized using zero-order hold (ZOH) discretization (see Section 2 Equation 4 in the Mamba paper [1])\n",
        "        deltaB_u = einsum(delta, B, u, 'b l d_in, b l n, b l d_in -> b l d_in n') # - B is discretized using a simplified Euler discretization instead of ZOH. From a discussion with authors: \"A is the more important term and the performance doesn't change much with the simplification on B\"\n",
        "\n",
        "        # Perform selective scan (see scan_SSM() in The Annotated S4 [2])\n",
        "        # Note that the below is sequential, while the official implementation does a much faster parallel scan that is additionally hardware-aware (like FlashAttention).\n",
        "        x = torch.zeros((b, d_in, n), device=deltaA.device)\n",
        "        ys = []\n",
        "        for i in range(l):\n",
        "            x = deltaA[:, i] * x + deltaB_u[:, i]\n",
        "            y = einsum(x, C[:, i, :], 'b d_in n, b n -> b d_in')\n",
        "            ys.append(y)\n",
        "        y = torch.stack(ys, dim=1)  # shape (b, l, d_in)\n",
        "        y = y + u * D\n",
        "        return y # (b, l, d_in)\n",
        "\n",
        "\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "@dataclass\n",
        "class ModelArgs:\n",
        "    d_model = 256\n",
        "    n_layer = 1\n",
        "    # vocab_size = 0\n",
        "    d_state = 16\n",
        "    expand = 2\n",
        "    d_conv = 4\n",
        "    # pad_vocab_size_multiple = 8\n",
        "    conv_bias = True\n",
        "    bias = False\n",
        "    d_inner = int(expand * d_model)\n",
        "    dt_rank = math.ceil(d_model / 16)\n",
        "\n",
        "\n",
        "pred = Mamba(ModelArgs)\n",
        "\n",
        "batch=4\n",
        "seq_len=500\n",
        "d_model=256\n",
        "x = torch.randn(batch, seq_len, d_model)\n",
        "y = pred(x)\n",
        "# print(x)\n",
        "# print(y)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iz66JEgroHjJ"
      },
      "outputs": [],
      "source": [
        "# print(h)\n",
        "# for j in h:\n",
        "#     print(j.shape)\n",
        "\n",
        "print(cache)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WcawLsIkRFm6"
      },
      "outputs": [],
      "source": [
        "# @title test einops\n",
        "# from einops import repeat, einsum\n",
        "# b,l,d_in,n = 4, 500, 256, 256\n",
        "# # einsum(A,B,'b l d, b l d -> b l d')\n",
        "\n",
        "# delta = torch.randn(b,l,d_in)\n",
        "# B = torch.randn(b,l,n)\n",
        "# u = torch.randn(b,l,d_in)\n",
        "# x0=einsum(delta, B, u, 'b l d_in, b l n, b l d_in -> b l d_in n')\n",
        "# x1=torch.einsum('bld,bln,bld->bldn', delta, B, u)\n",
        "# print((x0-x1).sum())\n",
        "\n",
        "# A=torch.randn(b,d_in,n)\n",
        "# C=torch.randn(b,d_in)\n",
        "# y0 = einsum(A, C, 'b d_in n, b n -> b d_in')\n",
        "# y1 = torch.einsum('bdn,bn->bd', A, C)\n",
        "# print((y0-y1).sum())\n",
        "\n",
        "\n",
        "# A = repeat(torch.arange(1, n + 1), 'n -> d n', d=d_in)\n",
        "# print(A)\n",
        "# A1 = torch.arange(1, n + 1).repeat(d_in, 1)\n",
        "# print((A-A1).sum())\n",
        "\n",
        "# %timeit x0=einsum(delta, B, u, 'b l d_in, b l n, b l d_in -> b l d_in n') # 304 ms, 300\n",
        "# %timeit x1=torch.einsum('bld,bln,bld->bldn', delta, B, u) # 303 ms, 301, 301\n",
        "# %timeit y0 = einsum(A, C, 'b d_in n, b n -> b d_in') # 159 Âµs, 160\n",
        "# %timeit y1 = torch.einsum('bdn,bn->bd', A, C) # 155, 146\n",
        "# %timeit pred(x) # ein 862ms, 866, 977, 984 ; torch 849, 959, 867, 924\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "yPX5CP0PWM6K",
        "5XQqp-uYIMWs",
        "npuIYJEhUie_"
      ],
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMdIFzr5XcR86fWcJ+HW4mk",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}