{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIWTelPyjQM0oRTlrx5W0u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/ssm/blob/main/LRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pytorch port of associative scan\n",
        "#@title PyTorch associative/parallel scan\n",
        "# Taken from https://github.com/i404788/s5-pytorch/blob/c74be7270fe2ec9dc13efcffcfd7f5355d884030/s5/jax_compat.py\n",
        "import torch\n",
        "from torch.utils._pytree import tree_flatten, tree_unflatten\n",
        "from typing import overload, Callable, Iterable, List, TypeVar, Any,Tuple\n",
        "from functools import partial\n",
        "\n",
        "\"\"\"\n",
        "Jax-Pytorch ported functions, mostly interfaces are kept the same but unsupported features are removed:\n",
        "* Jax-Keyed RNGs are sampled from global RNG\n",
        "* Canonical/Named shapes/dtypes/etc are now regular shapes,dtypes\n",
        "\"\"\"\n",
        "\n",
        "'''\n",
        "T = TypeVar(\"T\")\n",
        "T1 = TypeVar(\"T1\")\n",
        "T2 = TypeVar(\"T2\")\n",
        "T3 = TypeVar(\"T3\")\n",
        "@overload\n",
        "def safe_map(f: Callable[[T1], T], __arg1: Iterable[T1]) -> List[T]: ...\n",
        "@overload\n",
        "def safe_map(f: Callable[[T1, T2], T], __arg1: Iterable[T1], __arg2: Iterable[T2]) -> List[T]: ...\n",
        "@overload\n",
        "def safe_map(f: Callable[[T1, T2, T3], T], __arg1: Iterable[T1], __arg2: Iterable[T2], __arg3: Iterable[T3]) -> List[T]: ...\n",
        "@overload\n",
        "def safe_map(f: Callable[..., T], __arg1: Iterable[Any], __arg2: Iterable[Any], __arg3: Iterable[Any], __arg4: Iterable[Any], *args) -> List[T]: ...\n",
        "'''\n",
        "# def safe_map(f, *args):\n",
        "#     args = list(map(list, args))\n",
        "#     n = len(args[0])\n",
        "#     for arg in args[1:]:\n",
        "#         assert len(arg) == n, f'length mismatch: {list(map(len, args))}'\n",
        "#     return list(map(f, *args))\n",
        "\n",
        "def combine(tree, operator, a_flat, b_flat):\n",
        "    # Lower `fn` to operate on flattened sequences of elems.\n",
        "    a = tree_unflatten(a_flat, tree)\n",
        "    b = tree_unflatten(b_flat, tree)\n",
        "    c = operator(a, b)\n",
        "    c_flat, _ = tree_flatten(c)\n",
        "    return c_flat\n",
        "\n",
        "def _scan(tree, operator, elems, axis):\n",
        "    \"\"\"Perform scan on `elems`.\"\"\"\n",
        "    num_elems = elems[0].shape[axis]\n",
        "    if num_elems < 2: return elems\n",
        "    # Combine adjacent pairs of elements.\n",
        "    reduced_elems = combine(tree, operator, [torch.ops.aten.slice(elem, axis, 0, -1, 2) for elem in elems],\n",
        "                            [torch.ops.aten.slice(elem, axis, 1, None, 2) for elem in elems])\n",
        "\n",
        "    # Recursively compute scan for partially reduced tensors.\n",
        "    odd_elems = _scan(tree, operator, reduced_elems, axis)\n",
        "\n",
        "    if num_elems % 2 == 0:\n",
        "        even_elems = combine(tree, operator, [torch.ops.aten.slice(e, axis, 0, -1) for e in odd_elems],\n",
        "                             [torch.ops.aten.slice(e, axis, 2, None, 2) for e in elems])\n",
        "    else:\n",
        "        even_elems = combine(tree, operator, odd_elems,\n",
        "                             [torch.ops.aten.slice(e, axis, 2, None, 2) for e in elems])\n",
        "    # The first element of a scan is the same as the first element\n",
        "    # of the original `elems`.\n",
        "    even_elems = [\n",
        "        torch.cat([torch.ops.aten.slice(elem, axis, 0, 1), result], dim=axis)\n",
        "        if result.shape.numel() > 0 and elem.shape[axis] > 0 else\n",
        "        result if result.shape.numel() > 0 else\n",
        "        torch.ops.aten.slice(elem, axis, 0, 1)  # Jax allows/ignores concat with 0-dim, Pytorch does not\n",
        "        for (elem, result) in zip(elems, even_elems)]\n",
        "    # return list(safe_map(partial(_interleave, axis=axis), even_elems, odd_elems))\n",
        "    return list(list(map(partial(_interleave, axis=axis), even_elems, odd_elems)))\n",
        "\n",
        "\n",
        "def associative_scan(operator: Callable, elems, axis = 0, reverse: bool = False):\n",
        "    elems_flat, tree = tree_flatten(elems)\n",
        "    if reverse: elems_flat = [torch.flip(elem, [axis]) for elem in elems_flat]\n",
        "    assert axis >= 0 or axis < elems_flat[0].ndim, \"Axis should be within bounds of input\"\n",
        "    num_elems = int(elems_flat[0].shape[axis])\n",
        "    if not all(int(elem.shape[axis]) == num_elems for elem in elems_flat[1:]):\n",
        "        raise ValueError('Array inputs to associative_scan must have the same first dimension. (saw: {})'.format([elem.shape for elem in elems_flat]))\n",
        "    scans = _scan(tree, operator, elems_flat, axis)\n",
        "    if reverse: scans = [torch.flip(scanned, [axis]) for scanned in scans]\n",
        "    return tree_unflatten(scans, tree)\n",
        "\n",
        "def _interleave(a, b, axis):\n",
        "    # https://stackoverflow.com/questions/60869537/how-can-i-interleave-5-pytorch-tensors\n",
        "    if b_trunc := (a.shape[axis] == b.shape[axis] + 1):\n",
        "        pad = [0, 0] * b.ndim\n",
        "        pad[(b.ndim-axis-1)*2+1] = 1 # +1=always end of dim, pad-order is reversed so start is at end\n",
        "        b = torch.nn.functional.pad(b, pad)\n",
        "\n",
        "    stacked = torch.stack([a, b], dim=axis+1)\n",
        "    interleaved = torch.flatten(stacked, start_dim=axis, end_dim=axis+1)\n",
        "    if b_trunc:\n",
        "        # TODO: find torch alternative for slice_along axis for torch.jit.script to work\n",
        "        interleaved = torch.ops.aten.slice(interleaved, axis, 0, b.shape[axis]+a.shape[axis]-1)\n",
        "    return interleaved\n",
        "\n",
        "# Taken from https://github.com/i404788/s5-pytorch/blob/74e2fdae00b915a62c914bf3615c0b8a4279eb84/s5/s5_model.py\n",
        "@torch.jit.script\n",
        "def binary_operator_diag(q_i: Tuple[torch.Tensor, torch.Tensor], q_j: Tuple[torch.Tensor, torch.Tensor]):\n",
        "    \"\"\"Binary operator for parallel scan of linear recurrence. Assumes a diagonal matrix A.\n",
        "    Args:\n",
        "        q_i: tuple containing A_i and Bu_i at position i       (P,), (P,)\n",
        "        q_j: tuple containing A_j and Bu_j at position j       (P,), (P,)\n",
        "    Returns:\n",
        "        new element ( A_out, Bu_out )\n",
        "    \"\"\"\n",
        "    A_i, b_i = q_i\n",
        "    A_j, b_j = q_j\n",
        "    # return A_j * A_i, A_j * b_i + b_j\n",
        "    return A_j*A_i, torch.addcmul(b_j, A_j, b_i)\n"
      ],
      "metadata": {
        "id": "grNn9AHPsxsM",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USvxeRTnsS9v",
        "outputId": "1b71e068-e252-493f-becb-8f8906f0ea4d",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 10000, 20])\n"
          ]
        }
      ],
      "source": [
        "# @title forgi86/lru\n",
        "# https://github.com/forgi86/sysid-pytorch-lru/blob/main/lru/linear.py\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LRU(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, d_model, rmin=0.0, rmax=1.0, max_phase=6.283):\n",
        "        super().__init__()\n",
        "        self.in_dim, self.out_dim, self.d_model = in_dim, out_dim, d_model\n",
        "        B_re, B_im = torch.randn(d_model, in_dim) / math.sqrt(2*in_dim), torch.randn(d_model, in_dim) / math.sqrt(2*in_dim)\n",
        "        self.B = nn.Parameter(torch.complex(B_re, B_im)) # N, U\n",
        "        C_re, C_im = torch.randn(out_dim, d_model) / math.sqrt(d_model), torch.randn(out_dim, d_model) / math.sqrt(d_model)\n",
        "        self.C = nn.Parameter(torch.complex(C_re, C_im)) # H, N\n",
        "        self.D = nn.Parameter(torch.randn(out_dim, in_dim) / math.sqrt(in_dim))\n",
        "\n",
        "        self.nu_log = nn.Parameter(torch.log(-.5*torch.log(torch.rand(d_model)*(rmax+rmin)*(rmax-rmin)+rmin**2)))\n",
        "        self.theta_log = nn.Parameter(torch.log(max_phase*torch.rand(d_model)))\n",
        "\n",
        "        lambda_abs = torch.exp(-torch.exp(self.nu_log))\n",
        "        self.gamma_log = nn.Parameter(torch.log(torch.sqrt(torch.ones_like(lambda_abs) - torch.square(lambda_abs))))\n",
        "\n",
        "    def ss_params(self):\n",
        "        lambda_abs = torch.exp(-torch.exp(self.nu_log))\n",
        "        lambda_phase = torch.exp(self.theta_log)\n",
        "        lambda_re = lambda_abs * torch.cos(lambda_phase)\n",
        "        lambda_im = lambda_abs * torch.sin(lambda_phase)\n",
        "        lambdas = torch.complex(lambda_re, lambda_im)\n",
        "        #lambdas = lambda_abs*torch.exp(1j*lambda_phase)\n",
        "        gammas = torch.exp(self.gamma_log).unsqueeze(-1).to(self.B.device)\n",
        "        B = gammas * self.B\n",
        "        return lambdas, B, self.C, self.D\n",
        "\n",
        "\n",
        "    def ss_real_matrices(self, to_numpy=True):\n",
        "        lambdas, B, self.C, self.D = self.ss_params()\n",
        "        lambdas_full = torch.zeros(2*self.d_model, device=lambdas.device, dtype=lambdas.dtype)\n",
        "        lambdas_full[::2] = lambdas\n",
        "        lambdas_full[1::2] = lambdas.conj()\n",
        "\n",
        "        # First convert to complex conjugate system....\n",
        "        A_full = torch.diag(lambdas_full)\n",
        "        # B_full = torch.zeros((2*self.d_model, self.in_dim), device=lambdas.device, dtype=lambdas.dtype)\n",
        "        B_full = torch.empty((2*self.d_model, self.in_dim), device=lambdas.device, dtype=lambdas.dtype)\n",
        "        B_full[::2] = B\n",
        "        B_full[1::2] = B.conj()\n",
        "        # C_full = torch.zeros((self.out_dim, 2*self.d_model), device=lambdas.device, dtype=lambdas.dtype)\n",
        "        C_full = torch.empty((self.out_dim, 2*self.d_model), device=lambdas.device, dtype=lambdas.dtype)\n",
        "        C_full[:, ::2] = 0.5*self.C # we take the real part of the complex conjugate system as output...\n",
        "        C_full[:, 1::2] = 0.5*self.C.conj()\n",
        "        # D_full = self.D\n",
        "\n",
        "        # Then apply transformation to real domain\n",
        "        T_block = torch.tensor([[1, 1], [1j, -1j]], device=lambdas.device, dtype=lambdas.dtype)\n",
        "        T_block_inv = torch.linalg.inv(T_block)\n",
        "        T_full = torch.block_diag(*([T_block] * self.d_model))\n",
        "        T_full_inv = torch.block_diag(*([T_block_inv] * self.d_model))\n",
        "\n",
        "        A_real = (T_full @ A_full @ T_full_inv).real\n",
        "        B_real = (T_full @ B_full).real\n",
        "        C_real = (C_full @ T_full_inv).real\n",
        "        # D_real = D_full\n",
        "\n",
        "        # ss_real_params = [A_real, B_real, C_real, D_real]\n",
        "        # if to_numpy: ss_real_params = [ss_real_param.detach().numpy() for ss_real_param in ss_real_params]\n",
        "        # return (*ss_real_params, )\n",
        "        return A_real, B_real, C_real, self.D\n",
        "\n",
        "\n",
        "    def forward_loop(self, input, state=None): # Input size: (B, L, H)\n",
        "        lambdas, B, C, D = self.ss_params()\n",
        "        output = torch.empty([i for i in input.shape[:-1]] + [self.out_dim], device=self.B.device)\n",
        "        states = []\n",
        "        for u_step in input.split(1, dim=1): # 1 is the time dimension\n",
        "            u_step = u_step.squeeze(1)\n",
        "            state = lambdas * state + u_step.to(B.dtype) @ B.T\n",
        "            states.append(state)\n",
        "        states = torch.stack(states, 1)\n",
        "        output = (states @ C.mT).real + input @ D.T\n",
        "        return output\n",
        "\n",
        "    @torch.compiler.disable\n",
        "    def forward_scan(self, input, state=None): # (B, L, H)\n",
        "        # Batched parallel scan, borrows heavily from https://colab.research.google.com/drive/1RgIv_3WAOW53CS0BnT7_782VKTYis9WG?usp=sharing\n",
        "        # which in turn borrows from https://github.com/i404788/s5-pytorch\n",
        "        lambdas, B, C, D = self.ss_params()\n",
        "        lambda_elements = lambdas.tile(input.shape[1], 1) # [N]->[L,N]\n",
        "        # Calculate B@u for each step u of each input sequence in the batch.\n",
        "        # Bu_elements will have shape (B, L, N)\n",
        "        Bu_elements = input.to(B.dtype) @ B.T\n",
        "        if state is not None: Bu_elements[:, 0, :] = Bu_elements[:, 0, :] + lambdas * state\n",
        "        # Vmap the associative scan since Bu_elements is a batch of B sequences.\n",
        "        # Recall that Lambda_elements has been repeated L times to (L, N),\n",
        "        # while Bu_seq has shape (B, L, N)\n",
        "        inner_state_fn = lambda Bu_seq: associative_scan(binary_operator_diag, (lambda_elements, Bu_seq))[1]\n",
        "        # inner_states will be of shape (B, L, N)\n",
        "        inner_states = torch.vmap(inner_state_fn)(Bu_elements)\n",
        "        #y = (inner_states @ self.C.T).real + input_sequences * self.D\n",
        "        y = (inner_states @ C.T).real + input @ D.T\n",
        "        return y\n",
        "\n",
        "    def forward(self, input, state=None):\n",
        "        if state is None: state = torch.view_as_complex(torch.zeros((self.d_model, 2), device=input.device)) # default initial state, size N\n",
        "        y = self.forward_scan(input, state)\n",
        "        y = self.forward_loop(input, state)\n",
        "        return y\n",
        "\n",
        "\n",
        "d_model = 40 #256 # N\n",
        "H = 20 #512 # input/output dimension\n",
        "L = 10_000 #2048 # input sequence length\n",
        "b = 32\n",
        "layer = LRU(in_dim=H, out_dim=H, d_model=d_model)\n",
        "input_sequences = torch.randn(b, L, H) # multiple sequences\n",
        "output_sequences = layer(input_sequences)\n",
        "output_sequences_scan = layer.forward_scan(input_sequences)\n",
        "torch.allclose(output_sequences_scan, output_sequences, 1e-2)\n",
        "\n",
        "print(output_sequences_scan.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title forgi86 architectures.py\n",
        "# https://github.com/forgi86/sysid-pytorch-lru/blob/main/lru/architectures.py\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "@dataclass\n",
        "class DWNConfig:\n",
        "    d_model = 10\n",
        "    d_state = 64\n",
        "    n_layers = 6\n",
        "    dropout = 0.0\n",
        "    bias: bool = True\n",
        "    rmin = 0.0\n",
        "    rmax = 1.0\n",
        "    max_phase = 2*math.pi\n",
        "    ff: str = \"GLU\"\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\" Standard Transformer MLP \"\"\"\n",
        "    def __init__(self, config: DWNConfig):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.d_model, 4 * config.d_model, bias=config.bias)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.c_proj = nn.Linear(4 * config.d_model, config.d_model, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout) if config.dropout > 0 else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class GLU(nn.Module):\n",
        "    def __init__(self, config: DWNConfig):\n",
        "        super().__init__()\n",
        "        self.activation = nn.GELU()\n",
        "        self.dropout = nn.Dropout(config.dropout) if config.dropout > 0 else nn.Identity()\n",
        "        self.output_linear = nn.Sequential(\n",
        "            nn.Linear(config.d_model, 2 * config.d_model),#nn.Conv1d(config.d_model, 2 * config.d_model, kernel_size=1),\n",
        "            nn.GLU(dim=-1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(self.activation(x))\n",
        "        x = self.output_linear(x)\n",
        "        return x\n",
        "\n",
        "class DWNBlock(nn.Module):\n",
        "    def __init__(self, config: DWNConfig):\n",
        "        super().__init__()\n",
        "        self.ln = nn.LayerNorm(config.d_model, bias=config.bias)\n",
        "        self.lru = LRU(config.d_model, config.d_model, config.d_state, rmin=config.rmin, rmax=config.rmax, max_phase=config.max_phase)\n",
        "        # self.ff = GLU(config)\n",
        "        self.ff = MLP(config)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x, state=None, mode=\"scan\"):\n",
        "        z = x\n",
        "        z = self.ln(z)  # prenorm\n",
        "        z = self.lru(z, state, mode)\n",
        "        z = self.ff(z) # MLP or GLU\n",
        "        z = self.dropout(z)\n",
        "        x = z + x\n",
        "        return x\n",
        "\n",
        "\n",
        "class DWN(nn.Module):\n",
        "    def __init__(self, n_u, n_y, config: DWNConfig):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Linear(n_u, config.d_model)\n",
        "        self.blocks = nn.ModuleList([DWNBlock(config) for _ in range(config.n_layers)])\n",
        "        self.decoder = nn.Linear(config.d_model, n_y)\n",
        "\n",
        "    def forward(self, u, state=None, mode=\"scan\"):\n",
        "        x = self.encoder(u)\n",
        "        for layer, block in enumerate(self.blocks):\n",
        "            state_block = state[layer] if state is not None else None\n",
        "            x = block(x, state=state_block, mode=mode)\n",
        "        x = self.decoder(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "l79o8FrVORLe",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title hrpan torch_lru lru.py base\n",
        "# https://github.com/hrpan/torch_lru/blob/main/lru.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "def parallel_lcse(log_input, log_coeff): # On Structured State-Space Duality oct 2025 https://www.arxiv.org/pdf/2510.04944\n",
        "    t, b, d = log_input.shape\n",
        "    t_log_coeff = torch.arange(t, device=log_coeff.device)[:,None] * log_coeff[None,:] # [t,]\n",
        "    t_log_coeff = t_log_coeff.unsqueeze(1) # [t,1,]\n",
        "    return t_log_coeff + torch.logcumsumexp(log_input - t_log_coeff, dim=0)\n",
        "\n",
        "# def conv(_input, log_coeff):\n",
        "#     t, b, d = _input.shape\n",
        "#     t_log_coeff = torch.arange(t-1,-1,-1, device=log_coeff.device)[:,None] * log_coeff[None,:]\n",
        "#     kernel_transpose = torch.diag_embed(t_log_coeff.exp())\n",
        "#     kernel = kernel_transpose.permute(1,2,0)\n",
        "#     input_pad = F.pad(_input.permute(1,2,0), (t-1, 0, 0, 0, 0, 0)) # T B D -> B D T\n",
        "#     return F.conv1d(input_pad, kernel.to(dtype=torch.complex64)).permute(2,0,1) # B D T -> T B D\n",
        "\n",
        "class LRU(nn.Module):\n",
        "    def __init__(self, in_dim, dim, r_min=.5, r_max=.95, max_phase=6.283):\n",
        "        super().__init__()\n",
        "        # self.b_linear = nn.Linear(dim, dim, bias=False)\n",
        "        # self.b_linear.weight = nn.Parameter((torch.randn(dim, dim) + 1j * torch.randn(dim, dim)) / np.sqrt(2*dim))\n",
        "        # self.c_linear = nn.Linear(dim, dim, bias=False)\n",
        "        # self.c_linear.weight = nn.Parameter((torch.randn(dim, dim) + 1j * torch.randn(dim, dim)) / np.sqrt(dim))\n",
        "        # self.d = nn.Parameter(torch.randn(dim)) # ?\n",
        "\n",
        "        self.b_linear = nn.Linear(in_dim, dim, bias=False)\n",
        "        self.b_linear.weight = nn.Parameter((torch.randn(dim, in_dim) + 1j * torch.randn(dim, in_dim)) / np.sqrt(2*in_dim))\n",
        "        self.c_linear = nn.Linear(dim, in_dim, bias=False)\n",
        "        self.c_linear.weight = nn.Parameter((torch.randn(in_dim, dim) + 1j * torch.randn(in_dim, dim)) / np.sqrt(dim))\n",
        "        self.d = nn.Parameter(torch.randn(in_dim)) # ?\n",
        "\n",
        "        self.nu_log = nn.Parameter(torch.log(-.5 * torch.log(torch.rand(dim) * (r_max+r_min)*(r_max-r_min) + r_min**2)))\n",
        "        self.theta_log = nn.Parameter(torch.log(max_phase * torch.rand(dim)))\n",
        "\n",
        "        _lambda = torch.exp(-torch.exp(self.nu_log) + 1j * torch.exp(self.theta_log)).detach()\n",
        "        self.gamma_log = nn.Parameter(torch.log(torch.sqrt(1 - torch.abs(_lambda)**2)))\n",
        "        self.forward = self.lcse\n",
        "\n",
        "\n",
        "    def lcse(self, x, h=None, eps=1e-10):\n",
        "        x_complex = x.to(dtype=torch.complex64)\n",
        "        print(x_complex.shape, self.b_linear.weight.shape)\n",
        "        bx = self.b_linear(x_complex) + eps\n",
        "        log_lambda = -torch.exp(self.nu_log) + 1j * torch.exp(self.theta_log)\n",
        "        if h is not None:\n",
        "            x_in = torch.cat([h.log(), self.gamma_log + bx.log()], dim=0)\n",
        "            ht = parallel_lcse(x_in, log_lambda)[1:].exp()\n",
        "        else:\n",
        "            x_in = self.gamma_log + bx.log()\n",
        "            ht = parallel_lcse(x_in, log_lambda).exp()\n",
        "        y = self.c_linear(ht.to(dtype=torch.complex64)).real + self.d * x\n",
        "        return y, ht[-1]\n",
        "\n",
        "    # def conv(self, x, h=None):\n",
        "    #     if h is None: h = torch.zeros_like(x[0])\n",
        "    #     log_lambda = -torch.exp(self.nu_log) + 1j * torch.exp(self.theta_log)\n",
        "    #     x_complex = x.to(dtype=torch.complex64)\n",
        "    #     bx = self.gamma_log.exp() * self.b_linear(x_complex)\n",
        "    #     ht = conv(bx, log_lambda)\n",
        "    #     y = self.c_linear(ht.to(dtype=torch.complex64)).real + self.d * x\n",
        "    #     return y, ht[-1]\n",
        "\n",
        "    # def seq(self, x, h=None): # [b,t,d], [t,d]\n",
        "    #     x_complex = x.to(dtype=torch.complex64)\n",
        "    #     if h is None: h = torch.zeros_like(x[0])\n",
        "    #     log_lambda = -torch.exp(self.nu_log) + 1j * torch.exp(self.theta_log)\n",
        "    #     bx = self.gamma_log.exp() * self.b_linear(x_complex)\n",
        "    #     ht = []\n",
        "    #     _lambda = log_lambda.exp()\n",
        "    #     for t in range(x.size(0)):\n",
        "    #         ht.append(h * _lambda + bx[t])\n",
        "    #         h = ht[-1]\n",
        "    #     ht = torch.stack(ht)\n",
        "    #     y = self.c_linear(ht.to(dtype=torch.complex64)).real + self.d * x\n",
        "    #     return y, ht[-1]\n",
        "\n",
        "\n",
        "class LRUBlock(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.prenorm = nn.LayerNorm(dim)\n",
        "        self.rnn = LRU(dim)\n",
        "        self.linear = nn.Linear(dim, 2*dim)\n",
        "\n",
        "    def forward(self, x, h=None):\n",
        "        z = self.prenorm(x)\n",
        "        z, h = self.rnn(z)\n",
        "        z = F.gelu(z)\n",
        "        z1, z2 = self.linear(z).chunk(2, dim=-1)\n",
        "        z = z1 * torch.sigmoid(z2)\n",
        "        return z + x, h\n",
        "\n",
        "\n",
        "# torch.set_default_device('cuda')\n",
        "# torch.set_default_device('cpu')\n",
        "# b, t = 32, 10000\n",
        "b, t = 4, 100\n",
        "in_dim = 20\n",
        "dim=40\n",
        "x = torch.randn(b, t, in_dim, dtype=torch.float32)\n",
        "\n",
        "layer = LRU(in_dim, dim)\n",
        "y_lcse = layer.lcse(x)\n",
        "# y_conv = layer.conv(x)\n",
        "# y_seq = layer.seq(x)\n",
        "# print('LCSE/SEQ output allclose:', torch.allclose(y_lcse[0], y_seq[0], atol=1e-4))\n",
        "# print('CONV/SEQ output allclose:', torch.allclose(y_conv[0], y_seq[0], atol=1e-4))\n",
        "\n",
        "# y_conv[0].sum().backward()\n",
        "# conv_grad = {}\n",
        "# for n, p in layer.named_parameters():\n",
        "#     conv_grad[n] = p.grad\n",
        "#     p.grad = None\n",
        "\n",
        "y_lcse[0].sum().backward()\n",
        "lcse_grad = {}\n",
        "for n, p in layer.named_parameters():\n",
        "    lcse_grad[n] = p.grad\n",
        "    p.grad = None\n",
        "\n",
        "# y_seq[0].sum().backward()\n",
        "# seq_grad = {}\n",
        "# for n, p in layer.named_parameters():\n",
        "#     seq_grad[n] = p.grad\n",
        "#     p.grad = None\n",
        "\n",
        "# for k in conv_grad.keys():\n",
        "#     print(f'CONV/SEQ {k} grad allclose:', torch.allclose(conv_grad[k], seq_grad[k], atol=1e-4))\n",
        "#     print(f'LCSE/SEQ {k} grad allclose:', torch.allclose(lcse_grad[k], seq_grad[k], atol=1e-4))\n",
        "\n",
        "repeats = 2\n",
        "log_x = np.arange(2, 16)\n",
        "y_lcse = []\n",
        "y_conv = []\n",
        "y_seq = []\n",
        "\n",
        "import time\n",
        "for log_length in log_x:\n",
        "    print('Seq length:', 2**log_length)\n",
        "    x = torch.randn((2**log_length, 1, in_dim), dtype=torch.float32)\n",
        "\n",
        "    t0 = time.time()\n",
        "    for _ in range(repeats):\n",
        "        layer.lcse(x)\n",
        "    delta_t = time.time() - t0\n",
        "    print('lcse:', delta_t)\n",
        "    y_lcse.append(delta_t)\n",
        "\n",
        "    # t0 = time.time()\n",
        "    # for _ in range(repeats):\n",
        "    #     layer.conv(x)\n",
        "    # delta_t = time.time() - t0\n",
        "    # print('conv:', delta_t)\n",
        "    # y_conv.append(delta_t)\n",
        "\n",
        "    # t0 = time.time()\n",
        "    # for _ in range(repeats):\n",
        "    #     layer.seq(x)\n",
        "    # delta_t = time.time() - t0\n",
        "    # print('seq:', time.time() - t0)\n",
        "    # y_seq.append(delta_t)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "QkIbTU14yR6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title hrpan torch_lru lru.py\n",
        "# https://github.com/hrpan/torch_lru/blob/main/lru.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "# torch.set_default_device('cuda')\n",
        "\n",
        "def parallel_lcse(log_input, log_coeff): # On Structured State-Space Duality oct 2025 https://www.arxiv.org/pdf/2510.04944\n",
        "    t, b, d = log_input.shape\n",
        "    t_log_coeff = torch.arange(t, device=log_coeff.device)[:,None] * log_coeff[None,:] # [t,]\n",
        "    t_log_coeff = t_log_coeff.unsqueeze(1) # [t,1,]\n",
        "    return t_log_coeff + torch.logcumsumexp(log_input - t_log_coeff, dim=0)\n",
        "\n",
        "def conv(_input, log_coeff):\n",
        "    t, b, d = _input.shape\n",
        "    t_log_coeff = torch.arange(t-1,-1,-1, device=log_coeff.device)[:,None] * log_coeff[None,:]\n",
        "    kernel_transpose = torch.diag_embed(t_log_coeff.exp())\n",
        "    kernel = kernel_transpose.permute(1,2,0).to(dtype=torch.complex64)\n",
        "    input_pad = F.pad(_input.permute(1,2,0), (t-1, 0, 0, 0, 0, 0)) # T B D -> B D T\n",
        "    return F.conv1d(input_pad, kernel).permute(2,0,1) # B D T -> T B D\n",
        "\n",
        "class LRU(nn.Module):\n",
        "    def __init__(self, in_dim, d_model=None, out_dim=None, r_min=.5, r_max=.95, max_phase=6.283):\n",
        "        super().__init__()\n",
        "        d_model, out_dim = d_model or in_dim, out_dim or in_dim\n",
        "        self.b_linear = nn.Linear(in_dim, d_model, bias=False)\n",
        "        self.b_linear.weight = nn.Parameter((torch.randn(d_model, in_dim) + 1j * torch.randn(d_model, in_dim)) / np.sqrt(2*in_dim))\n",
        "        self.c_linear = nn.Linear(d_model, out_dim, bias=False)\n",
        "        self.c_linear.weight = nn.Parameter((torch.randn(out_dim, d_model) + 1j * torch.randn(out_dim, d_model)) / np.sqrt(d_model))\n",
        "        self.d = nn.Parameter(torch.randn(out_dim)) # ?\n",
        "\n",
        "        self.nu_log = nn.Parameter(torch.log(-.5 * torch.log(torch.rand(d_model) * (r_max+r_min)*(r_max-r_min) + r_min**2)))\n",
        "        self.theta_log = nn.Parameter(torch.log(max_phase * torch.rand(d_model)))\n",
        "        _lambda = torch.exp(-torch.exp(self.nu_log) + 1j * torch.exp(self.theta_log)).detach()\n",
        "        self.gamma_log = nn.Parameter(torch.log(torch.sqrt(1 - torch.abs(_lambda)**2)))\n",
        "        self.forward = self.lcse\n",
        "# h_t+1 = lamb h_t + b ipnut ut/x_t\n",
        "# out y = c hidden h + d input u/x\n",
        "\n",
        "    def lcse(self, x, h=None, eps=1e-10):\n",
        "        x_complex = x.to(dtype=torch.complex64)\n",
        "        bx = self.b_linear(x_complex) + eps\n",
        "        log_lambda = -torch.exp(self.nu_log) + 1j * torch.exp(self.theta_log)\n",
        "        if h is not None:\n",
        "            x_in = torch.cat([h.log(), self.gamma_log + bx.log()], dim=0)\n",
        "            ht = parallel_lcse(x_in, log_lambda)[1:].exp()\n",
        "        else:\n",
        "            x_in = self.gamma_log + bx.log()\n",
        "            ht = parallel_lcse(x_in, log_lambda).exp()\n",
        "        y = self.c_linear(ht.to(dtype=torch.complex64)).real + self.d * x\n",
        "        return y, ht[-1]\n",
        "\n",
        "    def conv(self, x, h=None):\n",
        "        x_complex = x.to(dtype=torch.complex64)\n",
        "        if h is None: h = torch.zeros_like(x[0])\n",
        "        log_lambda = -torch.exp(self.nu_log) + 1j * torch.exp(self.theta_log)\n",
        "        bx = self.gamma_log.exp() * self.b_linear(x_complex)\n",
        "        ht = conv(bx, log_lambda)\n",
        "        y = self.c_linear(ht.to(dtype=torch.complex64)).real + self.d * x\n",
        "        return y, ht[-1]\n",
        "\n",
        "    def seq(self, x, h=None): # [t,b,d], [t,d]\n",
        "        x_complex = x.to(dtype=torch.complex64)\n",
        "        if h is None: h = torch.zeros_like(x[0])\n",
        "        log_lambda = -torch.exp(self.nu_log) + 1j * torch.exp(self.theta_log)\n",
        "        bx = self.gamma_log.exp() * self.b_linear(x_complex)\n",
        "        ht = []\n",
        "        _lambda = log_lambda.exp()\n",
        "        for t in range(x.size(0)):\n",
        "            ht.append(h * _lambda + bx[t])\n",
        "            h = ht[-1]\n",
        "        ht = torch.stack(ht)\n",
        "        y = self.c_linear(ht.to(dtype=torch.complex64)).real + self.d * x\n",
        "        return y, ht[-1]\n",
        "\n",
        "# class LRUBlock(nn.Module):\n",
        "#     def __init__(self, dim):\n",
        "#         super().__init__()\n",
        "#         self.prenorm = nn.LayerNorm(dim)\n",
        "#         self.rnn = LRU(dim)\n",
        "#         self.linear = nn.Linear(dim, 2*dim)\n",
        "\n",
        "#     def forward(self, x, h=None):\n",
        "#         z = self.prenorm(x)\n",
        "#         z, h = self.rnn(z)\n",
        "#         z = F.gelu(z)\n",
        "#         z1, z2 = self.linear(z).chunk(2, dim=-1)\n",
        "#         z = z1 * torch.sigmoid(z2)\n",
        "#         return z + x, h\n",
        "\n",
        "\n",
        "# b, t = 32, 10000\n",
        "b, t = 4, 100\n",
        "in_dim = 20\n",
        "d_model = 40\n",
        "x = torch.randn(t,b,in_dim, dtype=torch.float32)\n",
        "\n",
        "layer = LRU(in_dim, d_model)\n",
        "y_lcse = layer.lcse(x)\n",
        "\n",
        "y_lcse[0].sum().backward()\n",
        "lcse_grad = {}\n",
        "for n, p in layer.named_parameters():\n",
        "    lcse_grad[n] = p.grad\n",
        "    p.grad = None\n",
        "\n",
        "repeats = 2\n",
        "log_x = np.arange(2,16)\n",
        "y_lcse = []\n",
        "y_conv = []\n",
        "y_seq = []\n",
        "\n",
        "import time\n",
        "for log_length in log_x:\n",
        "    print('Seq length:', 2**log_length)\n",
        "    x = torch.randn(2**log_length, 1, in_dim, dtype=torch.float32) # [t,b,in]\n",
        "\n",
        "    t0 = time.time()\n",
        "    for _ in range(repeats):\n",
        "        layer.lcse(x)\n",
        "    delta_t = time.time() - t0\n",
        "    print('lcse:', delta_t)\n",
        "    y_lcse.append(delta_t)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvHgbfe69VbK",
        "outputId": "5a019173-dae8-4d4d-b1e1-ea466990627b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seq length: 4\n",
            "lcse: 0.0011658668518066406\n",
            "Seq length: 8\n",
            "lcse: 0.001047372817993164\n",
            "Seq length: 16\n",
            "lcse: 0.001398324966430664\n",
            "Seq length: 32\n",
            "lcse: 0.0019273757934570312\n",
            "Seq length: 64\n",
            "lcse: 0.0030863285064697266\n",
            "Seq length: 128\n",
            "lcse: 0.005485057830810547\n",
            "Seq length: 256\n",
            "lcse: 0.010121345520019531\n",
            "Seq length: 512\n",
            "lcse: 0.020756244659423828\n",
            "Seq length: 1024\n",
            "lcse: 0.03751349449157715\n",
            "Seq length: 2048\n",
            "lcse: 0.06968951225280762\n",
            "Seq length: 4096\n",
            "lcse: 0.1582951545715332\n",
            "Seq length: 8192\n",
            "lcse: 0.27878808975219727\n",
            "Seq length: 16384\n",
            "lcse: 0.6257624626159668\n",
            "Seq length: 32768\n",
            "lcse: 1.2754313945770264\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title LRUBlock\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class GLU(nn.Module): # https://arxiv.org/pdf/2002.05202\n",
        "    def __init__(self, in_dim, d_model=None):\n",
        "        super().__init__()\n",
        "        d_model = d_model or in_dim\n",
        "        self.lin = nn.Sequential(\n",
        "            # nn.LayerNorm(in_dim),\n",
        "            nn.GELU(), # SiLU\n",
        "            # nn.Linear(in_dim, d_model)\n",
        "            zero_module(nn.Linear(in_dim, 2*d_model, bias=False))\n",
        "        )\n",
        "\n",
        "    def forward(self, x): # [b,t,d]\n",
        "        x0, x1 = self.lin(x).chunk(2, dim=-1)\n",
        "        x = x0 * torch.sigmoid(x1)\n",
        "        return x\n",
        "\n",
        "class LRUBlock(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.prenorm = nn.LayerNorm(d_model)\n",
        "        self.rnn = LRU(d_model)\n",
        "        self.glu = GLU(d_model)\n",
        "\n",
        "    def forward(self, x, h=None):\n",
        "        z, h = self.rnn(self.prenorm(x), h)\n",
        "        x = x + self.glu(z)\n",
        "        return x, h\n",
        "\n",
        "b, t, d_model = 4, 100, 40\n",
        "lrublock = LRUBlock(d_model)\n",
        "x = torch.randn(t,b,d_model, dtype=torch.float32)\n",
        "\n",
        "y, h = lrublock(x)\n",
        "print(y.shape, h.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4lFRk81Drxg",
        "outputId": "142b20a6-2212-4136-c854-5e02996691e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100, 4, 40]) torch.Size([4, 40])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Gothos LRU.py\n",
        "# https://github.com/Gothos/LRU-pytorch/blob/main/LRU_pytorch/LRU.py\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LRU(nn.Module):\n",
        "    def __init__(self,in_features,out_features,state_features, rmin=0, rmax=1,max_phase=6.283):\n",
        "        super().__init__()\n",
        "        self.out_features=out_features\n",
        "        self.D=nn.Parameter(torch.randn([out_features,in_features])/math.sqrt(in_features))\n",
        "        u1=torch.rand(state_features)\n",
        "        u2=torch.rand(state_features)\n",
        "        self.nu_log= nn.Parameter(torch.log(-0.5*torch.log(u1*(rmax+rmin)*(rmax-rmin) + rmin**2)))\n",
        "        self.theta_log= nn.Parameter(torch.log(max_phase*u2))\n",
        "        Lambda_mod=torch.exp(-torch.exp(self.nu_log))\n",
        "        self.gamma_log=nn.Parameter(torch.log(torch.sqrt(torch.ones_like(Lambda_mod)-torch.square(Lambda_mod))))\n",
        "        B_re=torch.randn([state_features,in_features])/math.sqrt(2*in_features)\n",
        "        B_im=torch.randn([state_features,in_features])/math.sqrt(2*in_features)\n",
        "        self.B=nn.Parameter(torch.complex(B_re,B_im))\n",
        "        C_re=torch.randn([out_features,state_features])/math.sqrt(state_features)\n",
        "        C_im=torch.randn([out_features,state_features])/math.sqrt(state_features)\n",
        "        self.C=nn.Parameter(torch.complex(C_re,C_im))\n",
        "        self.state=torch.complex(torch.zeros(state_features),torch.zeros(state_features))\n",
        "\n",
        "    def forward(self, input,state=None):\n",
        "        self.state=self.state.to(self.B.device) if state==None else state\n",
        "        Lambda_mod=torch.exp(-torch.exp(self.nu_log))\n",
        "        Lambda_re=Lambda_mod*torch.cos(torch.exp(self.theta_log))\n",
        "        Lambda_im=Lambda_mod*torch.sin(torch.exp(self.theta_log))\n",
        "        Lambda=torch.complex(Lambda_re,Lambda_im)\n",
        "        Lambda=Lambda.to(self.state.device)\n",
        "        gammas=torch.exp(self.gamma_log).unsqueeze(-1).to(self.B.device)\n",
        "        gammas=gammas.to(self.state.device)\n",
        "        output=torch.empty([i for i in input.shape[:-1]] +[self.out_features],device=self.B.device)\n",
        "        #Handle input of (Batches,Seq_length, Input size)\n",
        "        if input.dim()==3:\n",
        "            for i,batch in enumerate(input):\n",
        "                out_seq=torch.empty(input.shape[1],self.out_features)\n",
        "                for j,step in enumerate(batch):\n",
        "                    self.state=(Lambda*self.state + gammas* self.B@step.to(dtype= self.B.dtype))\n",
        "                    out_step= (self.C@self.state).real + self.D@step\n",
        "                    out_seq[j]=out_step\n",
        "                self.state=torch.complex(torch.zeros_like(self.state.real),torch.zeros_like(self.state.real))\n",
        "                output[i]=out_seq\n",
        "        #Handle input of (Seq_length, Input size)\n",
        "        if input.dim()==2:\n",
        "            for i,step in enumerate(input):\n",
        "                self.state=(Lambda*self.state + gammas* self.B@step.to(dtype= self.B.dtype))\n",
        "                out_step= (self.C@self.state).real + self.D@step\n",
        "                output[i]=out_step\n",
        "            self.state=torch.complex(torch.zeros_like(self.state.real),torch.zeros_like(self.state.real))\n",
        "        return output\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "5zbnMn7F_-aM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}